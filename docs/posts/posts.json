[
  {
    "path": "posts/20210115_kovats/",
    "title": "Kovats Index",
    "description": "R script for calculating Kovats Index",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\nContents\nBackground\nR workflow\nExampleLoad packages\nImport\nTransform\n\nReferences:\n\nBackground\nAbout 70% of my time at work is spent on interpreting GCMS and GC data. It is more of a qualitative type of identifying what each peak is, and this requires a seach based on mass spectra found in the GCMS library, as well as using the retention index. When working on GC data, I am even more reliant on the retention index for cross checking of peaks on GCMS, since there is no spectra information available.\nRetention time is influenced by GC conditions and column types. Using retention time alone is not useful when you are trying to compare with retention times stated in the literature, since the elution conditions are different.\nThe Kovats index (KI) may be used to convert retention times into standardised retention indices (RI), based on retention times of alkane standards. The equation for Non-Isothermal Kovats RI is shown below.\n\\[\nI_x = 100n + 100(t_x-t_n) / (t_(n+1) − t_n)\n\\]\nPrior to learning R, I used to do the calculation on an excel spreadsheet. This was cumbersome, first I had to key in the retention times of each alkane standard, and then update my formula for the range of retention times between each alkane standard, and then copy and paste all the compiled retention times into 2 columns. That involved a lot of clicking with the mouse.\nR workflow\nRun alkane standards on instrument (for example, GCMS) and compile the retention times in either .csv or .xlsx.\nCreate a function to calculate KI.\nCalculate the KI for retention times between each pair of alkane standard\nMerge the compiled retention times and corresponding KI together\nExport the data to excel and use the vlookup function to find out the KI when retention time is keyed in; alternatively, use inner_join function to tabulate calculated KI before identifying the peaks. I am using the former as there may be some small peaks that were not integrated, or coeluted with other peaks, so there is still a degree of manual input that is required.\nExample\nSample retention time data was retrieved from: https://massfinder.com/wiki/Retention_index_guide\nLoad packages\n\n\nlibrary(tidyverse)\n\n\n\nImport\n\n\n# Key in values\ncarbon_number <- c(\"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\")\nMS_RT <- c(1.85, 2.71, 3.69, 4.59, 5.37, 6.19, 7.17, 8.40, 9.99)\n\n# Create a tibble\nms_rt <- cbind(carbon_number, MS_RT) %>% as_tibble()\nms_rt$carbon_number <- as.numeric(ms_rt$carbon_number)\nms_rt$MS_RT <- as.numeric(ms_rt$MS_RT)\n\n# The data may also be imported from excel\n\n\n\nTransform\n\n\n# create function to calculate KI ####\nto_Calc_KI = function(n,Tn,m,Tm,Ti){\n  RI = 100*n + (100*(m-n)*((Ti-Tn)/(Tm-Tn)))\n  round(RI, 0)\n  \n}\n\n\n\n\n\n# create function to filter by carbon number ####\n# dat refers to data\n# col refers to column\n# val refers to values\n\nfilter_by_carbon_number <- function(dat, col, val){\n  filter(dat, col %in%  val)\n}\n\n\n\nThe following step could be improved on by creating another function to repeat the codes rather than manually changing the values.\n\n\nfil_c8c9 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(8,9)) \n\nfil_c9c10 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(9,10)) \n\nfil_c10c11 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(10,11)) \nfil_c11c12 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(11,12)) \nfil_c12c13 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(12,13)) \nfil_c13c14 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(13,14)) \nfil_c14c15 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(14,15)) \nfil_c15c16 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(15,16)) \n\n\n\n\n\n# create function to generate tibble for KI calculation\ncreate_KI_tibble <- function(msrt_col, n , m){\n  seq(from = min(msrt_col), to = max(msrt_col), by = 0.01) %>% \n    as_tibble() %>% \n    rename(\"Ti\" = value) %>% \n    mutate(n = n,\n           m = m,\n           Tn = min(msrt_col), \n           Tm = max(msrt_col)) %>% \n    dplyr::select(n, Tn, m, Tm, Ti) %>% \n    mutate(KI = pmap_dbl(., to_Calc_KI))\n}\n\n\n\n\n\nc8c9 <- create_KI_tibble(fil_c8c9$MS_RT, 8, 9)\nc9c10 <- create_KI_tibble(fil_c9c10$MS_RT, 9, 10)\nc10c11 <- create_KI_tibble(fil_c10c11$MS_RT, 10, 11)\nc11c12 <- create_KI_tibble(fil_c11c12$MS_RT, 11, 12)\nc12c13 <- create_KI_tibble(fil_c12c13$MS_RT, 12, 13)\nc13c14 <- create_KI_tibble(fil_c13c14$MS_RT, 13, 14)\nc14c15 <- create_KI_tibble(fil_c14c15$MS_RT, 14, 15)\nc15c16 <- create_KI_tibble(fil_c15c16$MS_RT, 15, 16)\n\ncalculated_MS_KI <- rbind(c8c9, c9c10, c10c11, c11c12, c12c13, \n                          c13c14, c14c15, c15c16) %>% \n  select(Ti, KI)\n\n# Export created file if needed\n# write_xlsx(calculated_MS_KI, \"Kovats_Indices.xlsx\")\n\n\n\nLooking at the first 6 lines of tabulated KI:\n\n\nhead(calculated_MS_KI)\n\n\n# A tibble: 6 x 2\n     Ti    KI\n  <dbl> <dbl>\n1  1.85   800\n2  1.86   801\n3  1.87   802\n4  1.88   803\n5  1.89   805\n6  1.9    806\n\nReferences:\nhttps://webbook.nist.gov/chemistry/gc-ri/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-15T11:19:30+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210118_calibration curves/",
    "title": "Calibration Curves Data",
    "description": "R script for calculating Limit of Detection and Limit of Quantification",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\nContents\nLoading required packages\nBackground\nImport Example Dataset\nVisualize\nModel\nErrors in slope and intercept of regression line\nPredict the value of x from y:\n\nNote: the data and theory on calibration curve were with reference from: Statistics and Chemometrics for Analytical Chemistry, James N. Miller and Jane Charlotte Miller, 6th edition, Chapter 5\nLoading required packages\n\n\nlibrary(pacman)\np_load(tidyverse, broom, chemCal)\n\n\n\nBackground\nChemists often work with calibration data using standards of known concentrations and putting them through instrumental analysis. When plotting a calibration curve, it is of interest to calculate the limit of detection (LOD) and limit of quantification (LOQ) of the method.\nImport Example Dataset\nThe fluorescence intensities of standard aqueous fluorescein solutions were analysed with a spectrophotometer, and the fluorescence results are shown below:\n\n# A tibble: 7 x 2\n  conc_pgml  fluo\n      <dbl> <dbl>\n1         0   2.1\n2         2   5  \n3         4   9  \n4         6  12.6\n5         8  17.3\n6        10  21  \n7        12  24.7\n\nVisualize\n\n\n\nModel\nLet’s fit a linear model to get the slope (b) and intercept(a).\n\\[\ny = a + bx\n\\]\n\n\nCall:\nlm(formula = fluo ~ conc_pgml, data = data)\n\nResiduals:\n       1        2        3        4        5        6        7 \n 0.58214 -0.37857 -0.23929 -0.50000  0.33929  0.17857  0.01786 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.5179     0.2949   5.146  0.00363 ** \nconc_pgml     1.9304     0.0409  47.197 8.07e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4328 on 5 degrees of freedom\nMultiple R-squared:  0.9978,    Adjusted R-squared:  0.9973 \nF-statistic:  2228 on 1 and 5 DF,  p-value: 8.066e-08\n\nFrom above, we can see that slope = 1.9304, and intercept = 1.5179.\nErrors in slope and intercept of regression line\nThe limit of detection is defined as:\n\\[\nLOD = \\gamma_B + 3_{SB}\n\\] where LOD is the analyte concentration wich gives a signal equal to the blank signal plus three standard deviations of the blank.\nA function was created to calculate LOD and LOQ:\n\n\ncalcLOD_y <- function(model) {\n  SSE <- sum(model$residuals**2)\n  n <- length(model$residuals) -2\n  Syx <- sqrt(SSE/n)\n  intercept <- as.numeric(model$coefficients[1])\n  calculated_y <- intercept + 3*Syx\n  names(calculated_y) <- \"calculated_y\"\n  print(calculated_y)\n  \n  chemCal::inverse.predict(model,\n                  newdata = calculated_y,\n                  alpha = 0.05) \n}\n\n\n\n\n\ncalcLOQ_y <- function(model) {\n  SSE <- sum(model$residuals**2)\n  n <- length(model$residuals) -2\n  Syx <- sqrt(SSE/n)\n  intercept <- as.numeric(model$coefficients[1])\n  calculated_y <- intercept + 10*Syx\n  names(calculated_y) <- \"calculated_y\"\n  print(calculated_y)\n  \n  chemCal::inverse.predict(model,\n                  newdata = calculated_y,\n                  alpha = 0.05) \n}\n\n\n\nInserting the linear model from the fluorescence data:\n\n\nLOD_x <- calcLOD_y(fl_mod)\n\n\ncalculated_y \n      2.8164 \n\nLOD_x$Prediction \n\n\n[1] 0.6726958\n\n\n\nLOQ_x <- calcLOQ_y(fl_mod)\n\n\ncalculated_y \n    5.846334 \n\nLOQ_x$Prediction \n\n\n[1] 2.242319\n\nPredict the value of x from y:\nTo predict the concentration of fluorescein that has fluorescence units of 2.9, we use the function inverse.predict():\n\n\nchemCal::inverse.predict(fl_mod, \n                newdata = 2.9,\n                alpha = 0.05)\n\n\n$Prediction\n[1] 0.7160037\n\n$`Standard Error`\n[1] 0.2645698\n\n$Confidence\n[1] 0.6800982\n\n$`Confidence Limits`\n[1] 0.03590545 1.39610195\n\n\n\n\n",
    "preview": "posts/20210118_calibration curves/calibration-curves_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-01-18T23:23:27+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210114_motivations/",
    "title": "Motivations",
    "description": "why R?",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\nWhy R?\nI attended a short modular course on R, and was introduced to more effective and efficient ways of structuring data for customised plots that look way better than on Excel and SPSS. At the end of the course, I really wanted to retain what I have learnt, and build on what I have learnt, so that I can be better at R.\nR, to me, is a new form of literacy (like how Microsoft Office was taught in school last time). It is also an effective approach to learn problem solving, as well as a job skill.\nAristotle — ‘The more you know, the more you know you don’t know.’\nand that makes me want to learn even more.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-14T20:47:28+08:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "My first post: Learning goals for 2021",
    "description": "pRactice corner for coding in R",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\nHi, this is my practice corner for coding in R. I would want to:\nlearn tidyverse\npractice on data visualization, exploration.\nlearn tidymodels/machine learning\nwork on chemistry related datasets using R\nlearn Design of Experiment\nlearn Chemometrics\nlearn how to analyse sensory data\nbe able to communicate insights from data analysis using the Rmarkdown/distill packages\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-18T21:54:53+08:00",
    "input_file": {}
  }
]
