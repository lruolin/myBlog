[
  {
    "path": "posts/20210424_Tidyverse Chap 1 - Data visualization/",
    "title": "Data visualization with ggplot2",
    "description": "R4DS 01 - Data visualization with ggplot 2",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-04-24",
    "categories": [],
    "contents": "\nR4DS Practice 01: Data visualization with ggplot 2\nThe codes below are from the practice exercises in https://r4ds.had.co.nz/, and are taken with reference from: https://jrnold.github.io/r4ds-exercise-solutions/\nLet’s begin now\nLoading tidyverse package\n\n\nlibrary(tidyverse)\n\n\n\nLoading mpg data frame: mpg dataset is the fuel economy data from 1999 to 2008 for 38 popular models of cars. This dataset contains a subset of the fuel economy data that the EPA makes available on https://fueleconomy.gov/. It contains only models which had a new release every year between 1999 and 2008 - this was used as a proxy for the popularity of the car.\n\n\nmpg\n\n\n# A tibble: 234 x 11\n   manufacturer model  displ  year   cyl trans drv     cty   hwy fl   \n   <chr>        <chr>  <dbl> <int> <int> <chr> <chr> <int> <int> <chr>\n 1 audi         a4       1.8  1999     4 auto… f        18    29 p    \n 2 audi         a4       1.8  1999     4 manu… f        21    29 p    \n 3 audi         a4       2    2008     4 manu… f        20    31 p    \n 4 audi         a4       2    2008     4 auto… f        21    30 p    \n 5 audi         a4       2.8  1999     6 auto… f        16    26 p    \n 6 audi         a4       2.8  1999     6 manu… f        18    26 p    \n 7 audi         a4       3.1  2008     6 auto… f        18    27 p    \n 8 audi         a4 qu…   1.8  1999     4 manu… 4        18    26 p    \n 9 audi         a4 qu…   1.8  1999     4 auto… 4        16    25 p    \n10 audi         a4 qu…   2    2008     4 manu… 4        20    28 p    \n# … with 224 more rows, and 1 more variable: class <chr>\n\nCreating a ggplot:\n\n\nmpg %>%  # this is the pipe operator\n  ggplot(aes(x = displ, y = hwy)) +  # variable names within aes(); use + and not %>%\n  geom_point() # type of plot\n\n\n\n\nThere is a negative relationship between engine size (displ) and fuel efficiency (hwy).\nThe bigger the engine size, the lower the fuel efficiency, the more fuel they use.\n\n\nggplot(mpg) # this returns a blank\n\n\n\n\nHow many rows are there in the mpg dataset?\n\n\nglimpse(mpg)\n\n\nRows: 234\nColumns: 11\n$ manufacturer <chr> \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\",…\n$ model        <chr> \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 q…\n$ displ        <dbl> 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.…\n$ year         <int> 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999,…\n$ cyl          <int> 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6,…\n$ trans        <chr> \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(a…\n$ drv          <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4…\n$ cty          <int> 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15,…\n$ hwy          <int> 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25,…\n$ fl           <chr> \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        <chr> \"compact\", \"compact\", \"compact\", \"compact\", \"co…\n\nThere are 234 rows and 11 columns.\nWhat does the drv variable describe?\n\n\n?mpg\n\n\n\nThe drv variable describes the type of drive train:\nf = front wheel drive\nr = rear wheel drive\n4 = 4 wheel drive\nMake a scatterplot of hwy vs cyl\n\n\nmpg %>% \n  ggplot(aes(x = cyl, y = hwy)) +\n  geom_point()\n\n\n\n\nWhat happens if you make a scatterplot of class vs drv?\ndrv: type of drive train class: type of car\n\n\nmpg %>% \n  ggplot(aes(class, drv)) +\n  geom_point()\n\n\n\n\nA scatter-plot is more suitable for visualizaing relationships between continuous x and continuous y data, hence is not suitable for categorical x and categorical y.\nAdding colors to the variable\nTo see if the cars that fall outside the linear trend, which have a higher mileage than expected, are hybrid cars.\n\n\nmpg %>% \n  ggplot(aes(displ, hwy, col = class)) + # adding color according to the class variable\n  geom_point() +\n  theme_classic() # changing the theme\n\n\n\n\n\n\nglimpse(mpg)\n\n\nRows: 234\nColumns: 11\n$ manufacturer <chr> \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\",…\n$ model        <chr> \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 q…\n$ displ        <dbl> 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.…\n$ year         <int> 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999,…\n$ cyl          <int> 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6,…\n$ trans        <chr> \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(a…\n$ drv          <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4…\n$ cty          <int> 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15,…\n$ hwy          <int> 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25,…\n$ fl           <chr> \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        <chr> \"compact\", \"compact\", \"compact\", \"compact\", \"co…\n\nContinuous variables are those that are numerical. Categorical are those that have .\n\n\n# values in cyl col:\n\nmpg %>% \n  select(cyl) %>% \n  unique()\n\n\n# A tibble: 4 x 1\n    cyl\n  <int>\n1     4\n2     6\n3     8\n4     5\n\nHowever, discrete continuous variables may also be categorical, eg for cyl (number of cylinders), the values are 4, 5, 6, 8.\nContinuous variables, when mapped to color, will take on different hues of the colour. When mapped to size, the size of the point will vary according to the value. When mapped to shape, there will be an error.\n\n\n# continuous variable cty mapped to color\nmpg %>% \n  ggplot(aes(displ, hwy, col = cty)) + # cty = city miles per gallon\n  geom_point()\n\n\n\n# continuous variable cty mapped to size\nmpg %>% \n  ggplot(aes(displ, hwy, size = cty)) + # cty = city miles per gallon\n  geom_point()\n\n\n\n# continuous variable cty mapped to color and size\nmpg %>% \n  ggplot(aes(displ, hwy, col = cty, size = cty)) + # cty = city miles per gallon\n  geom_point()\n\n\n\n\nWhat does the stroke aesthetic do? It changes the size of the border for shapes 21 - 25, which are the filled shapes.\n\n\nmpg %>% \n  ggplot(aes(displ, hwy)) +\n  geom_point(shape = 24, stroke = 1 )\n\n\n\nmpg %>% \n  ggplot(aes(displ, hwy)) +\n  geom_point(shape = 24, stroke = 5)\n\n\n\n\nWhat happens if you want to map an aesthetic to something other than a variable name, like aes(col = displ < 5)?\n\n\nmpg %>% \n  ggplot(aes(displ, hwy)) +\n  geom_point(aes(col = displ<5)) + # a temporary variable will be added to the data for plotting\n  theme_classic()\n\n\n\n\nFaceting\nFaceting is useful to split the plot by categories. You will still need to define the dataset, the ggplot command, the type of plot (geom_point) and then use facet_wrap.\nfacet_wrap:\n\n\nglimpse(mpg) \n\n\nRows: 234\nColumns: 11\n$ manufacturer <chr> \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"audi\",…\n$ model        <chr> \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4 q…\n$ displ        <dbl> 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8, 2.…\n$ year         <int> 1999, 1999, 2008, 2008, 1999, 1999, 2008, 1999,…\n$ cyl          <int> 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6, 6,…\n$ trans        <chr> \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"auto(a…\n$ drv          <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\", \"4…\n$ cty          <int> 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, 15,…\n$ hwy          <int> 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, 25,…\n$ fl           <chr> \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p…\n$ class        <chr> \"compact\", \"compact\", \"compact\", \"compact\", \"co…\n\n# categorical variables may be manufacturer, model\n\nmpg %>% \n  ggplot(aes(displ, hwy)) +\n  geom_point() +\n  facet_wrap( ~ class, nrow = 2) # facet wrap ( row ~ col, number of rows = 2)\n\n\n\n\nfacet_grid: facet_grid() forms a matrix of panels defined by row and column faceting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data. If you have only one variable with many levels, try facet_wrap().\n\n\nmpg %>% \n  ggplot(aes(displ, hwy)) +\n  geom_point() +\n  facet_grid(drv ~cyl)\n\n\n\n\nFaceting is not recommended for continuous variable since each panel will be a unique value of the continuous variable.\nEmpty cells mean that there is no observation in that plot.\nWhat plots do the following codes make?\nThe dot is used as a placeholder.\n\n\nmpg %>% \n  ggplot(aes(displ, hwy)) +\n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\n\n\nmpg %>% \n  ggplot(aes(displ, hwy)) +\n  geom_point(aes(x = displ, y = hwy)) +\n  facet_grid(. ~ cyl)\n\n\n\n\n\n\nmpg %>% \n  ggplot(aes(x = displ, y = hwy)) + \n  geom_point() +\n  facet_wrap( ~ class, nrow = 2)\n\n\n\n\nGeometric objects\n\n\nmpg %>% \n  ggplot(aes(x = displ, y = hwy)) +\n  geom_smooth(aes(linetype = drv))\n\n\n\n\n\n\nmpg %>% \n  ggplot(aes(displ, hwy, col = drv)) +\n  geom_smooth()\n\n\n\n\nRecreating R plots:\n\n\nmpg %>% \n  ggplot(aes(x = displ, y = hwy)) +\n  geom_smooth(se = F) +\n  geom_point()\n\n\n\nmpg %>% \n  ggplot(aes(x = displ, y = hwy)) +\n  geom_smooth(aes(group = drv), se = F) +\n  geom_point()\n\n\n\nmpg %>% \n  ggplot(aes(x = displ, y = hwy)) +\n  geom_smooth(aes(group = drv, col = drv), se = F) +\n  geom_point(aes(col = drv))\n\n\n\nmpg %>% \n  ggplot(aes(x = displ, y = hwy)) +\n  geom_smooth( se = F) +\n  geom_point(aes(col= drv))\n\n\n\nmpg %>% \n  ggplot(aes(x = displ, y = hwy)) +\n  geom_smooth(aes(linetype = drv), se = F) +\n  geom_point(aes(col = drv))\n\n\n\nmpg %>% \n  ggplot(aes(displ, hwy)) +\n  geom_point(size = 4, col = \"white\") +\n  geom_point(aes(col = drv))\n\n\n\n\nStatistical transformation, diamonds dataset\n\n\ndiamonds %>% \n  ggplot(aes(cut)) +\n  geom_bar() +\n  labs(title = \"Count of diamonds for various cuts\") +\n  theme_classic()\n\n\n\n\nOveriding the default mapping - to display a bar chart of proportion rather than count\n\n\ndiamonds %>% \n  ggplot(aes(cut)) +\n  geom_bar(mapping = aes(cut, y = ..prop.. , group = 1))\n\n\n\n\n\n\nggplot(data = diamonds) +\n  stat_summary(\n    mapping = aes( x = cut, y = depth),\n    fun.ymin = min,\n    fun.ymax = max,\n    fun.y = median\n  ) +\n  theme_classic()\n\n\n\n\nPosition adjustments: Stacked bar chart\n\n\ndiamonds %>% \n  ggplot(aes(x = cut)) +\n  geom_bar(aes(fill = clarity)) +\n  theme_classic()\n\n\n\n# to compare proportions across groups\ndiamonds %>% \n  ggplot(aes(x = cut)) +\n  geom_bar(aes(fill = clarity), position = \"fill\") +\n  theme_classic()\n\n\n\n# to compare proportions across groups\ndiamonds %>% \n  ggplot(aes(x = cut)) +\n  geom_bar(aes(fill = clarity), position = \"dodge\") + # to compare individual values\n  theme_classic()\n\n\n\n\nFor geom_boxplot, the default position is position_dodge2 (moves the geom horizontally to avoid overlapping other geoms.)\n\n\nmpg %>% \n  ggplot(aes(drv, hwy, col= class)) +\n  geom_boxplot() +\n  theme_classic()\n\n\n\n\nNotes:\nThis first chapter of R4DS is aimed at giving a basic and gentle introduction to the ggplot workflow. The main points are to remember to define the dataframe, to remember to add aes() for defining the variables, and to use + and not %>% for each new line in ggplot2. There are many geom objects (boxplot, barplot etc) which are available in the cheatsheet. There are also various ways to change the appearance of the plot and I like how easy it is to change the colors. The title, subtitle and axis titles may be changed using the labs() function. It is a bit different from the Excel workflow which allows you to point and click, but for ggplot2 you can only change the appearance one layer by one layer, and may take a while to get used to, but I appreciate the convenience of using commands rather than clicking of mouse, and the scalability in which I can put in new data and run all of the commands to generate the same plot.\nReference\nhttps://r4ds.had.co.nz/\nhttps://jrnold.github.io/r4ds-exercise-solutions/\n\n\n\n",
    "preview": "posts/20210424_Tidyverse Chap 1 - Data visualization/01---Data-visualization_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-04-24T21:42:38+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210421_DOE Full Factorial Another Example/",
    "title": "Design of Experiment - Full Factorial",
    "description": "2ˆk Factorial Design - Another Example",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-04-21",
    "categories": [],
    "contents": "\nSource\nPreviously, I worked on a simple example for full factorial design. Let me try to replicate the results on the application of full factorial design from the link: http://pen.ius.edu.ba/index.php/pen/article/viewFile/145/175\nPackages\n\n\nlibrary(pacman)\np_load(tidyverse, AlgDesign, ggthemes, ggfortify, DoE.base, FrF2)\n\n\n\nBackground\nIn this example DOE is applied on injection-molding process with the aim of improving product quality such as excessive flash.\nFactors considered as affecting for flash formation are:\npack pressure (A),\npack time (B),\ninjection speed (C),\nscrew RPM (D),\nwhile clamping pressure, injection pressure and melting temperature were under control. Each factor affecting flash formation is considered at low and high levels.\nGenerating the design matrix\n\n\nrep_1 <- gen.factorial(c(2,2,2, 2), # number of levels for the variables\n              nVars = 4, # number of variables, in this case it is 3\n              varNames = c(\"X1_pack_pressure\", \"X2_table_speed\", \n                           \"X3_inject_speed\", \"X4_screw_rpm\"))\n\nrep_1\n\n\n   X1_pack_pressure X2_table_speed X3_inject_speed X4_screw_rpm\n1                -1             -1              -1           -1\n2                 1             -1              -1           -1\n3                -1              1              -1           -1\n4                 1              1              -1           -1\n5                -1             -1               1           -1\n6                 1             -1               1           -1\n7                -1              1               1           -1\n8                 1              1               1           -1\n9                -1             -1              -1            1\n10                1             -1              -1            1\n11               -1              1              -1            1\n12                1              1              -1            1\n13               -1             -1               1            1\n14                1             -1               1            1\n15               -1              1               1            1\n16                1              1               1            1\n\nThe design matrix may be exported out to excel for keying in of results (Y: outcome = flash size in mm, a measure of flash formation)\nAfter carrying out the experiments in randomized order, the outcome may be keyed into the same excel file and imported back into Rstudio for analysis.\nOtherwise, the results can also be keyed in manually.\n\n\n# Adding in of outcome, creating a new column called \"Y_flash\" \n\nrep_1$Y_flash <- c(0.22,\n                   6.18,\n                   0,\n                   5.91,\n                   6.6,\n                   6.05,\n                   6.76,\n                   8.65,\n                   0.46,\n                   5.06,\n                   0.55,\n                   4.84,\n                   11.55,\n                   9.9,\n                   9.9,\n                   9.9)\n\nrep_1\n\n\n   X1_pack_pressure X2_table_speed X3_inject_speed X4_screw_rpm\n1                -1             -1              -1           -1\n2                 1             -1              -1           -1\n3                -1              1              -1           -1\n4                 1              1              -1           -1\n5                -1             -1               1           -1\n6                 1             -1               1           -1\n7                -1              1               1           -1\n8                 1              1               1           -1\n9                -1             -1              -1            1\n10                1             -1              -1            1\n11               -1              1              -1            1\n12                1              1              -1            1\n13               -1             -1               1            1\n14                1             -1               1            1\n15               -1              1               1            1\n16                1              1               1            1\n   Y_flash\n1     0.22\n2     6.18\n3     0.00\n4     5.91\n5     6.60\n6     6.05\n7     6.76\n8     8.65\n9     0.46\n10    5.06\n11    0.55\n12    4.84\n13   11.55\n14    9.90\n15    9.90\n16    9.90\n\nVisualization\nPreviously, I created the main effect boxplot using ggplot2. The ggplot2 packages gives me more freedom in terms of customization, and also shows the confidence intervals. The plot below would allow me to see more clearly what is the effect of increasing each X variable independently on the outcome.\n\n\nrep_1 %>% \n  pivot_longer(cols = c(starts_with(\"X\")),\n               names_to = \"X_variables\",\n               values_to = \"X_values\") %>% \n  ggplot(aes(x = factor(X_values), y = Y_flash)) +\n  geom_boxplot(aes(x = factor(X_values), y = Y_flash, fill = X_variables)) +\n  scale_fill_few() +\n  geom_point(col = \"darkgrey\", alpha = 0.8) +\n  facet_grid(~ X_variables) +\n  labs(x = \"\",\n       title = \"Plot of the main effects showing the outcome for each factor.\",\n       caption = \"http://pen.ius.edu.ba/index.php/pen/article/viewFile/145/175\") +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n\nAn increase in X1, X3 and X4 will lead to an increase in Y. The greatest increase in Y is due to an increase in X3.\nThere is another package, FrF2, which allows me to create the main effect plot and interactions plot easily. However, the input would be a linear model, so let me build the model first.\nModelling\n\n\nmodel_interactions <- lm(Y_flash ~ (.)^4, data = rep_1) # use (.) to indicate all X variables\n\nsummary(model_interactions) \n\n\n\nCall:\nlm.default(formula = Y_flash ~ (.)^4, data = rep_1)\n\nResiduals:\nALL 16 residuals are 0: no residual degrees of freedom!\n\nCoefficients:\n                                                              Estimate\n(Intercept)                                                   5.783125\nX1_pack_pressure                                              1.278125\nX2_table_speed                                                0.030625\nX3_inject_speed                                               2.880625\nX4_screw_rpm                                                  0.736875\nX1_pack_pressure:X2_table_speed                               0.233125\nX1_pack_pressure:X3_inject_speed                             -1.316875\nX1_pack_pressure:X4_screw_rpm                                -0.373125\nX2_table_speed:X3_inject_speed                                0.108125\nX2_table_speed:X4_screw_rpm                                  -0.253125\nX3_inject_speed:X4_screw_rpm                                  0.911875\nX1_pack_pressure:X2_table_speed:X3_inject_speed               0.278125\nX1_pack_pressure:X2_table_speed:X4_screw_rpm                 -0.065625\nX1_pack_pressure:X3_inject_speed:X4_screw_rpm                -0.000625\nX2_table_speed:X3_inject_speed:X4_screw_rpm                  -0.298125\nX1_pack_pressure:X2_table_speed:X3_inject_speed:X4_screw_rpm -0.033125\n                                                             Std. Error\n(Intercept)                                                          NA\nX1_pack_pressure                                                     NA\nX2_table_speed                                                       NA\nX3_inject_speed                                                      NA\nX4_screw_rpm                                                         NA\nX1_pack_pressure:X2_table_speed                                      NA\nX1_pack_pressure:X3_inject_speed                                     NA\nX1_pack_pressure:X4_screw_rpm                                        NA\nX2_table_speed:X3_inject_speed                                       NA\nX2_table_speed:X4_screw_rpm                                          NA\nX3_inject_speed:X4_screw_rpm                                         NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed                      NA\nX1_pack_pressure:X2_table_speed:X4_screw_rpm                         NA\nX1_pack_pressure:X3_inject_speed:X4_screw_rpm                        NA\nX2_table_speed:X3_inject_speed:X4_screw_rpm                          NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed:X4_screw_rpm         NA\n                                                             t value\n(Intercept)                                                       NA\nX1_pack_pressure                                                  NA\nX2_table_speed                                                    NA\nX3_inject_speed                                                   NA\nX4_screw_rpm                                                      NA\nX1_pack_pressure:X2_table_speed                                   NA\nX1_pack_pressure:X3_inject_speed                                  NA\nX1_pack_pressure:X4_screw_rpm                                     NA\nX2_table_speed:X3_inject_speed                                    NA\nX2_table_speed:X4_screw_rpm                                       NA\nX3_inject_speed:X4_screw_rpm                                      NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed                   NA\nX1_pack_pressure:X2_table_speed:X4_screw_rpm                      NA\nX1_pack_pressure:X3_inject_speed:X4_screw_rpm                     NA\nX2_table_speed:X3_inject_speed:X4_screw_rpm                       NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed:X4_screw_rpm      NA\n                                                             Pr(>|t|)\n(Intercept)                                                        NA\nX1_pack_pressure                                                   NA\nX2_table_speed                                                     NA\nX3_inject_speed                                                    NA\nX4_screw_rpm                                                       NA\nX1_pack_pressure:X2_table_speed                                    NA\nX1_pack_pressure:X3_inject_speed                                   NA\nX1_pack_pressure:X4_screw_rpm                                      NA\nX2_table_speed:X3_inject_speed                                     NA\nX2_table_speed:X4_screw_rpm                                        NA\nX3_inject_speed:X4_screw_rpm                                       NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed                    NA\nX1_pack_pressure:X2_table_speed:X4_screw_rpm                       NA\nX1_pack_pressure:X3_inject_speed:X4_screw_rpm                      NA\nX2_table_speed:X3_inject_speed:X4_screw_rpm                        NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed:X4_screw_rpm       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 15 and 0 DF,  p-value: NA\n\nThere is a lot of NA in the model summary. That is because there is not enough degree of freedom to calculate the p-value. However, we can still use the estimates to plot the Pareto chart. The Pareto chart shows the absolute values of the estimates, from the largest effect to the smallest effect, so that you can pinpoint the factor that has the greatest effect on the outcome at a look.\nLet’s visualize:\n\n\npid::paretoPlot(model_interactions) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFrom above, X3 has the greatest effect on outcome, so it is critical that X3 is low so that Y will be minimised.\nThe Pareto chart shows the descending order of factors with an effect on the outcome. How do we tell if the factors are significant?\nAn informal way to find out is to use the half normal probability plot. Factors that are significant deviate from the straight line and will be labellled out.\n\n\nglimpse(rep_1)\n\n\nRows: 16\nColumns: 5\n$ X1_pack_pressure <dbl> -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -…\n$ X2_table_speed   <dbl> -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -…\n$ X3_inject_speed  <dbl> -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1,…\n$ X4_screw_rpm     <dbl> -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1,…\n$ Y_flash          <dbl> 0.22, 6.18, 0.00, 5.91, 6.60, 6.05, 6.76, 8…\n\nDoE.base::halfnormal(model_interactions) # DoE.base package\n\n\n\n\nFrom the plot above, X1, X3, X1X3 and X3X4 showed up as significant terms.\n\n\nFrF2::MEPlot(model_interactions)\n\n\n\n\nX2 is not a significant variable in affecting flash size, and can be removed from the model.\n\n\nmodel_three_factors <- lm(Y_flash ~ X1_pack_pressure * X3_inject_speed * X4_screw_rpm, \n                          data = rep_1)\n\ngvlma::gvlma(model_three_factors) # meets assumptions\n\n\n\nCall:\nlm.default(formula = Y_flash ~ X1_pack_pressure * X3_inject_speed * \n    X4_screw_rpm, data = rep_1)\n\nCoefficients:\n                                  (Intercept)  \n                                     5.783125  \n                             X1_pack_pressure  \n                                     1.278125  \n                              X3_inject_speed  \n                                     2.880625  \n                                 X4_screw_rpm  \n                                     0.736875  \n             X1_pack_pressure:X3_inject_speed  \n                                    -1.316875  \n                X1_pack_pressure:X4_screw_rpm  \n                                    -0.373125  \n                 X3_inject_speed:X4_screw_rpm  \n                                     0.911875  \nX1_pack_pressure:X3_inject_speed:X4_screw_rpm  \n                                    -0.000625  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma::gvlma(x = model_three_factors) \n\n                       Value p-value                Decision\nGlobal Stat        1.630e+00  0.8033 Assumptions acceptable.\nSkewness           4.984e-35  1.0000 Assumptions acceptable.\nKurtosis           1.560e+00  0.2117 Assumptions acceptable.\nLink Function      2.478e-16  1.0000 Assumptions acceptable.\nHeteroscedasticity 7.036e-02  0.7908 Assumptions acceptable.\n\nDoE.base::halfnormal(model_three_factors) # this is an informal way, and we should rely on the linear regression summary to confirm which are the significant factors. \n\n\n\nsummary(model_three_factors)\n\n\n\nCall:\nlm.default(formula = Y_flash ~ X1_pack_pressure * X3_inject_speed * \n    X4_screw_rpm, data = rep_1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -1.30  -0.11   0.00   0.11   1.30 \n\nCoefficients:\n                                               Estimate Std. Error\n(Intercept)                                    5.783125   0.194514\nX1_pack_pressure                               1.278125   0.194514\nX3_inject_speed                                2.880625   0.194514\nX4_screw_rpm                                   0.736875   0.194514\nX1_pack_pressure:X3_inject_speed              -1.316875   0.194514\nX1_pack_pressure:X4_screw_rpm                 -0.373125   0.194514\nX3_inject_speed:X4_screw_rpm                   0.911875   0.194514\nX1_pack_pressure:X3_inject_speed:X4_screw_rpm -0.000625   0.194514\n                                              t value Pr(>|t|)    \n(Intercept)                                    29.731 1.78e-09 ***\nX1_pack_pressure                                6.571 0.000175 ***\nX3_inject_speed                                14.809 4.25e-07 ***\nX4_screw_rpm                                    3.788 0.005325 ** \nX1_pack_pressure:X3_inject_speed               -6.770 0.000142 ***\nX1_pack_pressure:X4_screw_rpm                  -1.918 0.091362 .  \nX3_inject_speed:X4_screw_rpm                    4.688 0.001566 ** \nX1_pack_pressure:X3_inject_speed:X4_screw_rpm  -0.003 0.997515    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7781 on 8 degrees of freedom\nMultiple R-squared:  0.9775,    Adjusted R-squared:  0.9579 \nF-statistic: 49.76 on 7 and 8 DF,  p-value: 5.696e-06\n\n\n\npid::paretoPlot(model_three_factors) # X3 has the greatest effect on Y\n\n\n\nFrF2::MEPlot(model_three_factors) # X3 has the greatest effect on outcome\n\n\n\n\nThe three main effects that are positive are X1, X3 and X4; indicating that the molding process should be performed at low level for these three variables to minimize flash size (ie have better product quality).\nLet’s visualize the interactions\n\n\nwith(rep_1, {\ninteraction.plot(x.factor     = X1_pack_pressure,\n                 trace.factor = X3_inject_speed,\n                 response     = Y_flash,\n                 fun = mean,\n                 type=\"b\",\n                 col=c(\"black\",\"red\",\"green\"),  ### Colors for levels of trace var.\n                 pch=c(19, 17, 15),             ### Symbols for levels of trace var.\n                 fixed=TRUE,                    ### Order by factor order in data\n                 leg.bty = \"o\")\n})\n\n\n\nwith(rep_1, {\ninteraction.plot(x.factor     = X3_inject_speed,\n                 trace.factor = X4_screw_rpm,\n                 response     = Y_flash,\n                 fun = mean,\n                 type=\"b\",\n                 col=c(\"black\",\"red\",\"green\"),  ### Colors for levels of trace var.\n                 pch=c(19, 17, 15),             ### Symbols for levels of trace var.\n                 fixed=TRUE,                    ### Order by factor order in data\n                 leg.bty = \"o\")\n})\n\n\n\n\nAs mentioned earlier, the FrF2 package allows me to visualize the main effect and interactions easily,\n\n\nFrF2::IAPlot(model_three_factors)\n\n\n\n\nInteraction effects occur when the effect of one variable depends on the value of another variable. It is important to take into interactions when doing linear regression. From the model summary above, X1X3 and X3X4 are significant interaction terms. This means that the outcome, Y, depends on both X1 and X3, and X3 and X4, and not just individual factors alone. Y will increase when X3 is increased, but will increase even higher if X4 is also increased.\nLearning points\nThere are many packages that can be used for DOE in R. I used FrF2 functions this time to create main effect plots and interaction plots. The workflow is more or less similar to the previous post, and I learnt how half normal probability plots can be used as an informal way to gauge which factors are significant.\nReferences\nhttp://pen.ius.edu.ba/index.php/pen/article/viewFile/145/175\n\n\n\n",
    "preview": "posts/20210421_DOE Full Factorial Another Example/DOE-Full-Factorial---Another-Example_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-04-23T22:47:12+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210414_DOE Full Factorial/",
    "title": "Design of Experiment - Full Factorial",
    "description": "2ˆk Factorial Design - Which factors matter?",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-04-14",
    "categories": [],
    "contents": "\nIntroduction\nDesign of Experiment (DOE) is something I wished I had the chance to learn in university or at my workplace when I was younger, it would have made me a more effective scientist. I took an “Advance Design of Experiment” course with Singapore Quality Institute (SQI) last year, and had a brief introduction to how DOE works. The instructor was using Minitab for his course, and I want to translate it to something that I can do with R.\nExperimental design is important for:\nidentifying the factors which may affect the result of an experiment\ndesigning the experiment such that the effects of uncontrolled factors are minimised\nusing statistical analysis to evaluate the effects of factors involved\n((Miller and Miller 2005))\nIt is important to define clearly the purpose of the experiment before you can choose the type of experimental design.\nScreening experiments: There may be many factors involved, and in such cases, it is better to go for fractional factorial designs rather than full factorial designs.\nCharacterization experiments: The aim of such experiments is to pinpoint the effect of each factor, and to understand if there are any interactions. This is usually done after screening, and there are only 2-3 factors left for consideration. In such cases, the full factorial design may be used.\nOptimization experiments: After identifying the factors and interactions of factors, it is desired to determine the combination of factor levels that will provide the optimum response. For such cases, one would have to look at response surface methodology.\nDOE approach vs One Variable At a Time Approach\nThe advantages of using a DOE approach as compared to a OVAT approach are outlined below:(Marini 2013)\nDOE approach gives a global view of the factors, whereas OVAT approach only looks at the effect of each factor locally.\nDOE approach gives a higher quality of information.\nDOE approach is more effective and requires less number of experiments.\nDOE approach takes into account the interactions among the variables, whereas OVAT does not. Interactions means that the outcome is not solely dependent on one variable, and more than one variable would affect the outcome. For example, the best cooking time for a cookie depends on the oven temperature and size of dough. If we only looked at one variable and neglect the other, we are not studying the outcome “with a global perspective.”\nThe full factorial designs are the simplest possible designs, and as a start, I will practice with it first.\nFull Factorial Design (2ˆk)\nFull factorial design requires 2ˆk number of experiments, where k refers to the number of variables in the study. As such, it is not practical to look at more than three factors as the number of experiments will exponentially increase. This is suitable for characterization of variables, and is usually done after screening of variables.\nWorkflow:\nPlan the experiments: define the factors, ranges, number of replicates\nPerform the experiments in a randomized manner\nAnalyse the data by modelling and visualization\nLoad packages\n\n\nlibrary(pacman)\np_load(tidyverse, AlgDesign, ggfortify, jtools, ggstance, interactions,\n       DoE.base, ggthemes, pid)\n\n\n\nCase study\nLet me use the worked example in https://www.itl.nist.gov/div898/handbook/pri/section3/pri3331.htm\nOutcome: Product uniformity\nFactors: Pressure (X1), Table Speed (X2), Down force (X3) No. of replicates: 2\nTotal number of runs: 2^3 x 2 = 16 runs.\nGenerating the design plan\nAlgDesign package\nThe gen.factorial() function is easy to understand, but does not allow for creating replicates and randomizing the order.\n\n\n# from AlgDesign package\nrep_1 <- gen.factorial(c(2,2,2), # number of levels for the variables\n              nVars = 3, # number of variables, in this case it is 3\n              varNames = c(\"X1_pressure\", \"X2_table_speed\", \"X3_down_force\"))\n\nrep_1\n\n\n  X1_pressure X2_table_speed X3_down_force\n1          -1             -1            -1\n2           1             -1            -1\n3          -1              1            -1\n4           1              1            -1\n5          -1             -1             1\n6           1             -1             1\n7          -1              1             1\n8           1              1             1\n\n# to create two sets of replicates\nrep_both <- rbind(rep_1, rep_1) \n\nrep_both\n\n\n   X1_pressure X2_table_speed X3_down_force\n1           -1             -1            -1\n2            1             -1            -1\n3           -1              1            -1\n4            1              1            -1\n5           -1             -1             1\n6            1             -1             1\n7           -1              1             1\n8            1              1             1\n9           -1             -1            -1\n10           1             -1            -1\n11          -1              1            -1\n12           1              1            -1\n13          -1             -1             1\n14           1             -1             1\n15          -1              1             1\n16           1              1             1\n\n# to randomize the order of experiments, use the function: sample()\n\nglimpse(rep_both)\n\n\nRows: 16\nColumns: 3\n$ X1_pressure    <dbl> -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1,…\n$ X2_table_speed <dbl> -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1,…\n$ X3_down_force  <dbl> -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1…\n\n# randomize all rows in dataframe and rename created column as \"order\"\nrep_both$order <- sample(1:16) \n\nrep_both <- rep_both %>% \n  as_tibble() %>% \n  dplyr::select(order, everything())\n\nglimpse(rep_both)\n\n\nRows: 16\nColumns: 4\n$ order          <int> 6, 16, 13, 4, 1, 2, 3, 10, 7, 14, 8, 5, 15, 1…\n$ X1_pressure    <dbl> -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1,…\n$ X2_table_speed <dbl> -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1,…\n$ X3_down_force  <dbl> -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1…\n\nAdding outcome to the dataframe\n\n\nrep_both$outcome <- c(-3,0,-1,2,-1,2,1,6,\n                      -1,-1,0,3,0,1,1,5)\n\nrep_both\n\n\n# A tibble: 16 x 5\n   order X1_pressure X2_table_speed X3_down_force outcome\n   <int>       <dbl>          <dbl>         <dbl>   <dbl>\n 1     6          -1             -1            -1      -3\n 2    16           1             -1            -1       0\n 3    13          -1              1            -1      -1\n 4     4           1              1            -1       2\n 5     1          -1             -1             1      -1\n 6     2           1             -1             1       2\n 7     3          -1              1             1       1\n 8    10           1              1             1       6\n 9     7          -1             -1            -1      -1\n10    14           1             -1            -1      -1\n11     8          -1              1            -1       0\n12     5           1              1            -1       3\n13    15          -1             -1             1       0\n14    12           1             -1             1       1\n15     9          -1              1             1       1\n16    11           1              1             1       5\n\nVisualization\n\n\nglimpse(rep_both)\n\n\nRows: 16\nColumns: 5\n$ order          <int> 6, 16, 13, 4, 1, 2, 3, 10, 7, 14, 8, 5, 15, 1…\n$ X1_pressure    <dbl> -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1,…\n$ X2_table_speed <dbl> -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1,…\n$ X3_down_force  <dbl> -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1…\n$ outcome        <dbl> -3, 0, -1, 2, -1, 2, 1, 6, -1, -1, 0, 3, 0, 1…\n\n# boxplot for all data\n\nrep_both %>% \n  dplyr::select(-order) %>% \n  pivot_longer(cols = c(starts_with(\"X\")),\n               names_to = \"X_variables\",\n               values_to = \"X_values\") %>% \n  ggplot(aes(x = factor(X_values), y = outcome)) +\n  geom_boxplot(aes(x = factor(X_values), y = outcome, fill = X_variables)) +\n  scale_fill_few() +\n  geom_point(col = \"darkgrey\", alpha = 0.5) +\n  facet_grid(~ X_variables) +\n  labs(x = \"\",\n       title = \"Plot of the main effects showing the outcome for each factor.\",\n       caption = \"Source: http://www.itl.nist.gov/div898/handbook/\") +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n\nModelling - Multiple Linear Regression\n\n\nglimpse(rep_both)\n\n\nRows: 16\nColumns: 5\n$ order          <int> 6, 16, 13, 4, 1, 2, 3, 10, 7, 14, 8, 5, 15, 1…\n$ X1_pressure    <dbl> -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1,…\n$ X2_table_speed <dbl> -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1,…\n$ X3_down_force  <dbl> -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1…\n$ outcome        <dbl> -3, 0, -1, 2, -1, 2, 1, 6, -1, -1, 0, 3, 0, 1…\n\nmodel <- lm(outcome ~ X1_pressure * X2_table_speed * X3_down_force,\n            data = rep_both)\n\n# checking diagnostic plots for normality using ggfortify\n\nautoplot(model)\n\n\n\n\nTaken from: http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/\nThe diagnostic plots show residuals in four different ways:\nResiduals vs Fitted. Used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship, what is good.\nNormal Q-Q. Used to examine whether the residuals are normally distributed. It’s good if residuals points follow the straight dashed line.\nScale-Location (or Spread-Location). Used to check the homogeneity of variance of the residuals (homoscedasticity). Horizontal line with equally spread points is a good indication of homoscedasticity. This is not the case in our example, where we have a heteroscedasticity problem.\nResiduals vs Leverage. Used to identify influential cases, that is extreme values that might influence the regression results when included or excluded from the analysis.\nInterpretation of model results\n\n\nsummary(model)\n\n\n\nCall:\nlm.default(formula = outcome ~ X1_pressure * X2_table_speed * \n    X3_down_force, data = rep_both)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -1.0   -0.5    0.0    0.5    1.0 \n\nCoefficients:\n                                         Estimate Std. Error t value\n(Intercept)                                0.8750     0.1976   4.427\nX1_pressure                                1.3750     0.1976   6.957\nX2_table_speed                             1.2500     0.1976   6.325\nX3_down_force                              1.0000     0.1976   5.060\nX1_pressure:X2_table_speed                 0.5000     0.1976   2.530\nX1_pressure:X3_down_force                  0.2500     0.1976   1.265\nX2_table_speed:X3_down_force               0.1250     0.1976   0.632\nX1_pressure:X2_table_speed:X3_down_force   0.1250     0.1976   0.632\n                                         Pr(>|t|)    \n(Intercept)                              0.002205 ** \nX1_pressure                              0.000118 ***\nX2_table_speed                           0.000227 ***\nX3_down_force                            0.000977 ***\nX1_pressure:X2_table_speed               0.035265 *  \nX1_pressure:X3_down_force                0.241504    \nX2_table_speed:X3_down_force             0.544737    \nX1_pressure:X2_table_speed:X3_down_force 0.544737    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7906 on 8 degrees of freedom\nMultiple R-squared:  0.9388,    Adjusted R-squared:  0.8853 \nF-statistic: 17.54 on 7 and 8 DF,  p-value: 0.0002897\n\nAnother way to look at model information using jtools package\n\n\nsumm(model)\n\n\n\nObservations\n\n\n16\n\n\nDependent variable\n\n\noutcome\n\n\nType\n\n\nOLS linear regression\n\n\nF(7,8)\n\n\n17.54\n\n\nR²\n\n\n0.94\n\n\nAdj. R²\n\n\n0.89\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n(Intercept)\n\n\n0.88\n\n\n0.20\n\n\n4.43\n\n\n0.00\n\n\nX1_pressure\n\n\n1.38\n\n\n0.20\n\n\n6.96\n\n\n0.00\n\n\nX2_table_speed\n\n\n1.25\n\n\n0.20\n\n\n6.32\n\n\n0.00\n\n\nX3_down_force\n\n\n1.00\n\n\n0.20\n\n\n5.06\n\n\n0.00\n\n\nX1_pressure:X2_table_speed\n\n\n0.50\n\n\n0.20\n\n\n2.53\n\n\n0.04\n\n\nX1_pressure:X3_down_force\n\n\n0.25\n\n\n0.20\n\n\n1.26\n\n\n0.24\n\n\nX2_table_speed:X3_down_force\n\n\n0.12\n\n\n0.20\n\n\n0.63\n\n\n0.54\n\n\nX1_pressure:X2_table_speed:X3_down_force\n\n\n0.12\n\n\n0.20\n\n\n0.63\n\n\n0.54\n\n\n Standard errors: OLS\n\n\nKey points to take note of:\nThe adjusted R-squared value is 0.8853, meaning 88.5% of the variability in outcome is explained by the X variables.\nAll X variables are significant. This should be the case because for full factorial design, we only want to concentrate on characterization of important variables. If this was a screening exercise, variables that are not significant would not be important variables in the experiment.\nThe estimate shows the effect size. When there is an increase in X1, X2, X3, there will be an increase in Y. When X1 increases by 1 unit, Y will increase by 1.375 unit. When X2 increases by 1 unit, Y will increase by 1.25 unit. When X3 increases by 1 unit, Y also increases by 1 unit.\nThe interaction term between X1_pressure and X2_table speed is also significant.\nA better way to visualize is to look at the main effect plot and interaction plot.\nMain Effect Plot\n\n\nplot_summs(model)\n\n\n\n\nAnother way of looking at model coefficients using the pid package:\n\n\npid::paretoPlot(model)\n\n\n\n\nProbing interactions\n\n\ni_1 <- interact_plot(model,\n              pred = X1_pressure,\n              modx = X2_table_speed)\n\ni_2 <- interact_plot(model,\n              pred = X1_pressure,\n              modx = X3_down_force)\n\ni_3 <- interact_plot(model,\n              pred = X2_table_speed,\n              modx = X3_down_force)\n\ni_1\n\n\n\ni_2\n\n\n\ni_3\n\n\n\n\nIn this case, there is interaction, but not within the selected range for X1_pressure. When there is an increase in X1 and X2, Y will also increase.\nThere is no interaction between X1 and X3, and X2 and X3.\nConcluding remarks\nThis exercise allowed me to go through the workflow for planning and analysing the results of a simple full factorial DOE. There are many packages in the R ecosystem that allows for DOE design planning, results interpretation and visualization, and it was fun trying to explore the different functions in various packages.\nAs a next step, I will try to find a more food science related example to work on, ideally one in which there are interactions between factors.\nReferences\nhttps://www.r-bloggers.com/2009/12/design-of-experiments-%E2%80%93-full-factorial-designs/\nhttps://www.itl.nist.gov/div898/handbook/pri/section3/pri3331.htm\nhttps://www.itl.nist.gov/div898/handbook/pri/section4/pri471.htm\n\n\n\nMarini, Federico, ed. 2013. Chemometrics in Food Chemistry. First edition. Data Handling in Science and Technology, volume 28. Amsterdam ; Boston: Elsevier.\n\n\nMiller, James N., and Jane Charlotte Miller. 2005. Statistics and Chemometrics for Analytical Chemistry. Pearson/Prentice Hall.\n\n\n\n\n",
    "preview": "posts/20210414_DOE Full Factorial/DOE-Full-Factorial_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-04-15T21:04:07+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210405_Outliers/",
    "title": "Analytical Chemistry - Comparing Mean, Variance and Detecting Outliers",
    "description": "Using Statistical Tests in Analytical Chemistry",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-04-05",
    "categories": [],
    "contents": "\nBackground\nI was looking at the titration results for our existing titration method today. Our Quality colleagues wanted to minimise variability in lab measurements for a certain analyte, and there was a newly developed method with improved sample preparation steps developed by the lab in another site. We decided to carry out a study locally to check on the variability for the existing method, and also to compare with the improved method.\nMy two interns gave me a series of 5-6 readings done by each of them, and in total I was looking at 11 readings. Some questions I had were:\nWas there any difference in the readings done by the two interns?\nWere the readings different from the true value?\nOne of the readings look quite low, was that an outlier?\nAt a later stage, I would want to compare the existing method with the new method. What metrics should I be looking at?\nHow am I to find out the answers in a statistically sound manner?\nLoading packages\n\n\nlibrary(pacman)\np_load(tidyverse, outliers, ggthemes, ggstatsplot, ggpubr, car)\n\n\n\nData:\nAs I can’t put the company data up online, let me use the various worked examples I found on: http://dpuadweb.depauw.edu/harvey_web/eTextProject/pdfFiles/Chapter14.pdf.\nAll data were sourced from the link above.\nComparing Sulfanilamide analysis results done by four different analysts\nThe data below shows the determination of the percentage purity of a sulfanilamide preparation by four analysts:\n\n\na <- c(94.09, 94.64, 95.08, 94.54, 95.38, 93.62)\nb <- c(99.55, 98.24, 101.1, 100.4, 100.1, NA)\nc <- c(95.14, 94.62, 95.28, 94.59, 94.24, NA)\nd <- c(93.88, 94.23, 96.05, 93.89, 94.95, 95.49)\n\nresults <- cbind(a,b,c,d) %>% \n  as_tibble()\n\nglimpse(results)\n\n\nRows: 6\nColumns: 4\n$ a <dbl> 94.09, 94.64, 95.08, 94.54, 95.38, 93.62\n$ b <dbl> 99.55, 98.24, 101.10, 100.40, 100.10, NA\n$ c <dbl> 95.14, 94.62, 95.28, 94.59, 94.24, NA\n$ d <dbl> 93.88, 94.23, 96.05, 93.89, 94.95, 95.49\n\nLet’s reshape the data:\n\n\nreshaped_data <- results %>% \n  pivot_longer(everything(),\n               names_to = \"analyst\",\n               values_to = \"readings\") %>% \n  mutate(analyst = factor(analyst)) # factor instead of character\n\nglimpse(reshaped_data)\n\n\nRows: 24\nColumns: 2\n$ analyst  <fct> a, b, c, d, a, b, c, d, a, b, c, d, a, b, c, d, a, …\n$ readings <dbl> 94.09, 99.55, 95.14, 93.88, 94.64, 98.24, 94.62, 94…\n\nLet’s look at the mean and standard deviation for results for each analyst:\n\n\nreshaped_data %>% \n  group_by(analyst) %>% \n  summarise(mean = round(mean(readings, na.rm = T), 3),\n            sd = round(sd(readings, na.rm = T), 3)) # na.rm for working with missing data\n\n\n# A tibble: 4 x 3\n  analyst  mean    sd\n  <fct>   <dbl> <dbl>\n1 a        94.6 0.641\n2 b        99.9 1.07 \n3 c        94.8 0.428\n4 d        94.7 0.899\n\nreshaped_data %>% \n  group_by(analyst) %>% \n  summarise(mean = round(mean(readings, na.rm = T), 3)) %>% \n  ggplot(aes(x = analyst, y = mean, label = mean)) +\n  geom_col(fill = \"deepskyblue4\") +\n  geom_text(vjust = -0.2) +\n  labs(title = \"% Purity of a Sulfanilamide Preparation by Four Analysts\",\n       x = \"Analyst\",\n       y = \"Mean of replicates\",\n       caption = \"Source: http://dpuadweb.depauw.edu/harvey_web/eTextProject/pdfFiles/Chapter14.pdf\") +\n  ggthemes::theme_few()\n\n\n\n\nLet’s test if the results differ among the analysts:\n\n\nglimpse(reshaped_data)\n\n\nRows: 24\nColumns: 2\n$ analyst  <fct> a, b, c, d, a, b, c, d, a, b, c, d, a, b, c, d, a, …\n$ readings <dbl> 94.09, 99.55, 95.14, 93.88, 94.64, 98.24, 94.62, 94…\n\nm1 <- aov(readings ~analyst, data = reshaped_data)\nsummary(m1)\n\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nanalyst      3 104.20   34.73   54.66 3.05e-09 ***\nResiduals   18  11.44    0.64                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n2 observations deleted due to missingness\n\nFrom the p value of 3.05 x 10^-9, there is a significant difference between the mean values for the analysts. To find out which pair is statistically different, we use the Tukey’s Honest Significant Difference method.\n\n\nTukeyHSD(m1, which = \"analyst\", ordered = F)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = readings ~ analyst, data = reshaped_data)\n\n$analyst\n           diff       lwr       upr     p adj\nb-a  5.31966667  3.955487  6.683846 0.0000000\nc-a  0.21566667 -1.148513  1.579846 0.9693980\nd-a  0.19000000 -1.110694  1.490694 0.9755572\nc-b -5.10400000 -6.528839 -3.679161 0.0000000\nd-b -5.12966667 -6.493846 -3.765487 0.0000000\nd-c -0.02566667 -1.389846  1.338513 0.9999438\n\nLet’s show the results in a visual manner:\n\n\nplot(TukeyHSD(m1, which = \"analyst\", cex.axist = 0.5))\n\n\n\n\nThe plot above shows that the readings for Analyst B is significantly higher than the other analysts.\nI learnt that there is a package in R that can carry out modelling and visualization in 1 step, which is the ggstatsplot package.\n\n\nggbetweenstats(\n  data = reshaped_data,\n  x = analyst,\n  y = readings,\n  plot.type = \"box\", \n  type = \"p\", # parametric, non-parametric, robust or bayes\n  title = \"% Purity of a Sulfanilamide Preparation by Four Analysts\",\n  ggtheme = theme_few()\n)\n\n\n\n\nDifferent statistical tests were used for this package (Games-Howell instead of Tukey HSD). Games-Howell assumes uneven variance for the data. The plot above also shows the individual data points, which is a good practice.\nComparing acid-base titration results done by two analysts\nWhat if I wanted to compare the results of two analysts? In that case, t-test should be used in place of ANOVA.\n\n\ntitration_data <- tribble(\n  ~person_a, ~person_b,\n  86.82,      81.01,\n  87.04,      86.15,\n  86.93,      81.73,\n  87.01,      83.19,\n  86.20,      80.27,\n  87.00,      83.94\n)\n\ntitration_data\n\n\n# A tibble: 6 x 2\n  person_a person_b\n     <dbl>    <dbl>\n1     86.8     81.0\n2     87.0     86.2\n3     86.9     81.7\n4     87.0     83.2\n5     86.2     80.3\n6     87       83.9\n\n# reshape the data\n\ntitration_reshaped <- titration_data %>% \n  pivot_longer(cols = everything(),\n               names_to = \"analyst\",\n               values_to = \"readings\") %>% \n  mutate(analyst = factor(analyst))\n\ntitration_reshaped\n\n\n# A tibble: 12 x 2\n   analyst  readings\n   <fct>       <dbl>\n 1 person_a     86.8\n 2 person_b     81.0\n 3 person_a     87.0\n 4 person_b     86.2\n 5 person_a     86.9\n 6 person_b     81.7\n 7 person_a     87.0\n 8 person_b     83.2\n 9 person_a     86.2\n10 person_b     80.3\n11 person_a     87  \n12 person_b     83.9\n\ntitration_reshaped %>% \n  group_by(analyst) %>% \n  summarise(mean = mean(readings),\n           sd = sd(readings))\n\n\n# A tibble: 2 x 3\n  analyst   mean    sd\n  <fct>    <dbl> <dbl>\n1 person_a  86.8 0.320\n2 person_b  82.7 2.16 \n\n# to compare the means of results by the two analysts:\n\nt.test(readings ~ analyst, data = titration_reshaped)\n\n\n\n    Welch Two Sample t-test\n\ndata:  readings by analyst\nt = 4.6147, df = 5.219, p-value = 0.005177\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1.852919 6.383748\nsample estimates:\nmean in group person_a mean in group person_b \n              86.83333               82.71500 \n\nThe mean titration readings obtained by Person B is significantly higher than that of Person A.\n\n\nggbetweenstats(\n  data = titration_reshaped,\n  x = analyst, \n  y = readings,\n  title = \"Comparison of titration results for determining the % w/w of sodium bicarbonate in soda ash.\"\n)\n\n\n\n\nComparing Variance\nWhat if I want to compare the variance? This is useful if I want to check if method B reduces the variability of the measure.\n\n\n# Load data that compares the mass of a coin\n\nmtd_a <- c(3.080, 3.094, 3.107, 3.056, 3.112, 3.174, 3.198)\nmtd_b <- c(3.052, 3.141, 3.083, 3.083, 3.048, NA, NA)\n\ncoin_data <- cbind(mtd_a, mtd_b) %>% \n  as_tibble() %>% \n  pivot_longer(cols = everything(),\n               names_to = \"method\",\n               values_to = \"mass_g\")\n\nglimpse(coin_data)\n\n\nRows: 14\nColumns: 2\n$ method <chr> \"mtd_a\", \"mtd_b\", \"mtd_a\", \"mtd_b\", \"mtd_a\", \"mtd_b\",…\n$ mass_g <dbl> 3.080, 3.052, 3.094, 3.141, 3.107, 3.083, 3.056, 3.08…\n\n# computing the mean, standard deviation and variance:\n\ncoin_data %>% \n  group_by(method) %>% \n  summarise(mean = mean(mass_g, na.rm = T),\n            sd = sd(mass_g, na.rm = T),\n            var = var(mass_g, na.rm = T))\n\n\n# A tibble: 2 x 4\n  method  mean     sd     var\n  <chr>  <dbl>  <dbl>   <dbl>\n1 mtd_a   3.12 0.0509 0.00259\n2 mtd_b   3.08 0.0372 0.00138\n\n# use F-test to compare the variance (assuming normal distribution)\n\nvar.test(mass_g ~ method, data = coin_data, alternative = \"two.sided\")\n\n\n\n    F test to compare two variances\n\ndata:  mass_g by method\nF = 1.8726, num df = 6, denom df = 4, p-value = 0.5661\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n  0.2036028 11.6609726\nsample estimates:\nratio of variances \n          1.872598 \n\nshapiro.test(coin_data$mass_g) # p>0.05\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  coin_data$mass_g\nW = 0.90871, p-value = 0.2054\n\nbartlett.test(mass_g ~ method, data = coin_data) # for more than two groups\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  mass_g by method\nBartlett's K-squared = 0.4039, df = 1, p-value = 0.5251\n\n# if distribution is not normally distributed, the Levene test may be used:\n\ncar::leveneTest(mass_g ~ method, data = coin_data)  # same as above\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  0.4028 0.5399\n      10               \n\nAt 95% confidence interval, there is no evidence to suggest that there is a difference in precision between the two methods.\nComparing replicate readings against true, known value\nThis example is also from the same chapter cited below, in the Reference section.\nA sample is known to have 98.76% sodium bicarbonate. Five replicate measurements were taken, and we want to find out if the analysis is giving inaccurate results.\n\n\nbicarb <- c(98.71, 98.59, 98.62, 98.44, 98.58)\nmean(bicarb)\n\n\n[1] 98.588\n\nsd(bicarb)\n\n\n[1] 0.09731393\n\n# visualize\nshapiro.test(bicarb) # follows normal distribution\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  bicarb\nW = 0.94407, p-value = 0.6949\n\nggqqplot(bicarb) # Q-Q plot\n\n\n\n# boxplot\nggboxplot(bicarb,\n                  ylab = \"readings\")\n\n\n\nt.test(bicarb, mu = 98.76, alternative = \"two.sided\")\n\n\n\n    One Sample t-test\n\ndata:  bicarb\nt = -3.9522, df = 4, p-value = 0.01679\nalternative hypothesis: true mean is not equal to 98.76\n95 percent confidence interval:\n 98.46717 98.70883\nsample estimates:\nmean of x \n   98.588 \n\nThe data suggests that the experimental data is significantly different from the known value, and that there is indeed a source of error when conducting the experiments.\nOutlier Tests\nWhen there are data-points that appear not to be consistent with the other data points, how do you determine if it is an outlier?\nOne way is by visualizing using the box-plot, and an outlier is defined as a point outside of the inter-quartile range (1.5 x IQR).\nThere are some significance tests that can be used to identify outliers, which include the Dixon’s Q-Test (not recommended by ISO), the Grubb’s Test (can be carried out using the outliers package) and the Chauvenet’s Criterion. I haven’t found a package which I can check for outliers using Chauvenet’s Criterion, and will only use the Grubb’s test below.\nGrubb’s test\nThe Grubbs test is a test used to detect outliers (assuming normally distributed data).\nUsing penny weight dataset shown below, it is of interest to test if a penny with a mass of 2.514g is an outlier datapoint.\n\n\npennies <- c(3.067, 3.049, 2.514, 3.048, 3.079, 3.094, 3.109, 3.102)\n\n# Boxplot\n\nshapiro.test(pennies) # not normally distributed (p<0.05), use grubb's test with caveat\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  pennies\nW = 0.52689, p-value = 2.172e-05\n\n# Grubbs test\ngrubbs.test(pennies, type = 10, two.sided = T)\n\n\n\n    Grubbs test for one outlier\n\ndata:  pennies\nG = 2.45880, U = 0.01295, p-value = 5.456e-06\nalternative hypothesis: lowest value 2.514 is an outlier\n\nThe null hypothesis is that there is no outlier, and the alternative hypothesis is that there is an outlier. In this case, as p<0.05, 2.514 is indeed an outlier datapoint.\nLet us check again using a box-plot visualization:\n\n\nggboxplot(pennies) +\n  labs(title = \"Using the 1.5IQR rule, 2.514g penny is an outlier datapoint.\")\n\n\n\n\nLearning Points\nAll my questions I had at the beginning were answered in statistically sound manner. I got to revise the t-test, ANOVA, learnt how to carry out modelling and visualization in one step. I also deepened my analytical chemistry results interpretation by knowing whether to compare the mean or to comare the variance when assessing usability of newly developed methods.\nI learnt a new way to detect for outliers, which is the Grubb’s test. However, I am on the lookout for ways to test for outlier using the Chauvenet’s Criterion.\nReferences\nhttp://dpuadweb.depauw.edu/harvey_web/eTextProject/pdfFiles/Chapter14.pdf\nhttp://www.sthda.com/english/wiki/one-sample-t-test-in-r\nhttp://www.sthda.com/english/wiki/f-test-compare-two-variances-in-r\n\n\n\n",
    "preview": "posts/20210405_Outliers/Outliers_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-04-07T22:13:44+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210328_NIR Meat Data (PLS-regression)/",
    "title": "Meat NIR data",
    "description": "PLS regression on Meat NIR data",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-03-18",
    "categories": [],
    "contents": "\nBackground\nThis example provides the workflow for analysing NIR data spectra to predict the fat, water and protein content of meat.\nWorkflow\nExploratory Data Analysis\nPCA for dimension reduction/exploratory to determine the effective dimension of the dataset.\nSplit dataset into TRAINING and TESTING dataset. Split training dataset into cross validation dataset.\nData processing for training dataset, with the understanding the PLS regression will be used for modeling. Data should be centered and scaled, especially if the predictors are on scales of different magnitude.\nBuild model: In this case, PLS-regression will be used, due to high correlation among X variables; and also because the number of X variables is greater than the number of samples.\nAssess model for cross-validation dataset, and tune to select optimal number of PC to be retained. Resampling techniques include k-fold cross validation, Leave One Out (LOO) cross validation and bootstrapping. For large datasets, 10-fold CV would suffice. For small datasets, repeated 10-fold CV is recommended. For very small datasets (n<50), LOO is recommended.\nAssess model on TESTING dataset\nVisualization of model performance - number of components to be retained and model coefficients, observed vs predicted values (indicator of accuracy)\nFor models with more than 1 Y variable, the PLS1 approach is taken: each Y outcome has its own tuned model.\nReporting model coefficients\nAssessing variable importance, using variable importance scores. As a rule of thumb, VIP values exceeding 1 are considered to contain predictive information for the outcome, Y.\nPLS regression - Theory\nOne of the many advantages of PLS is that it can handle many noisy, collinear (correlated) and missing variables, and can also simultaneously model several response variables Y.\nThe default algorithm is the NIPALS algorithm, which seeks to find the latent (or hidden) relationships among the X variables, which are highly correlated with the response (Y outcome).\nLike PCA, PLS finds linear combinations of the predictors. These linear combinations are commonly called components, or latent variables. PLS linear combinations of X variables are chosen to maximally summarise covariance with the Y outcomes. This means that PLS finds components that maximally summarise the variation of the predictors (X) while simultaneously requiring these components to have maximum correlation with the response (Y). PLS is a supervised dimension reduction procedure, as compared to Principal Component Regression (PCR), which is an unsupervised procedure. PLS will identify the optimal predictor space dimension reduction for the purpose of regression with the outcome, Y. For PCA, the variance in X is captured as much as possible, but there may be cases in which variation in X is not related to variation in Y, or there may be high noise present, and in turn is not related to Y. Hence, Y cannot be predicted correctly since there is no relationship.\nThe NIPALS algorithm is suitable for large datasets (when number of samples, n is much greater than number of predictors, X). For scenarios where there are more predictors than samples, the algorithm by Rannar, which is a kernel based algorithm, is computationally more efficient.\nThere are two versions of PLS - PLS 1 and PLS 2. PLS 1 is used when there is only one Y outcome variable, and PLS 2 may be used when there are more than one Y outcome variables. In PLS 1, a model is build and tuned specific for a Y outcome variabe. In PLS 2, the model is tuned for more than one Y outcomes. It is preferred to tune a model specific for each Y outcome rather in practice.\nIR\nIR spectroscopy is used to determine the chemical makeup of a substance. The theory of IR spectroscopy holds that unique molecualr structures absorb IR frequencies differently. In practice, a spectrometer fires a series of IR frequencies into a sample material, and the devide measures the absorbance of the sample at each individual frequency. This series of measurements creates a spectrum profile which can then be used to determine the chemical makeup of the sample material.\nData\nA Tecator Infratec Food and Feed Analyzer instrument was used to analyse 215 samples of meat across 100 frequencies (800-1050 nm). In addition to an IR profile, the percent content of water, fat and protein for each sample was determined using analytical chemistry.\nThe objective is the establish a predictive relationship between IR spectrum and fat content, to predict a sample’s fat content with IR.\nPackages required\n\n\n# Load packages: \n\nlibrary(pacman)\n\np_load(pls, AppliedPredictiveModeling, tidyverse, modeldata, tidymodels,  janitor, skimr, caret, ggthemes, ggrepel, psych, GGally)\n\n\n\nImport data\nThese data are recorded on a Tecator Infratec Food and Feed Analyzer working in the wavelength range 850 - 1050 nm by the Near Infrared Transmission (NIT) principle. Each sample contains finely chopped pure meat with different moisture, fat and protein contents.\nFor each meat sample the data consists of a 100 channel spectrum of absorbances and the contents of moisture (water), fat and protein. The absorbance is -log10 of the transmittance measured by the spectrometer. The three contents, measured in percent, are determined by analytic chemistry.\n\n\ndata(tecator)\n\n\n\nThe matrix absorp contains the 100 absorbance values for the 215 samples The matrix endpoints contains the percent of moisture, fat, and protein in columns 1, 2, 3 respectively.\n\n\nwavelengths = seq(800,1050, length.out = 100)\n\nmatplot(wavelengths, t(absorp), type = \"l\", lty = 1,\n        ylab = \"Absorbance (-log 10 of transmittance)\")\n\n\n\n\nPCA - the base R way\n\n\nnir.prcomp <- prcomp(absorp, rank. = 8)\n\nsummary(nir.prcomp)\n\n\nImportance of first k=8 (out of 100) components:\n                          PC1     PC2     PC3     PC4     PC5     PC6\nStandard deviation     5.1115 0.48840 0.28009 0.17374 0.03903 0.02581\nProportion of Variance 0.9868 0.00901 0.00296 0.00114 0.00006 0.00003\nCumulative Proportion  0.9868 0.99580 0.99876 0.99990 0.99996 0.99999\n                           PC7     PC8\nStandard deviation     0.01433 0.01041\nProportion of Variance 0.00001 0.00000\nCumulative Proportion  0.99999 1.00000\n\nOne can see that the first component explains 98.7% of variance in X.\nScree plot\n\n\nplot(nir.prcomp, main = \"NIR Meat PCA scree plot\",\n     xlab = \"No. of PC\")\n\n\n\n\nOne can choose number of PC = 2\nLoadings\n\n\nnir.loadings <- nir.prcomp$rotation[, 1:2]\n\n# find max loading for PC 1\nnir.loadings %>% \n  as_tibble() %>% \n  rowid_to_column() %>% \n  pivot_longer(cols = starts_with(\"PC\"),\n               names_to = \"PC\",\n               values_to = \"loadings\") %>% \n  dplyr::filter(PC == \"PC1\") %>% \n  mutate(abs_loading = abs(loadings)) %>% \n  arrange(desc(abs_loading)) %>% \n  head(n = 10)\n\n\n# A tibble: 10 x 4\n   rowid PC    loadings abs_loading\n   <int> <chr>    <dbl>       <dbl>\n 1    42 PC1      0.106       0.106\n 2    41 PC1      0.106       0.106\n 3    43 PC1      0.106       0.106\n 4    67 PC1      0.106       0.106\n 5    68 PC1      0.106       0.106\n 6    69 PC1      0.106       0.106\n 7    66 PC1      0.106       0.106\n 8    70 PC1      0.106       0.106\n 9    84 PC1      0.106       0.106\n10    71 PC1      0.106       0.106\n\n# find max loading for PC 2\nnir.loadings %>% \n  as_tibble() %>% \n  rowid_to_column() %>% \n  pivot_longer(cols = starts_with(\"PC\"),\n               names_to = \"PC\",\n               values_to = \"loadings\") %>% \n  dplyr::filter(PC == \"PC2\") %>% \n  mutate(abs_loading = abs(loadings)) %>% \n  arrange(desc(abs_loading)) %>% \n  head(n = 10)\n\n\n# A tibble: 10 x 4\n   rowid PC    loadings abs_loading\n   <int> <chr>    <dbl>       <dbl>\n 1    14 PC2      0.129       0.129\n 2    15 PC2      0.129       0.129\n 3    13 PC2      0.129       0.129\n 4    16 PC2      0.129       0.129\n 5    12 PC2      0.128       0.128\n 6    17 PC2      0.128       0.128\n 7    11 PC2      0.128       0.128\n 8    18 PC2      0.127       0.127\n 9    10 PC2      0.127       0.127\n10     9 PC2      0.126       0.126\n\n# check which wavelengths\n\nwavelengths[42]\n\n\n[1] 903.5354\n\nwavelengths[68]\n\n\n[1] 969.1919\n\nwavelengths[14]\n\n\n[1] 832.8283\n\nwavelengths[17]\n\n\n[1] 840.404\n\n# Plot\noffset <- c(0, 0.009)\nplot(nir.loadings[, 1:2], type = \"l\",\n     xlim = range(nir.loadings[, 1]) + offset,\n     xlab = \"PC 1 (98.7%)\", ylab = \"PC 2 (0.009%)\")\n\npoints(nir.loadings[c(42,68, 14, 17), 1:2])\n\ntext(nir.loadings[c(42,68, 14, 17), 1:2], pos = 4,\n     labels = paste(c(903,969,832,840), \"nm\"))\n\n\n\n\nTidymodels way for PCA\nI personally prefer this method because the plots can be customised, rather than using the default base R graphics.\n\n\n# Y\nendpoints.tibble <- endpoints %>% as_tibble(.name_repair = \"unique\")\n\n# set names for Y\nnames(endpoints.tibble) <- c(\"water\", \"fat\", \"protein\")\n\n\n# X\nabsorp.tibble <- absorp %>% as_tibble(.name_repair = \"unique\") %>% \n  clean_names()\n\n# combine both datasets\ndata_meat_x <- absorp.tibble\ndata_meat_y <- endpoints.tibble\n\n# combine both datasets\ndata_meat <- cbind(endpoints.tibble, absorp.tibble) %>% \n  mutate(id = seq(1:215))\n\n\n\nChecking assumptions for PCA\n\n\n# Checking assumptions ----\n\ndata_meat_x %>% \n  cor() %>% \n  KMO() # Overal MSA = 0.97, greater than 0.70\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = .)\nOverall MSA =  0.97\nMSA for each item = \n  x1   x2   x3   x4   x5   x6   x7   x8   x9  x10  x11  x12  x13  x14 \n0.99 0.98 0.97 0.97 0.96 0.96 0.96 0.98 0.98 0.98 0.97 0.96 0.96 0.96 \n x15  x16  x17  x18  x19  x20  x21  x22  x23  x24  x25  x26  x27  x28 \n0.97 0.98 0.98 0.99 0.98 0.96 0.96 0.96 0.96 0.96 0.97 0.99 0.98 0.98 \n x29  x30  x31  x32  x33  x34  x35  x36  x37  x38  x39  x40  x41  x42 \n0.97 0.97 0.97 0.97 0.97 0.97 0.98 0.98 0.98 0.98 0.98 0.98 0.97 0.96 \n x43  x44  x45  x46  x47  x48  x49  x50  x51  x52  x53  x54  x55  x56 \n0.96 0.97 0.97 0.98 0.98 0.98 0.98 0.97 0.97 0.97 0.96 0.96 0.96 0.96 \n x57  x58  x59  x60  x61  x62  x63  x64  x65  x66  x67  x68  x69  x70 \n0.97 0.97 0.98 0.98 0.98 0.98 0.98 0.98 0.97 0.96 0.96 0.96 0.97 0.97 \n x71  x72  x73  x74  x75  x76  x77  x78  x79  x80  x81  x82  x83  x84 \n0.97 0.98 0.98 0.97 0.98 0.98 0.97 0.98 0.98 0.98 0.98 0.98 0.97 0.97 \n x85  x86  x87  x88  x89  x90  x91  x92  x93  x94  x95  x96  x97  x98 \n0.97 0.97 0.97 0.98 0.97 0.96 0.96 0.97 0.97 0.98 0.98 0.98 0.98 0.98 \n x99 x100 \n0.97 0.97 \n\ndata_meat_x %>% \n  cor() %>% \n  cortest.bartlett(., n = 215) # p < 0.05\n\n\n$chisq\n[1] Inf\n\n$p.value\n[1] 0\n\n$df\n[1] 4950\n\n# all assumptions met, ok to do PCA\n\n\n\nModel\n\n\n# tidymodels PCA -----\n\n# recipe\nmeat_pca_recipe <- recipe(~ ., data = data_meat) %>% \n  update_role(id, new_role = \"id\") %>% \n  update_role(water, fat, protein, new_role = \"outcome\") %>% \n  step_normalize(all_predictors()) %>% \n  step_pca(all_predictors(), id = \"pca\")\n\nmeat_pca_recipe\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          3\n predictor        100\n\nOperations:\n\nCentering and scaling for all_predictors()\nNo PCA components were extracted.\n\n# prep\n\nmeat_pca_prep <- prep(meat_pca_recipe)\n\n# loadings\ntidy_pca_loadings <- meat_pca_prep %>% \n  tidy(id = \"pca\")\n\ntidy_pca_loadings\n\n\n# A tibble: 10,000 x 4\n   terms  value component id   \n   <chr>  <dbl> <chr>     <chr>\n 1 x1    0.0997 PC1       pca  \n 2 x2    0.0997 PC1       pca  \n 3 x3    0.0997 PC1       pca  \n 4 x4    0.0997 PC1       pca  \n 5 x5    0.0997 PC1       pca  \n 6 x6    0.0996 PC1       pca  \n 7 x7    0.0996 PC1       pca  \n 8 x8    0.0997 PC1       pca  \n 9 x9    0.0997 PC1       pca  \n10 x10   0.0997 PC1       pca  \n# … with 9,990 more rows\n\n# bake\nmeat_pca_bake <- bake(meat_pca_prep, data_meat)\nmeat_pca_bake # PCA LOADING VECTORS\n\n\n# A tibble: 215 x 9\n   water   fat protein    id    PC1    PC2     PC3      PC4     PC5\n   <dbl> <dbl>   <dbl> <int>  <dbl>  <dbl>   <dbl>    <dbl>   <dbl>\n 1  60.5  22.5    16.7     1 -4.30  -0.404 -0.137  -0.0939   0.105 \n 2  46    40.1    13.5     2  0.998  0.486 -1.20   -0.0169   0.0303\n 3  71     8.4    20.5     3 -7.14   1.41   0.155   0.244   -0.0815\n 4  72.8   5.9    20.7     4 -1.81   1.15   0.544   0.00581  0.128 \n 5  58.3  25.5    15.5     5  0.988 -1.19  -0.0591 -0.0248   0.101 \n 6  44    42.7    13.7     6  5.56   0.226 -1.24   -0.0877   0.0470\n 7  44    42.7    13.7     7  4.96   0.590 -1.16    0.129   -0.129 \n 8  69.3  10.6    19.3     8 -6.31  -0.625  0.166  -0.144    0.0222\n 9  61.4  19.9    17.7     9 11.6   -0.354  0.185  -0.269    0.0274\n10  61.4  19.9    17.7    10 14.6   -0.274  0.459  -0.0457  -0.111 \n# … with 205 more rows\n\n# Check number of PC: ------\n\n# Check number of PCs by eigenvalues\n\nmeat_pca_prep$steps[[2]]$res$sdev %>% as_tibble() %>% \n  filter(value>1) # only 1 PC\n\n\n# A tibble: 1 x 1\n  value\n  <dbl>\n1  9.93\n\n# check using scree plots\n\nproportion_scree_plot <- meat_pca_prep %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms == \"percent variance\") %>% \n  ggplot(aes(component, value, label = value)) +\n  geom_point(size = 2) +\n  geom_line(size = 1) +\n  geom_text(aes(label = round(value, 2)), vjust = -0.2, size = 3) +\n  labs(title = \"% Variance explained\",\n       y = \"% total variance\",\n       x = \"PC\",\n       subtitle = \"98.63 % of variance is explained in PC 1\",\n       caption = \"Source: Tecator Dataset {caret}\") +\n  theme_few() +\n  theme(axis.title = element_text(face = \"bold\", size = 12),\n        axis.text = element_text(size = 10),\n        plot.title = element_text(size = 14, face = \"bold\"))\n\nproportion_scree_plot\n\n\n\n\nPLS regression\nChange dataset into a tibble format\nAlternatively, another ready-to-use format of date may be imported.\n\n\ndata(meats) # from modeldata package\nmeats\n\n\n# A tibble: 215 x 103\n   x_001 x_002 x_003 x_004 x_005 x_006 x_007 x_008 x_009 x_010 x_011\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  2.62  2.62  2.62  2.62  2.62  2.62  2.62  2.62  2.63  2.63  2.63\n 2  2.83  2.84  2.84  2.85  2.85  2.86  2.86  2.87  2.87  2.88  2.88\n 3  2.58  2.58  2.59  2.59  2.59  2.59  2.59  2.60  2.60  2.60  2.60\n 4  2.82  2.82  2.83  2.83  2.83  2.83  2.83  2.84  2.84  2.84  2.84\n 5  2.79  2.79  2.79  2.79  2.80  2.80  2.80  2.80  2.81  2.81  2.81\n 6  3.01  3.02  3.02  3.03  3.03  3.04  3.04  3.05  3.06  3.06  3.07\n 7  2.99  2.99  3.00  3.01  3.01  3.02  3.02  3.03  3.04  3.04  3.05\n 8  2.53  2.53  2.53  2.53  2.53  2.53  2.53  2.53  2.54  2.54  2.54\n 9  3.27  3.28  3.29  3.29  3.30  3.31  3.31  3.32  3.33  3.33  3.34\n10  3.40  3.41  3.41  3.42  3.43  3.43  3.44  3.45  3.46  3.47  3.48\n# … with 205 more rows, and 92 more variables: x_012 <dbl>,\n#   x_013 <dbl>, x_014 <dbl>, x_015 <dbl>, x_016 <dbl>, x_017 <dbl>,\n#   x_018 <dbl>, x_019 <dbl>, x_020 <dbl>, x_021 <dbl>, x_022 <dbl>,\n#   x_023 <dbl>, x_024 <dbl>, x_025 <dbl>, x_026 <dbl>, x_027 <dbl>,\n#   x_028 <dbl>, x_029 <dbl>, x_030 <dbl>, x_031 <dbl>, x_032 <dbl>,\n#   x_033 <dbl>, x_034 <dbl>, x_035 <dbl>, x_036 <dbl>, x_037 <dbl>,\n#   x_038 <dbl>, x_039 <dbl>, x_040 <dbl>, x_041 <dbl>, x_042 <dbl>,\n#   x_043 <dbl>, x_044 <dbl>, x_045 <dbl>, x_046 <dbl>, x_047 <dbl>,\n#   x_048 <dbl>, x_049 <dbl>, x_050 <dbl>, x_051 <dbl>, x_052 <dbl>,\n#   x_053 <dbl>, x_054 <dbl>, x_055 <dbl>, x_056 <dbl>, x_057 <dbl>,\n#   x_058 <dbl>, x_059 <dbl>, x_060 <dbl>, x_061 <dbl>, x_062 <dbl>,\n#   x_063 <dbl>, x_064 <dbl>, x_065 <dbl>, x_066 <dbl>, x_067 <dbl>,\n#   x_068 <dbl>, x_069 <dbl>, x_070 <dbl>, x_071 <dbl>, x_072 <dbl>,\n#   x_073 <dbl>, x_074 <dbl>, x_075 <dbl>, x_076 <dbl>, x_077 <dbl>,\n#   x_078 <dbl>, x_079 <dbl>, x_080 <dbl>, x_081 <dbl>, x_082 <dbl>,\n#   x_083 <dbl>, x_084 <dbl>, x_085 <dbl>, x_086 <dbl>, x_087 <dbl>,\n#   x_088 <dbl>, x_089 <dbl>, x_090 <dbl>, x_091 <dbl>, x_092 <dbl>,\n#   x_093 <dbl>, x_094 <dbl>, x_095 <dbl>, x_096 <dbl>, x_097 <dbl>,\n#   x_098 <dbl>, x_099 <dbl>, x_100 <dbl>, water <dbl>, fat <dbl>,\n#   protein <dbl>\n\nSplit into training and testing dataset\nNote: preprocessing of data, should be done only for TRAINING and not the whole dataset to ensure independence of training and testing dataset.\n\n\nset.seed(20210318)\n\nmeat_split <- initial_split(meats, prop = 0.80)\n\nmeat_training <- meat_split %>% training()\n\nmeat_testing <- meat_split %>% testing()\n\n\n\nEDA\nJust to check the data to see preprocessing steps required.\n\n\nskim(meat_training)\n\n\nTable 1: Data summary\nName\nmeat_training\nNumber of rows\n173\nNumber of columns\n103\n_______________________\n\nColumn type frequency:\n\nnumeric\n103\n________________________\n\nGroup variables\nNone\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nx_001\n0\n1\n2.80\n0.40\n2.07\n2.53\n2.74\n2.99\n4.22\n▃▇▃▁▁\nx_002\n0\n1\n2.80\n0.40\n2.07\n2.53\n2.74\n3.00\n4.23\n▃▇▃▁▁\nx_003\n0\n1\n2.81\n0.40\n2.07\n2.53\n2.74\n3.01\n4.24\n▃▇▃▁▁\nx_004\n0\n1\n2.81\n0.41\n2.06\n2.53\n2.74\n3.01\n4.25\n▃▇▃▁▁\nx_005\n0\n1\n2.81\n0.41\n2.06\n2.53\n2.74\n3.02\n4.26\n▃▇▃▁▁\nx_006\n0\n1\n2.81\n0.41\n2.06\n2.52\n2.75\n3.03\n4.27\n▃▇▃▁▁\nx_007\n0\n1\n2.82\n0.41\n2.06\n2.53\n2.75\n3.03\n4.28\n▃▇▃▁▁\nx_008\n0\n1\n2.82\n0.42\n2.06\n2.53\n2.75\n3.04\n4.29\n▃▇▃▂▁\nx_009\n0\n1\n2.82\n0.42\n2.06\n2.53\n2.76\n3.05\n4.30\n▃▇▃▂▁\nx_010\n0\n1\n2.83\n0.42\n2.06\n2.53\n2.76\n3.05\n4.31\n▃▇▃▂▁\nx_011\n0\n1\n2.83\n0.42\n2.06\n2.53\n2.77\n3.06\n4.33\n▃▇▃▂▁\nx_012\n0\n1\n2.84\n0.43\n2.06\n2.54\n2.77\n3.07\n4.34\n▃▇▃▂▁\nx_013\n0\n1\n2.84\n0.43\n2.06\n2.54\n2.78\n3.07\n4.35\n▃▇▃▂▁\nx_014\n0\n1\n2.85\n0.43\n2.06\n2.54\n2.78\n3.08\n4.37\n▃▇▃▂▁\nx_015\n0\n1\n2.85\n0.43\n2.07\n2.55\n2.78\n3.09\n4.38\n▃▇▃▂▁\nx_016\n0\n1\n2.86\n0.44\n2.07\n2.55\n2.79\n3.10\n4.40\n▃▇▃▁▁\nx_017\n0\n1\n2.86\n0.44\n2.07\n2.55\n2.79\n3.11\n4.42\n▅▇▃▁▁\nx_018\n0\n1\n2.87\n0.44\n2.07\n2.56\n2.80\n3.12\n4.43\n▅▇▃▁▁\nx_019\n0\n1\n2.88\n0.45\n2.07\n2.56\n2.80\n3.13\n4.45\n▅▇▃▁▁\nx_020\n0\n1\n2.89\n0.45\n2.07\n2.57\n2.81\n3.14\n4.47\n▅▇▅▁▁\nx_021\n0\n1\n2.90\n0.45\n2.07\n2.57\n2.81\n3.15\n4.49\n▅▇▅▁▁\nx_022\n0\n1\n2.90\n0.46\n2.08\n2.57\n2.82\n3.16\n4.51\n▅▇▅▁▁\nx_023\n0\n1\n2.91\n0.46\n2.08\n2.58\n2.82\n3.17\n4.53\n▅▇▅▁▁\nx_024\n0\n1\n2.92\n0.46\n2.08\n2.59\n2.83\n3.17\n4.55\n▅▇▅▁▁\nx_025\n0\n1\n2.93\n0.47\n2.08\n2.59\n2.84\n3.18\n4.57\n▅▇▅▁▁\nx_026\n0\n1\n2.93\n0.47\n2.09\n2.60\n2.85\n3.19\n4.59\n▅▇▅▁▁\nx_027\n0\n1\n2.94\n0.47\n2.09\n2.60\n2.86\n3.20\n4.61\n▅▇▅▁▁\nx_028\n0\n1\n2.95\n0.48\n2.09\n2.61\n2.87\n3.21\n4.63\n▅▇▅▁▁\nx_029\n0\n1\n2.96\n0.48\n2.09\n2.61\n2.88\n3.22\n4.65\n▅▇▅▁▁\nx_030\n0\n1\n2.97\n0.48\n2.09\n2.62\n2.88\n3.24\n4.68\n▅▇▅▂▁\nx_031\n0\n1\n2.98\n0.49\n2.10\n2.62\n2.90\n3.26\n4.70\n▅▇▅▂▁\nx_032\n0\n1\n2.99\n0.49\n2.10\n2.63\n2.91\n3.28\n4.73\n▅▇▅▂▁\nx_033\n0\n1\n3.00\n0.50\n2.10\n2.64\n2.91\n3.31\n4.76\n▅▇▅▂▁\nx_034\n0\n1\n3.01\n0.50\n2.10\n2.65\n2.93\n3.33\n4.78\n▅▇▅▂▁\nx_035\n0\n1\n3.03\n0.51\n2.11\n2.66\n2.94\n3.36\n4.81\n▅▇▅▂▁\nx_036\n0\n1\n3.04\n0.51\n2.11\n2.68\n2.94\n3.39\n4.84\n▅▇▅▂▁\nx_037\n0\n1\n3.05\n0.52\n2.12\n2.69\n2.95\n3.42\n4.87\n▅▇▅▂▁\nx_038\n0\n1\n3.07\n0.52\n2.13\n2.70\n2.97\n3.45\n4.90\n▅▇▅▂▁\nx_039\n0\n1\n3.09\n0.52\n2.14\n2.72\n2.99\n3.47\n4.93\n▅▇▅▂▁\nx_040\n0\n1\n3.11\n0.53\n2.15\n2.73\n3.01\n3.48\n4.96\n▅▇▅▂▁\nx_041\n0\n1\n3.12\n0.53\n2.17\n2.74\n3.03\n3.50\n4.98\n▅▇▅▂▁\nx_042\n0\n1\n3.14\n0.53\n2.18\n2.76\n3.04\n3.52\n5.00\n▅▇▅▂▁\nx_043\n0\n1\n3.15\n0.53\n2.20\n2.78\n3.05\n3.53\n5.01\n▅▇▅▂▁\nx_044\n0\n1\n3.16\n0.53\n2.22\n2.79\n3.06\n3.52\n5.02\n▅▇▅▂▁\nx_045\n0\n1\n3.17\n0.52\n2.24\n2.80\n3.08\n3.50\n5.02\n▅▇▅▂▁\nx_046\n0\n1\n3.19\n0.52\n2.27\n2.82\n3.10\n3.49\n5.02\n▅▇▅▁▁\nx_047\n0\n1\n3.20\n0.51\n2.29\n2.84\n3.10\n3.50\n5.03\n▅▇▅▁▁\nx_048\n0\n1\n3.22\n0.51\n2.32\n2.86\n3.12\n3.50\n5.04\n▅▇▅▁▁\nx_049\n0\n1\n3.25\n0.51\n2.35\n2.89\n3.15\n3.52\n5.06\n▅▇▅▁▁\nx_050\n0\n1\n3.28\n0.51\n2.39\n2.93\n3.19\n3.55\n5.09\n▅▇▅▁▁\nx_051\n0\n1\n3.32\n0.51\n2.43\n2.97\n3.22\n3.58\n5.13\n▅▇▅▁▁\nx_052\n0\n1\n3.36\n0.51\n2.48\n3.01\n3.27\n3.61\n5.17\n▅▇▃▂▁\nx_053\n0\n1\n3.41\n0.51\n2.53\n3.05\n3.32\n3.65\n5.23\n▅▇▃▁▁\nx_054\n0\n1\n3.45\n0.52\n2.57\n3.09\n3.37\n3.70\n5.28\n▅▇▃▁▁\nx_055\n0\n1\n3.50\n0.52\n2.61\n3.13\n3.41\n3.74\n5.33\n▅▇▃▁▁\nx_056\n0\n1\n3.53\n0.52\n2.65\n3.16\n3.45\n3.78\n5.37\n▅▇▃▁▁\nx_057\n0\n1\n3.56\n0.53\n2.67\n3.19\n3.48\n3.81\n5.41\n▅▇▃▁▁\nx_058\n0\n1\n3.58\n0.53\n2.69\n3.21\n3.51\n3.83\n5.43\n▅▇▃▁▁\nx_059\n0\n1\n3.59\n0.53\n2.71\n3.22\n3.53\n3.84\n5.44\n▅▇▃▁▁\nx_060\n0\n1\n3.60\n0.53\n2.72\n3.23\n3.54\n3.85\n5.45\n▅▇▃▁▁\nx_061\n0\n1\n3.61\n0.53\n2.72\n3.24\n3.55\n3.86\n5.46\n▅▇▃▁▁\nx_062\n0\n1\n3.61\n0.53\n2.73\n3.25\n3.55\n3.86\n5.47\n▆▇▃▁▁\nx_063\n0\n1\n3.61\n0.53\n2.73\n3.25\n3.56\n3.87\n5.47\n▆▇▃▁▁\nx_064\n0\n1\n3.62\n0.53\n2.73\n3.24\n3.56\n3.87\n5.47\n▆▇▃▁▁\nx_065\n0\n1\n3.61\n0.53\n2.73\n3.24\n3.55\n3.87\n5.47\n▆▇▃▁▁\nx_066\n0\n1\n3.61\n0.53\n2.73\n3.24\n3.55\n3.86\n5.47\n▆▇▃▁▁\nx_067\n0\n1\n3.60\n0.53\n2.73\n3.23\n3.54\n3.85\n5.47\n▆▇▃▁▁\nx_068\n0\n1\n3.60\n0.53\n2.72\n3.22\n3.53\n3.83\n5.46\n▆▇▃▁▁\nx_069\n0\n1\n3.59\n0.53\n2.72\n3.20\n3.52\n3.82\n5.45\n▆▇▃▁▁\nx_070\n0\n1\n3.58\n0.53\n2.71\n3.19\n3.51\n3.82\n5.44\n▆▇▃▁▁\nx_071\n0\n1\n3.56\n0.53\n2.70\n3.18\n3.50\n3.81\n5.43\n▆▇▃▁▁\nx_072\n0\n1\n3.55\n0.53\n2.69\n3.17\n3.49\n3.80\n5.42\n▆▇▃▁▁\nx_073\n0\n1\n3.54\n0.53\n2.68\n3.16\n3.48\n3.79\n5.40\n▆▇▃▁▁\nx_074\n0\n1\n3.52\n0.53\n2.66\n3.14\n3.46\n3.78\n5.39\n▆▇▃▁▁\nx_075\n0\n1\n3.51\n0.53\n2.65\n3.13\n3.45\n3.77\n5.37\n▆▇▃▁▁\nx_076\n0\n1\n3.49\n0.53\n2.63\n3.11\n3.43\n3.75\n5.36\n▆▇▃▁▁\nx_077\n0\n1\n3.47\n0.53\n2.62\n3.09\n3.40\n3.74\n5.34\n▆▇▃▁▁\nx_078\n0\n1\n3.45\n0.53\n2.60\n3.07\n3.38\n3.73\n5.32\n▆▇▃▁▁\nx_079\n0\n1\n3.43\n0.53\n2.59\n3.05\n3.37\n3.72\n5.30\n▆▇▃▁▁\nx_080\n0\n1\n3.42\n0.53\n2.57\n3.03\n3.35\n3.71\n5.29\n▆▇▃▁▁\nx_081\n0\n1\n3.40\n0.53\n2.55\n3.01\n3.33\n3.68\n5.27\n▇▇▃▁▁\nx_082\n0\n1\n3.38\n0.53\n2.53\n2.99\n3.30\n3.66\n5.25\n▇▇▅▁▁\nx_083\n0\n1\n3.36\n0.53\n2.51\n2.97\n3.28\n3.65\n5.23\n▇▇▅▁▁\nx_084\n0\n1\n3.34\n0.53\n2.50\n2.96\n3.26\n3.63\n5.21\n▇▇▅▂▁\nx_085\n0\n1\n3.32\n0.53\n2.48\n2.94\n3.23\n3.62\n5.19\n▇▇▅▂▁\nx_086\n0\n1\n3.30\n0.53\n2.46\n2.92\n3.21\n3.60\n5.17\n▇▇▅▂▁\nx_087\n0\n1\n3.28\n0.53\n2.44\n2.90\n3.18\n3.57\n5.15\n▇▇▅▂▁\nx_088\n0\n1\n3.25\n0.53\n2.42\n2.87\n3.16\n3.55\n5.13\n▇▇▅▂▁\nx_089\n0\n1\n3.23\n0.53\n2.40\n2.85\n3.13\n3.53\n5.10\n▇▇▅▂▁\nx_090\n0\n1\n3.21\n0.53\n2.38\n2.83\n3.11\n3.51\n5.08\n▇▇▅▂▁\nx_091\n0\n1\n3.19\n0.52\n2.36\n2.81\n3.09\n3.49\n5.05\n▇▇▅▂▁\nx_092\n0\n1\n3.17\n0.52\n2.34\n2.78\n3.08\n3.48\n5.03\n▇▇▅▂▁\nx_093\n0\n1\n3.15\n0.52\n2.32\n2.77\n3.06\n3.46\n5.01\n▇▇▅▂▁\nx_094\n0\n1\n3.13\n0.52\n2.30\n2.75\n3.04\n3.45\n4.99\n▇▇▅▂▁\nx_095\n0\n1\n3.11\n0.52\n2.28\n2.73\n3.01\n3.43\n4.97\n▇▇▅▂▁\nx_096\n0\n1\n3.09\n0.52\n2.26\n2.70\n2.99\n3.42\n4.95\n▇▇▅▂▁\nx_097\n0\n1\n3.07\n0.52\n2.24\n2.68\n2.97\n3.41\n4.92\n▇▇▅▂▁\nx_098\n0\n1\n3.05\n0.52\n2.22\n2.67\n2.95\n3.39\n4.90\n▇▇▅▂▁\nx_099\n0\n1\n3.04\n0.52\n2.21\n2.65\n2.94\n3.37\n4.88\n▇▇▅▂▁\nx_100\n0\n1\n3.02\n0.52\n2.19\n2.63\n2.92\n3.35\n4.85\n▇▇▅▂▁\nwater\n0\n1\n63.32\n9.99\n40.20\n55.50\n65.90\n72.00\n76.60\n▂▂▃▅▇\nfat\n0\n1\n18.07\n12.83\n0.90\n7.20\n14.30\n28.10\n48.20\n▇▅▃▂▂\nprotein\n0\n1\n17.69\n3.08\n11.00\n15.30\n18.90\n20.20\n21.80\n▂▃▃▇▇\n\nChecking the correlation between the Y outcomes:\n\n\nmeat_training %>% \n  select(water:protein) %>% \n  ggcorr(label = T, label_alpha = T, label_round = 2)\n\n\n\n\nFrom the exploratory data analysis, we can see that:\nnumber of samples << number of X variables (fat dataset)\nthere are no missing values, so there is no need for missing values imputation\nY variables are skewed, would need to normalize data\nX variables are highly correlated, and it is recommended to only carry out means-centering and not scaling (Ron Wehren’s book for Chemometrics)\nY variables are also highly correlated with each other\nPLS2 Modelling\nThis section below is taken from: https://www.tidymodels.org/learn/models/pls/, with some changes to the preprocessing steps.\nThis step includes preprocessing, specifying pls model to be used and model tuning.\nFor spectral data, autoscaling (normalization) is not usually recommended. When every spectral variable is set to the same standard deviation, the noise is blown up to the same size as the signal that contains the actual information. In such cases, only means-centering is required.\nPreprocessing\n\n\n# Preprocessing only for TRAINING dataset\n\n# recipe\nmeat_reciped_pls2 <- recipe(water + fat + protein ~., data = meat_training) %>% \n  update_role(water, fat, protein, new_role = \"outcome\") %>% \n  step_center(all_predictors())\n  \n# set folds for cross-validation\n\n# repeated 10-fold cross validation for tuning model to select optimal number of components, since this is a small dataset\n\nset.seed(202103182)\n\nfolds_pls2 <- vfold_cv(meat_training, repeats = 10)\n\nfolds_pls2 <- \n  folds_pls2 %>%\n  mutate(recipes = map(splits, prepper, recipe = meat_reciped_pls2))\n\n\n\nTuning of model using repeated Cross-validation dataset\nTo fit the model, we need to:\nformat the X and Y into 2 different matrices, one for X and one for Y. THis is the format which the pls package requires.\nEstimate the Y outcomes\n\n\nget_var_explained <- function(recipe, ...) {\n  \n  # Extract the predictors and outcomes into their own matrices\n  y_mat <- bake(recipe, new_data = NULL, composition = \"matrix\", all_outcomes())\n  \n  x_mat <- bake(recipe, new_data = NULL, composition = \"matrix\", all_predictors())\n  \n  # The pls package prefers the data in a data frame where the outcome\n  # and predictors are in _matrices_. To make sure this is formatted\n  # properly, use the `I()` function to inhibit `data.frame()` from making\n  # all the individual columns. `pls_format` should have two columns.\n  pls_format <- data.frame(\n    endpoints = I(y_mat),\n    measurements = I(x_mat)\n  )\n  # Fit the model\n  mod <- plsr(endpoints ~ measurements, data = pls_format)\n  \n  # Get the proportion of the predictor variance that is explained\n  # by the model for different number of components. \n  xve <- explvar(mod)/100 \n\n  # To do the same for the outcome, it is more complex. This code \n  # was extracted from pls:::summary.mvr. \n  explained <- \n    drop(pls::R2(mod, estimate = \"train\", intercept = FALSE)$val) %>% \n    # transpose so that components are in rows\n    t() %>% \n    as_tibble() %>%\n    # Add the predictor proportions\n    mutate(predictors = cumsum(xve) %>% as.vector(),\n           components = seq_along(xve)) %>%\n    # Put into a tidy format that is tall\n    pivot_longer(\n      cols = c(-components),\n      names_to = \"source\",\n      values_to = \"proportion\"\n    )\n}\n\n# Compute this dataframe for each resample, and save the results in different columns:\n\nfolds_pls2 <- \n  folds_pls2 %>%\n  dplyr::mutate(var = map(recipes, get_var_explained),\n         var = unname(var))\n\n\n\nScree plot\nExtract variance data:\n\n\nvariance_data <- \n  bind_rows(folds_pls2[[\"var\"]]) %>% # select var col in folds dataset\n  filter(components <= 20) %>% # limit components to from 1 to 20\n  group_by(components, source) %>%\n  summarize(proportion = mean(proportion))\n\nggplot(variance_data, aes(x = components, y = proportion, \n                          col = source, label = round(proportion,2))) + \n  geom_line() + \n  geom_point() +\n  geom_text(nudge_y = 0.05) +\n  labs(title = \"Variance captured for different number of components\", \n       subtitle = \"Predictors (X) variance is captured very well by a single componenet.\\nTo estimate all Y outcomes, one may need 11-13 components.\",\n       x = \"Number of components\",\n       y = \"Mean Cumulative Variance\",\n       caption = \"Source: Code is adapted from https://www.tidymodels.org/learn/models/pls/\") +\n  facet_wrap(source ~., ncol = 2, scales = \"fixed\") +\n  scale_x_continuous(breaks = c(1:20), n.breaks = 19) +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n\nPLS 1 modelling - Water\nSimilar to earlier, to evaluate the PLS model, 10 repeats of the 10-folds cross validation will be used (100 holdout samples) to evaluate the overall model performance (RMSE, MAE, r-sq). However, individual models will be built for water, protein and fat.\nSteps involved:\nsplit dataset (already carried out)\nrecipe: specify pre-processing steps\nfit model\nput into workflow\n\n\n# folds for repeated cross validation\nset.seed(20210325)\nfolds_pls1 <- vfold_cv(meat_training, v = 10, repeats = 10)\n\n# recipe\nmeat_reciped_water_pls1 <- recipe(water ~., data = meats) %>% \n  update_role(fat, protein, new_role = \"other_y\") %>% \n  step_center(all_predictors())\n\nmeat_reciped_water_pls1\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   other_y          2\n   outcome          1\n predictor        100\n\nOperations:\n\nCentering for all_predictors()\n\n# fit model\n\npls_water_model <- plsmod::pls(num_comp = tune()) %>% \n  set_mode(\"regression\") %>% # can be either classification or regression\n  set_engine(\"mixOmics\") # to specify which package to use\n\n\n# put into workflow\n\npls_water_workflow <- workflow() %>% \n  add_recipe(meat_reciped_water_pls1) %>% \n  add_model(pls_water_model)\n\n# create grid\n\npls_1_grid <- expand.grid(num_comp = seq(from = 1, to = 20, by = 1))\n\n\ntuned_pls1_water_results <- pls_water_workflow %>% \n  tune_grid(\n    resamples = folds_pls1,\n    grid = pls_1_grid,\n    metrics = metric_set(rmse, rsq, mae)\n  )\n\n# model results\n\nwater_model_results <- tuned_pls1_water_results %>% \n  collect_metrics()\n\nwater_model_results\n\n\n# A tibble: 60 x 7\n   num_comp .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1        1 mae     standard   7.40    100  0.113  Preprocessor1_Mode…\n 2        1 rmse    standard   8.77    100  0.137  Preprocessor1_Mode…\n 3        1 rsq     standard   0.252   100  0.0162 Preprocessor1_Mode…\n 4        2 mae     standard   4.78    100  0.0874 Preprocessor1_Mode…\n 5        2 rmse    standard   6.06    100  0.110  Preprocessor1_Mode…\n 6        2 rsq     standard   0.657   100  0.0121 Preprocessor1_Mode…\n 7        3 mae     standard   3.31    100  0.0635 Preprocessor1_Mode…\n 8        3 rmse    standard   4.10    100  0.0857 Preprocessor1_Mode…\n 9        3 rsq     standard   0.823   100  0.0102 Preprocessor1_Mode…\n10        4 mae     standard   2.78    100  0.0439 Preprocessor1_Mode…\n# … with 50 more rows\n\n# best model\n\ntuned_pls1_water_results %>% \n  select_best(metric = \"rmse\") # num_comp = 18\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       18 Preprocessor1_Model18\n\ntuned_pls1_water_results %>% \n  select_best(metric = \"rsq\") # num_comp = 18\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       18 Preprocessor1_Model18\n\ntuned_pls1_water_results %>% \n  select_best(metric = \"mae\") # num_comp = 18\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       18 Preprocessor1_Model18\n\n# visualize\ntuned_pls1_water_results %>% \n  collect_metrics() %>% \n  ggplot(aes(num_comp, mean, col = .metric)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(n.breaks = 20) +\n  labs(x = \"Number of components\",\n       y = \"Indicator\",\n       title = \"Plot of MAE, RMSE and R-SQ vs No. of components for TRAINING dataset, with 10-fold repeated cross validation\",\n       subtitle = \"Predicting Water Content: Optimal number of components is 18.\") +\n facet_grid(.metric ~.) +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n# Update model and workflow:\n\nupdated_pls_water_model <-   plsmod::pls(num_comp = 18) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"mixOmics\")\n\nupdated_pls_water_workflow <- pls_water_workflow %>% \n  update_model(updated_pls_water_model)\n\npls_water_fit <- updated_pls_water_workflow %>% \n  fit(data = meat_training)\n\n# Check the most important X variables for the updated Water Model:\n\ntidy_pls_water <- pls_water_fit %>% \n  pull_workflow_fit() %>% \n  tidy()\n\n# Variable importance\n\ntidy_pls_water %>% \n  filter(term != \"Y\",\n         component == c(1:18)) %>%\n  group_by(component) %>% \n  slice_max(abs(value), n = 20) %>% \n  ungroup() %>% \n  ggplot(aes(value, fct_reorder(term, value), fill = factor(component))) +\n  geom_col(show.legend = F) +\n  facet_wrap( ~ component, scales = \"free_y\") +\n  labs( y = NULL) +\n  theme_few()\n\n\n\n# Assess\n\npls_water_fit %>% \n  predict(new_data = meat_training) %>% \n  mutate(truth = meat_training$water) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TRAINING dataset\",\n       x = \"Actual Water Content\",\n       y = \"Predicted Water Content\") +\n  theme_few()\n\n\n\n# for TEST dataset\n  \npls_water_fit %>% \n  predict(new_data = meat_testing) %>% \n  mutate(truth = meat_testing$water) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TEST dataset\",\n       x = \"Actual Water Content\",\n       y = \"Predicted Water Content\") +\n  theme_few()\n\n\n\n# assessing model on test data\nupdated_pls_water_workflow %>% \n  last_fit(meat_split) %>% \n  collect_metrics()\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       2.32  Preprocessor1_Model1\n2 rsq     standard       0.944 Preprocessor1_Model1\n\nFrom above, it seems that the prediction of moisture content is more accurate for higher water content samples.\nTo predict water content for future data:\npls_water_fit %>% predict(trial_data), where trial_data has the NIR spectrum.\nPLS 1 modelling for fat\nRepeat the workflow above for fat:\n\n\nmeat_reciped_fat_pls1 <- recipe(fat ~., data = meats) %>% \n  update_role(water, protein, new_role = \"other_y\") %>% \n  step_center(all_predictors())\n\n\npls_fat_model <- plsmod::pls(num_comp = tune()) %>% \n  set_mode(\"regression\") %>% # can be either classification or regression\n  set_engine(\"mixOmics\")\n\npls_fat_workflow <- workflow() %>% \n  add_recipe(meat_reciped_fat_pls1) %>% \n  add_model(pls_fat_model)\n\ntuned_pls1_fat_results <- pls_fat_workflow %>% \n  tune_grid(\n    resamples = folds_pls1,\n    grid = pls_1_grid,\n    metrics = metric_set(rmse, rsq, mae)\n  )\n\n# model results\n\nfat_model_results <- tuned_pls1_fat_results %>% \n  collect_metrics()\n\nfat_model_results\n\n\n# A tibble: 60 x 7\n   num_comp .metric .estimator   mean     n std_err .config           \n      <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>             \n 1        1 mae     standard    9.50    100  0.149  Preprocessor1_Mod…\n 2        1 rmse    standard   11.5     100  0.178  Preprocessor1_Mod…\n 3        1 rsq     standard    0.223   100  0.0155 Preprocessor1_Mod…\n 4        2 mae     standard    6.32    100  0.120  Preprocessor1_Mod…\n 5        2 rmse    standard    8.21    100  0.143  Preprocessor1_Mod…\n 6        2 rsq     standard    0.617   100  0.0126 Preprocessor1_Mod…\n 7        3 mae     standard    4.24    100  0.0914 Preprocessor1_Mod…\n 8        3 rmse    standard    5.43    100  0.133  Preprocessor1_Mod…\n 9        3 rsq     standard    0.809   100  0.0123 Preprocessor1_Mod…\n10        4 mae     standard    3.57    100  0.0576 Preprocessor1_Mod…\n# … with 50 more rows\n\n# best model\n\ntuned_pls1_fat_results %>% \n  select_best(metric = \"rmse\") # num_comp = 19\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       19 Preprocessor1_Model19\n\ntuned_pls1_fat_results %>% \n  select_best(metric = \"rsq\") # num_comp = 18\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       18 Preprocessor1_Model18\n\ntuned_pls1_fat_results %>% \n  select_best(metric = \"mae\") # num_comp = 19\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       19 Preprocessor1_Model19\n\n# visualize\ntuned_pls1_fat_results %>% \n  collect_metrics() %>% \n  ggplot(aes(num_comp, mean, col = .metric)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(n.breaks = 20) +\n  labs(x = \"Number of components\",\n       y = \"Indicator\",\n       title = \"Plot of MAE, RMSE and R-SQ vs No. of components for TRAINING dataset, with 10-fold repeated cross validation\",\n       subtitle = \"Predicting Fat Content: Optimal number of components is 18.\") +\n facet_grid(.metric ~.) +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n# Update model and workflow:\n\nupdated_pls_fat_model <-   plsmod::pls(num_comp = 18) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"mixOmics\")\n\nupdated_pls_fat_workflow <- pls_fat_workflow %>% \n  update_model(updated_pls_fat_model)\n\npls_fat_fit <- updated_pls_fat_workflow %>% \n  fit(data = meat_training)\n\n# Check the most important X variables for the updated Water Model:\n\ntidy_pls_fat <- pls_fat_fit %>% \n  pull_workflow_fit() %>% \n  tidy()\n\n# Variable importance\n\ntidy_pls_fat %>% \n  filter(term != \"Y\",\n         component == c(1:18)) %>%\n  group_by(component) %>% \n  slice_max(abs(value), n = 20) %>% \n  ungroup() %>% \n  ggplot(aes(value, fct_reorder(term, value), fill = factor(component))) +\n  geom_col(show.legend = F) +\n  facet_wrap( ~ component, scales = \"free_y\") +\n  labs( y = NULL) +\n  theme_few()\n\n\n\n# Assess\n\npls_fat_fit %>% \n  predict(new_data = meat_training) %>% \n  mutate(truth = meat_training$fat) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TRAINING dataset\",\n       x = \"Actual Fat Content\",\n       y = \"Predicted Fat Content\") +\n  theme_few()\n\n\n\n# for TEST dataset\n  \npls_fat_fit %>% \n  predict(new_data = meat_testing) %>% \n  mutate(truth = meat_testing$fat) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TEST dataset\",\n       x = \"Actual Fat Content\",\n       y = \"Predicted Fat Content\") +\n  theme_few()\n\n\n\n# assessing model on test data\nupdated_pls_fat_workflow %>% \n  last_fit(meat_split) %>% \n  collect_metrics()\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       2.66  Preprocessor1_Model1\n2 rsq     standard       0.959 Preprocessor1_Model1\n\nFat content prediction is less accurate for samples with higher fat content.\nTo predict fat content for future data:\npls_fat_fit %>% predict(trial_data), where trial_data has the NIR spectrum.\nPLS 1 modelling for protein\n\n\nmeat_reciped_protein_pls1 <- recipe(protein ~., data = meats) %>%\n  update_role(water, fat, new_role = \"other_y\") %>% \n  step_center(all_predictors())\n\n\npls_protein_model <- plsmod::pls(num_comp = tune()) %>% \n  set_mode(\"regression\") %>% # can be either classification or regression\n  set_engine(\"mixOmics\") \n\npls_protein_workflow <- workflow() %>% \n  add_recipe(meat_reciped_protein_pls1) %>% \n  add_model(pls_protein_model)\n\ntuned_pls1_protein_results <- pls_protein_workflow %>% \n  tune_grid(\n    resamples = folds_pls1,\n    grid = pls_1_grid,\n    metrics = metric_set(rmse, rsq, mae)\n  )\n\n# model results\n\nprotein_model_results <- tuned_pls1_protein_results %>% \n  collect_metrics()\n\nprotein_model_results\n\n\n# A tibble: 60 x 7\n   num_comp .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1        1 mae     standard   2.54    100  0.0378 Preprocessor1_Mode…\n 2        1 rmse    standard   2.97    100  0.0395 Preprocessor1_Mode…\n 3        1 rsq     standard   0.126   100  0.0136 Preprocessor1_Mode…\n 4        2 mae     standard   1.92    100  0.0345 Preprocessor1_Mode…\n 5        2 rmse    standard   2.38    100  0.0366 Preprocessor1_Mode…\n 6        2 rsq     standard   0.406   100  0.0164 Preprocessor1_Mode…\n 7        3 mae     standard   1.37    100  0.0333 Preprocessor1_Mode…\n 8        3 rmse    standard   1.84    100  0.0450 Preprocessor1_Mode…\n 9        3 rsq     standard   0.635   100  0.0170 Preprocessor1_Mode…\n10        4 mae     standard   1.24    100  0.0255 Preprocessor1_Mode…\n# … with 50 more rows\n\n# best model\n\ntuned_pls1_protein_results %>% \n  select_best(metric = \"rmse\") # num_comp = 14\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       14 Preprocessor1_Model14\n\ntuned_pls1_protein_results %>% \n  select_best(metric = \"rsq\") # num_comp = 14\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       15 Preprocessor1_Model15\n\ntuned_pls1_protein_results %>% \n  select_best(metric = \"mae\") # num_comp = 15\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       15 Preprocessor1_Model15\n\n# visualize\ntuned_pls1_protein_results %>% \n  collect_metrics() %>% \n  ggplot(aes(num_comp, mean, col = .metric)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(n.breaks = 20) +\n  labs(x = \"Number of components\",\n       y = \"Indicator\",\n       title = \"Plot of MAE, RMSE and R-SQ vs No. of components for TRAINING dataset, with 10-fold repeated cross validation\",\n       subtitle = \"Predicting protein Content: Optimal number of components is 14.\") +\n facet_grid(.metric ~.) +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n# Update model and workflow:\n\nupdated_pls_protein_model <-   plsmod::pls(num_comp = 14) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"mixOmics\")\n\nupdated_pls_protein_workflow <- pls_protein_workflow %>% \n  update_model(updated_pls_protein_model)\n\npls_protein_fit <- updated_pls_protein_workflow %>% \n  fit(data = meat_training)\n\n# Check the most important X variables for the updated Water Model:\n\ntidy_pls_protein <- pls_protein_fit %>% \n  pull_workflow_fit() %>% \n  tidy()\n\n# Variable importance\n\ntidy_pls_protein %>% \n  filter(term != \"Y\",\n         component == c(1:14)) %>%\n  group_by(component) %>% \n  slice_max(abs(value), n = 20) %>% \n  ungroup() %>% \n  ggplot(aes(value, fct_reorder(term, value), fill = factor(component))) +\n  geom_col(show.legend = F) +\n  facet_wrap( ~ component, scales = \"free_y\") +\n  labs( y = NULL) +\n  theme_few()\n\n\n\n# Assess\n\npls_protein_fit %>% \n  predict(new_data = meat_training) %>% \n  mutate(truth = meat_training$protein) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TRAINING dataset\",\n       x = \"Actual protein Content\",\n       y = \"Predicted protein Content\") +\n  theme_few()\n\n\n\n# for TEST dataset\n  \npls_protein_fit %>% \n  predict(new_data = meat_testing) %>% \n  mutate(truth = meat_testing$protein) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TEST dataset\",\n       x = \"Actual protein Content\",\n       y = \"Predicted protein Content\") +\n  theme_few()\n\n\n\n# assessing model on test data\nupdated_pls_protein_workflow %>% \n  last_fit(meat_split) %>% \n  collect_metrics()\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.463 Preprocessor1_Model1\n2 rsq     standard       0.980 Preprocessor1_Model1\n\nTo predict protein content for future data:\npls_protein_fit %>% predict(trial_data), where trial_data has the NIR spectrum.\nLearning pointers\nThrough this exercise, I learnt:\nhow to carry out PCA and PLS regression on NIR data. However, I am not very sure whether to carry out both means-centering and scaling on NIR data or not?\nhow to efficiently visualize the number of components for PLS regression by following the steps listed on https://www.tidymodels.org/learn/models/pls/\nthe difference between PLS 1 and PLS 2, although I think tidymodels cannot handle PLS 2 yet.\nInitially I tried to carry out PLS regression by visual inspection for optimal number of components so that there will be a tradeoff between over-fitting and bias, but I find that the accuracy is really compromised, so I will stick to the optimal number of components by looking at the three different metrics and choosing from there.\nI think this is a good practice for real-life datasets, but I would probably need to practice more on other datasets to get the hang of PLS regression. It would also be good practice to work on comparison of different models on test dataset.\nReferences\nhttps://mixomicsteam.github.io/Bookdown/pls.html https://www.tidymodels.org/learn/models/pls/ https://www.tmwr.org/resampling.html https://rsample.tidymodels.org/articles/Working_with_rsets.html https://conf20-intro-ml.netlify.app/slides/07-cv.html#1 https://www.sciencedirect.com/science/article/pii/S1878535214000343 https://stackoverflow.com/questions/64582463/how-do-i-specify-a-pls-model-in-tidy-models https://github.com/tidymodels/workflows/issues/37\n\n\n\n",
    "preview": "posts/20210328_NIR Meat Data (PLS-regression)/Meat-NIR-PLS-regression_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-28T23:46:45+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210311_notes on resampling/",
    "title": "Notes - Resampling",
    "description": "Types of resampling techniqes and considerations on which to choose",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\nBackground\nWhen I was trying out PLS regression on the gasoline data (a small dataset of 60 samples with NIR spectra measured over more than 400 wavelengths), I was a little stuck at the resampling part. Most examples online used k-fold cross validation as the resampling method, but I was also wondering about bootstrapping, because I remembered that my Prof mentioned it would also help in model performance to prevent overfitting.\nI think there was a need for me to get my concepts right for:\nOverfitting - why it is bad –> so that new data can be predicted accurately, not just having an accurate model for old training data.\nResampling, when do you need to do it? –> to improve model tuning, and give good predictions of how well the model will perform on future samples.\nHow to split the data: 80/20? 70/30? 75/25? –> if you don’t have a lot of data, suggest to use 80/20 so that more datapoints can be used to training.\nWhat are the various resampling techniques available, and how to choose which one to use?\nI got the information below from the book Applied Predictive Modelling http://appliedpredictivemodeling.com/, Chapter 4.\nResampling techniques\nk-fold cross-validation\nSamples are randomly divided into k sets of the same size. A model is fit using all the samples except the first subset. The first subset is then used to assess model performance. The whole process is repeated k times, using other subsets. Model performance may be assessed by comparing error rates or r-squared values.\nk-fold cross-validation usually has a high variance (low bias) compared to other methods. However, for large training sets, the variance-bias issue should be negligible.\nk is usually fixed as 5 or 10. The bias is smaller for k = 10.\nThis method is recommended for tuning model parameters to get the best indicator of performance for small sample sizes as the bias-variance properties are good, and does not come with high computational costs (unlike leave-one-out method).\nBootstrapping\nA bootstrap sample is a random sample of the data taken with replacement. Some samples may be selected multiple times, while there may be others that are not selected at all. In general, bootstrap error rates tend to have less uncertainty than k-fold.\nThis method may be preferred if the aim is to choose between models, as bootstrapping technique has low variance (high bias).\nChoosing models\nThere is a spectrum of interpretability and flexibility for models. Choose variaous models that occur at different parts of the spectrum, for example, a simple and inflexible but easy to interpret linear model, as compared to partial least squares which is lower down the interpretability but higher up the flexibility, as compared to random forests which are hard to interpret but very flexible.\nReflections\nI am really glad to have this Applied Predictive Modelling book with me. It clarified a lot on the concepts part, but most of the code is given in the older coding styles. Looking forward to try on the worked examples using tidymodels framework!\nReferences\nhttps://stackoverflow.com/questions/64254295/predictor-importance-for-pls-model-trained-with-tidymodels\nhttps://rpubs.com/RandallThompson15/HW10_624\nhttps://www.tidyverse.org/blog/2020/04/parsnip-adjacent/\nApplied Predictive Modelling, Max Kuhn and Kjell Johnson, Chapter 4. http://appliedpredictivemodeling.com/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-11T21:41:14+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210309_tidy models plsr gasoline/",
    "title": "Tidy Models - Regression",
    "description": "Predicting numerical outcomes using partial least squares on gasoline dataset",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\nOverview\nThe partial least square (PLS) regression method may be used to predict one or more numerical Y outcome variable when the X variables are highly correlated. In the chemistry field, it is very useful for relating spectra to chemical properties.\nThe problem with using ordinary least square linear regression for such datasets is that when X variables are highly correlated, it is difficult to interpret coefficients of different X meaningfully, since they are all correlated.\nPLS technique is also useful when the number of samples is lesser than the number of variables. It maximises the covariance of both X and Y to calculate scores (how much a particular object contributes to a latent variable) and loadings (variable coefficients used to define a component). For principal component regression (PCR), only X variables are taken into account. If the variability of X is not related to variability of Y, then PCR will have difficulty identifying a predictive relationship, when one might exist. It helps to uncover latent structures in the highly correlated X variables, so as to predict Y.\nIn a way, PLS is a supervised dimension reduction. It finds components that maximally summarise the variation of the predictors while simultaneiously requiring these components to have maximum correlation with the response.\nThe workflow below is for predicting one Y variable, using multiple X variables, on the gasoline NIR dataset. The outcome variable of interest is the octane number.\nGeneral workflow:\nExploratory data analysis: Check if there are missing values, correlation of variables, identify the pre-processing steps required. How many variables are there? How are they distributed? Is it suitable for PLS?\nThe data may also be visualized before modelling.\nSplit the dataset into training and testing dataset. The training dataset is used to build and tune the models, and the testing dataset is used to evaluate the model. Define a 10-fold cross validation dataset, from the training dataset. Resampling is useful if the sample size is small, to have a better bias-variance tradeoff (minimise over-fitting of model) for better predictive ability.\nPreprocess the training dataset. Data would have to be imputed if there are missing values, and also normalised since PLS is using variance to understand dissimilarity in the X variables.\nTrain the model using the training dataset. In this case, the pls model is used. For PLS model, there is one tuning parameter, which is the number of components. The tune package will be used for tuning, and the data will be the 10-fold cross validation dataset.\nAssess the training model using root mean square error (RMSE) and mean absolute error (MAE), as well as r-sq for accuracy.\nDetermine which variables are important in the model\nPredict new data\nPackages required\n\n\nlibrary(pacman)\np_load(modeldata, pls, tidyverse, tidymodels, corrplot, \n       skimr, plsmod, ggthemes)\n\n\n\nImport data\nThis dataset is from the pls package, and has octane number (Y outcome variable), as well as NIR spectra of 60 gasoline samples.\n\n\ndata(gasoline)\n\n\n\nVisualize\n\n\ndata_plot <- cbind(gasoline, as.data.frame((unclass(gasoline$NIR)))) %>% \n  dplyr::select(-NIR) %>% \n  as_tibble() %>% \n  rowid_to_column() %>% \n  janitor::clean_names() %>% \n  pivot_longer(cols = starts_with(\"x\"),\n               names_to = \"wavelength\",\n               values_to = \"reflectance\") %>% \n  mutate(wavelength_number = parse_number(wavelength))\n\ndata_plot %>% \n  ggplot(aes(x = wavelength_number, y = reflectance, col = factor(rowid))) +\n  geom_line(show.legend = F) +\n  labs(y = \"log(1/R)\",\n       x = \"nm\",\n       title = \"Plot of NIR spectra for 60 gasoline samples\") +\n  coord_cartesian(xlim = c(900, 1800)) +\n  theme_classic()\n\n\n\n\nModelling\nTo use the dataset for modelling using the tidymodels framework, convert the data into a tidy tibble structure. The pls package requires that the data be provided as a matrix format, but for the tidymodels framework, the data should be in a tibble format for ease in creating plots and modelling.\n\n\ngasoline_tidy <-  cbind(gasoline, as.data.frame((unclass(gasoline$NIR)))) %>% \n  dplyr::select(-NIR) %>% \n  as_tibble() %>% \n  janitor::clean_names() \n\nglimpse(gasoline_tidy) # 60 rows, 402 columns\n\n\nRows: 60\nColumns: 402\n$ octane   <dbl> 85.30, 85.25, 88.45, 83.40, 87.90, 85.50, 88.90, 88…\n$ x900_nm  <dbl> -0.050193, -0.044227, -0.046867, -0.046705, -0.0508…\n$ x902_nm  <dbl> -0.045903, -0.039602, -0.041260, -0.042240, -0.0451…\n$ x904_nm  <dbl> -0.042187, -0.035673, -0.036979, -0.038561, -0.0410…\n$ x906_nm  <dbl> -0.037177, -0.030911, -0.031458, -0.034513, -0.0363…\n$ x908_nm  <dbl> -0.033348, -0.026675, -0.026520, -0.030206, -0.0327…\n$ x910_nm  <dbl> -0.031207, -0.023871, -0.023346, -0.027680, -0.0314…\n$ x912_nm  <dbl> -0.030036, -0.022571, -0.021392, -0.026042, -0.0314…\n$ x914_nm  <dbl> -0.031298, -0.025410, -0.024993, -0.028280, -0.0346…\n$ x916_nm  <dbl> -0.034217, -0.028960, -0.029309, -0.030920, -0.0377…\n$ x918_nm  <dbl> -0.036012, -0.032740, -0.033920, -0.034012, -0.0407…\n$ x920_nm  <dbl> -0.039792, -0.036683, -0.038539, -0.037082, -0.0440…\n$ x922_nm  <dbl> -0.043037, -0.040169, -0.042571, -0.040444, -0.0473…\n$ x924_nm  <dbl> -0.047313, -0.044899, -0.047511, -0.044858, -0.0514…\n$ x926_nm  <dbl> -0.048103, -0.046266, -0.048487, -0.046544, -0.0520…\n$ x928_nm  <dbl> -0.050627, -0.048627, -0.050455, -0.048978, -0.0540…\n$ x930_nm  <dbl> -0.053830, -0.052014, -0.053913, -0.052483, -0.0571…\n$ x932_nm  <dbl> -0.054604, -0.053635, -0.055195, -0.054078, -0.0583…\n$ x934_nm  <dbl> -0.056676, -0.055454, -0.056713, -0.055723, -0.0599…\n$ x936_nm  <dbl> -0.058428, -0.056777, -0.057447, -0.056999, -0.0606…\n$ x938_nm  <dbl> -0.060644, -0.059331, -0.059620, -0.059409, -0.0628…\n$ x940_nm  <dbl> -0.061712, -0.060518, -0.060656, -0.060543, -0.0637…\n$ x942_nm  <dbl> -0.063148, -0.062084, -0.061767, -0.061868, -0.0650…\n$ x944_nm  <dbl> -0.064480, -0.063331, -0.062859, -0.062899, -0.0657…\n$ x946_nm  <dbl> -0.065987, -0.065059, -0.064668, -0.064401, -0.0672…\n$ x948_nm  <dbl> -0.066894, -0.065536, -0.065566, -0.064814, -0.0679…\n$ x950_nm  <dbl> -0.068201, -0.066877, -0.066890, -0.065989, -0.0692…\n$ x952_nm  <dbl> -0.069434, -0.067899, -0.068397, -0.067180, -0.0707…\n$ x954_nm  <dbl> -0.069594, -0.067662, -0.068439, -0.066987, -0.0707…\n$ x956_nm  <dbl> -0.071336, -0.069050, -0.070171, -0.068829, -0.0724…\n$ x958_nm  <dbl> -0.071208, -0.068497, -0.070077, -0.069063, -0.0722…\n$ x960_nm  <dbl> -0.071454, -0.068434, -0.070639, -0.069995, -0.0726…\n$ x962_nm  <dbl> -0.071469, -0.067953, -0.070578, -0.070687, -0.0726…\n$ x964_nm  <dbl> -0.071631, -0.067061, -0.070137, -0.070725, -0.0722…\n$ x966_nm  <dbl> -0.070886, -0.066073, -0.069803, -0.070661, -0.0718…\n$ x968_nm  <dbl> -0.071010, -0.065564, -0.069738, -0.071043, -0.0718…\n$ x970_nm  <dbl> -0.070609, -0.065335, -0.070031, -0.071324, -0.0718…\n$ x972_nm  <dbl> -0.070464, -0.064051, -0.069496, -0.070894, -0.0714…\n$ x974_nm  <dbl> -0.070433, -0.063666, -0.069789, -0.071157, -0.0717…\n$ x976_nm  <dbl> -0.069957, -0.062513, -0.069002, -0.070798, -0.0711…\n$ x978_nm  <dbl> -0.070208, -0.061955, -0.069288, -0.071120, -0.0713…\n$ x980_nm  <dbl> -0.070523, -0.060906, -0.069398, -0.071266, -0.0715…\n$ x982_nm  <dbl> -0.070252, -0.059413, -0.068613, -0.070540, -0.0707…\n$ x984_nm  <dbl> -0.069360, -0.058242, -0.067675, -0.069737, -0.0698…\n$ x986_nm  <dbl> -0.068338, -0.056855, -0.066685, -0.068637, -0.0687…\n$ x988_nm  <dbl> -0.066991, -0.055982, -0.065196, -0.067211, -0.0673…\n$ x990_nm  <dbl> -0.065362, -0.055107, -0.063317, -0.065586, -0.0655…\n$ x992_nm  <dbl> -0.063971, -0.054763, -0.061604, -0.064064, -0.0641…\n$ x994_nm  <dbl> -0.062210, -0.054189, -0.059695, -0.062089, -0.0624…\n$ x996_nm  <dbl> -0.060678, -0.054361, -0.058403, -0.060639, -0.0612…\n$ x998_nm  <dbl> -0.059275, -0.054014, -0.056868, -0.059018, -0.0598…\n$ x1000_nm <dbl> -0.059126, -0.054822, -0.056419, -0.058185, -0.0595…\n$ x1002_nm <dbl> -0.058903, -0.055303, -0.056021, -0.057415, -0.0592…\n$ x1004_nm <dbl> -0.058488, -0.055101, -0.055192, -0.056311, -0.0583…\n$ x1006_nm <dbl> -0.057291, -0.054473, -0.053844, -0.054969, -0.0570…\n$ x1008_nm <dbl> -0.055408, -0.053119, -0.051961, -0.052664, -0.0552…\n$ x1010_nm <dbl> -0.053862, -0.051626, -0.050363, -0.050925, -0.0537…\n$ x1012_nm <dbl> -0.051421, -0.048967, -0.047896, -0.048186, -0.0516…\n$ x1014_nm <dbl> -0.050485, -0.047722, -0.046456, -0.046844, -0.0508…\n$ x1016_nm <dbl> -0.048349, -0.045717, -0.044870, -0.045207, -0.0492…\n$ x1018_nm <dbl> -0.047586, -0.044365, -0.044068, -0.044741, -0.0484…\n$ x1020_nm <dbl> -0.046898, -0.043336, -0.043379, -0.044397, -0.0481…\n$ x1022_nm <dbl> -0.047726, -0.043581, -0.044227, -0.044697, -0.0487…\n$ x1024_nm <dbl> -0.048686, -0.044036, -0.045469, -0.045614, -0.0498…\n$ x1026_nm <dbl> -0.049626, -0.044856, -0.046493, -0.046452, -0.0506…\n$ x1028_nm <dbl> -0.050714, -0.046041, -0.048213, -0.047669, -0.0520…\n$ x1030_nm <dbl> -0.051541, -0.046858, -0.049097, -0.048569, -0.0528…\n$ x1032_nm <dbl> -0.052976, -0.048652, -0.050967, -0.050055, -0.0546…\n$ x1034_nm <dbl> -0.054019, -0.049853, -0.052127, -0.051178, -0.0555…\n$ x1036_nm <dbl> -0.055507, -0.051562, -0.053886, -0.052781, -0.0569…\n$ x1038_nm <dbl> -0.056386, -0.052526, -0.054432, -0.053594, -0.0576…\n$ x1040_nm <dbl> -0.056779, -0.053738, -0.055172, -0.054460, -0.0580…\n$ x1042_nm <dbl> -0.056876, -0.054069, -0.055257, -0.054997, -0.0582…\n$ x1044_nm <dbl> -0.056439, -0.053925, -0.054866, -0.055201, -0.0578…\n$ x1046_nm <dbl> -0.056708, -0.054880, -0.055286, -0.055934, -0.0585…\n$ x1048_nm <dbl> -0.056432, -0.054857, -0.055256, -0.055994, -0.0583…\n$ x1050_nm <dbl> -0.056974, -0.055695, -0.055922, -0.056841, -0.0590…\n$ x1052_nm <dbl> -0.057091, -0.055656, -0.055649, -0.056687, -0.0586…\n$ x1054_nm <dbl> -0.057541, -0.056352, -0.056044, -0.057192, -0.0590…\n$ x1056_nm <dbl> -0.057746, -0.056501, -0.056164, -0.057521, -0.0589…\n$ x1058_nm <dbl> -0.058387, -0.057275, -0.056653, -0.058097, -0.0592…\n$ x1060_nm <dbl> -0.058997, -0.057927, -0.057349, -0.058708, -0.0597…\n$ x1062_nm <dbl> -0.058975, -0.058044, -0.057620, -0.058940, -0.0601…\n$ x1064_nm <dbl> -0.059624, -0.058311, -0.058109, -0.059339, -0.0607…\n$ x1066_nm <dbl> -0.059737, -0.058253, -0.057978, -0.059002, -0.0607…\n$ x1068_nm <dbl> -0.060552, -0.058841, -0.058726, -0.059353, -0.0617…\n$ x1070_nm <dbl> -0.060416, -0.058138, -0.058442, -0.058568, -0.0615…\n$ x1072_nm <dbl> -0.061099, -0.058603, -0.058857, -0.058309, -0.0623…\n$ x1074_nm <dbl> -0.060784, -0.058309, -0.058779, -0.057284, -0.0621…\n$ x1076_nm <dbl> -0.061292, -0.058444, -0.059351, -0.057273, -0.0625…\n$ x1078_nm <dbl> -0.061811, -0.058993, -0.060286, -0.057433, -0.0631…\n$ x1080_nm <dbl> -0.061852, -0.059121, -0.060681, -0.057911, -0.0634…\n$ x1082_nm <dbl> -0.062380, -0.059445, -0.061533, -0.059303, -0.0640…\n$ x1084_nm <dbl> -0.062816, -0.059475, -0.061879, -0.060453, -0.0642…\n$ x1086_nm <dbl> -0.063620, -0.060274, -0.062840, -0.062042, -0.0651…\n$ x1088_nm <dbl> -0.063730, -0.060256, -0.063150, -0.062812, -0.0654…\n$ x1090_nm <dbl> -0.064291, -0.060515, -0.063667, -0.063620, -0.0658…\n$ x1092_nm <dbl> -0.064209, -0.060839, -0.063744, -0.063369, -0.0657…\n$ x1094_nm <dbl> -0.064162, -0.060784, -0.063969, -0.063106, -0.0659…\n$ x1096_nm <dbl> -0.063681, -0.060401, -0.063588, -0.062210, -0.0654…\n$ x1098_nm <dbl> -0.062757, -0.059631, -0.062893, -0.061071, -0.0646…\n$ x1100_nm <dbl> -0.061833, -0.058887, -0.062346, -0.060476, -0.0637…\n$ x1102_nm <dbl> -0.060653, -0.057861, -0.060954, -0.059613, -0.0623…\n$ x1104_nm <dbl> -0.059872, -0.057174, -0.060277, -0.059053, -0.0614…\n$ x1106_nm <dbl> -0.058221, -0.055943, -0.058933, -0.057879, -0.0596…\n$ x1108_nm <dbl> -0.057000, -0.054586, -0.057725, -0.056993, -0.0583…\n$ x1110_nm <dbl> -0.054962, -0.052730, -0.056403, -0.055674, -0.0566…\n$ x1112_nm <dbl> -0.052726, -0.050650, -0.054639, -0.053633, -0.0544…\n$ x1114_nm <dbl> -0.050188, -0.047988, -0.052604, -0.051368, -0.0521…\n$ x1116_nm <dbl> -0.047313, -0.045184, -0.050002, -0.048753, -0.0490…\n$ x1118_nm <dbl> -0.044383, -0.042775, -0.047952, -0.046565, -0.0458…\n$ x1120_nm <dbl> -0.040658, -0.039567, -0.044838, -0.043878, -0.0415…\n$ x1122_nm <dbl> -0.036981, -0.036483, -0.041595, -0.041106, -0.0366…\n$ x1124_nm <dbl> -0.030389, -0.031762, -0.036552, -0.036145, -0.0297…\n$ x1126_nm <dbl> -0.022454, -0.026061, -0.030222, -0.029643, -0.0205…\n$ x1128_nm <dbl> -0.011760, -0.017961, -0.021030, -0.020369, -0.0080…\n$ x1130_nm <dbl> 0.000941, -0.008352, -0.010311, -0.009442, 0.006771…\n$ x1132_nm <dbl> 0.018067, 0.004104, 0.003633, 0.005123, 0.026072, 0…\n$ x1134_nm <dbl> 0.039563, 0.020129, 0.021227, 0.023279, 0.050595, 0…\n$ x1136_nm <dbl> 0.065490, 0.040174, 0.043017, 0.045674, 0.080282, 0…\n$ x1138_nm <dbl> 0.093330, 0.061190, 0.065780, 0.069009, 0.111046, 0…\n$ x1140_nm <dbl> 0.119886, 0.082808, 0.088740, 0.092430, 0.140255, 0…\n$ x1142_nm <dbl> 0.143335, 0.103309, 0.110123, 0.113654, 0.162922, 0…\n$ x1144_nm <dbl> 0.161862, 0.121239, 0.128801, 0.131457, 0.178850, 0…\n$ x1146_nm <dbl> 0.178692, 0.139799, 0.147827, 0.148927, 0.192453, 0…\n$ x1148_nm <dbl> 0.190440, 0.154684, 0.163220, 0.162499, 0.200921, 0…\n$ x1150_nm <dbl> 0.194380, 0.163895, 0.171823, 0.169830, 0.200736, 0…\n$ x1152_nm <dbl> 0.187846, 0.163702, 0.170286, 0.168042, 0.189637, 0…\n$ x1154_nm <dbl> 0.173436, 0.155861, 0.160042, 0.159366, 0.170796, 0…\n$ x1156_nm <dbl> 0.158487, 0.146830, 0.147558, 0.149124, 0.152340, 0…\n$ x1158_nm <dbl> 0.146299, 0.139954, 0.137904, 0.141988, 0.138688, 0…\n$ x1160_nm <dbl> 0.143034, 0.140520, 0.136222, 0.142149, 0.133949, 0…\n$ x1162_nm <dbl> 0.146884, 0.147677, 0.142161, 0.148574, 0.137171, 0…\n$ x1164_nm <dbl> 0.156093, 0.159739, 0.153683, 0.159490, 0.146438, 0…\n$ x1166_nm <dbl> 0.167857, 0.174511, 0.168846, 0.173304, 0.159196, 0…\n$ x1168_nm <dbl> 0.179468, 0.189393, 0.184297, 0.186928, 0.172705, 0…\n$ x1170_nm <dbl> 0.192422, 0.204829, 0.201057, 0.201306, 0.187194, 0…\n$ x1172_nm <dbl> 0.205774, 0.222001, 0.219339, 0.217320, 0.202939, 0…\n$ x1174_nm <dbl> 0.222009, 0.240784, 0.238474, 0.235025, 0.221091, 0…\n$ x1176_nm <dbl> 0.238851, 0.259705, 0.257398, 0.253360, 0.239527, 0…\n$ x1178_nm <dbl> 0.257624, 0.279918, 0.277868, 0.273140, 0.260289, 0…\n$ x1180_nm <dbl> 0.282144, 0.305844, 0.303828, 0.298527, 0.286929, 0…\n$ x1182_nm <dbl> 0.316637, 0.342649, 0.340386, 0.334143, 0.324194, 0…\n$ x1184_nm <dbl> 0.360689, 0.390617, 0.388889, 0.379790, 0.370256, 0…\n$ x1186_nm <dbl> 0.410005, 0.445219, 0.444171, 0.431122, 0.418339, 0…\n$ x1188_nm <dbl> 0.454409, 0.497316, 0.497294, 0.478623, 0.458811, 0…\n$ x1190_nm <dbl> 0.485965, 0.535230, 0.535431, 0.512772, 0.480722, 0…\n$ x1192_nm <dbl> 0.497682, 0.552865, 0.552290, 0.529281, 0.483581, 0…\n$ x1194_nm <dbl> 0.489535, 0.545858, 0.541118, 0.523347, 0.465097, 0…\n$ x1196_nm <dbl> 0.466098, 0.516133, 0.504049, 0.497873, 0.432696, 0…\n$ x1198_nm <dbl> 0.432583, 0.474097, 0.453667, 0.462310, 0.392652, 0…\n$ x1200_nm <dbl> 0.394805, 0.429533, 0.400454, 0.423781, 0.351462, 0…\n$ x1202_nm <dbl> 0.359732, 0.388781, 0.353163, 0.387603, 0.315084, 0…\n$ x1204_nm <dbl> 0.331966, 0.357836, 0.318767, 0.359001, 0.288042, 0…\n$ x1206_nm <dbl> 0.309186, 0.334257, 0.293785, 0.336412, 0.267363, 0…\n$ x1208_nm <dbl> 0.292567, 0.316834, 0.276645, 0.319126, 0.251579, 0…\n$ x1210_nm <dbl> 0.275393, 0.300208, 0.260872, 0.301999, 0.235251, 0…\n$ x1212_nm <dbl> 0.255452, 0.280041, 0.242538, 0.282530, 0.216596, 0…\n$ x1214_nm <dbl> 0.235247, 0.259029, 0.224122, 0.261341, 0.198460, 0…\n$ x1216_nm <dbl> 0.213967, 0.237029, 0.206128, 0.239522, 0.181406, 0…\n$ x1218_nm <dbl> 0.192011, 0.215364, 0.188049, 0.218119, 0.165114, 0…\n$ x1220_nm <dbl> 0.170424, 0.192902, 0.169174, 0.195463, 0.148187, 0…\n$ x1222_nm <dbl> 0.150943, 0.171982, 0.152376, 0.174744, 0.133288, 0…\n$ x1224_nm <dbl> 0.132428, 0.151256, 0.136216, 0.154083, 0.118446, 0…\n$ x1226_nm <dbl> 0.115921, 0.133448, 0.121756, 0.135921, 0.105215, 0…\n$ x1228_nm <dbl> 0.099535, 0.115749, 0.106574, 0.118325, 0.090738, 0…\n$ x1230_nm <dbl> 0.084481, 0.099265, 0.090796, 0.101493, 0.076775, 0…\n$ x1232_nm <dbl> 0.071254, 0.084244, 0.076286, 0.086168, 0.063483, 0…\n$ x1234_nm <dbl> 0.058049, 0.070886, 0.062592, 0.072287, 0.050650, 0…\n$ x1236_nm <dbl> 0.047446, 0.060095, 0.051793, 0.061218, 0.040134, 0…\n$ x1238_nm <dbl> 0.036880, 0.049378, 0.041423, 0.050044, 0.030024, 0…\n$ x1240_nm <dbl> 0.029082, 0.040630, 0.033518, 0.040826, 0.021975, 0…\n$ x1242_nm <dbl> 0.021789, 0.032988, 0.026652, 0.032597, 0.015352, 0…\n$ x1244_nm <dbl> 0.016038, 0.026985, 0.021066, 0.026030, 0.010021, 0…\n$ x1246_nm <dbl> 0.011251, 0.021400, 0.016164, 0.020097, 0.005572, 0…\n$ x1248_nm <dbl> 0.006186, 0.016041, 0.011051, 0.014526, 0.001400, 0…\n$ x1250_nm <dbl> 0.003116, 0.012094, 0.007728, 0.010441, -0.001737, …\n$ x1252_nm <dbl> -0.000998, 0.007511, 0.003719, 0.005578, -0.005442,…\n$ x1254_nm <dbl> -0.004703, 0.003694, -0.000068, 0.001684, -0.008756…\n$ x1256_nm <dbl> -0.009270, -0.001214, -0.004753, -0.002925, -0.0128…\n$ x1258_nm <dbl> -0.012907, -0.005412, -0.008519, -0.007083, -0.0162…\n$ x1260_nm <dbl> -0.017234, -0.010057, -0.012552, -0.011555, -0.0201…\n$ x1262_nm <dbl> -0.020832, -0.014084, -0.016265, -0.015370, -0.0235…\n$ x1264_nm <dbl> -0.024051, -0.017764, -0.019766, -0.018619, -0.0270…\n$ x1266_nm <dbl> -0.027297, -0.021488, -0.023338, -0.021719, -0.0301…\n$ x1268_nm <dbl> -0.029792, -0.024172, -0.025883, -0.023654, -0.0323…\n$ x1270_nm <dbl> -0.032400, -0.027224, -0.029186, -0.025991, -0.0353…\n$ x1272_nm <dbl> -0.033746, -0.029156, -0.031037, -0.027169, -0.0369…\n$ x1274_nm <dbl> -0.035721, -0.031021, -0.033466, -0.028784, -0.0383…\n$ x1276_nm <dbl> -0.036701, -0.032030, -0.034469, -0.029554, -0.0394…\n$ x1278_nm <dbl> -0.037363, -0.032565, -0.035675, -0.030701, -0.0404…\n$ x1280_nm <dbl> -0.037377, -0.032049, -0.035766, -0.031122, -0.0405…\n$ x1282_nm <dbl> -0.037489, -0.031394, -0.036086, -0.031775, -0.0405…\n$ x1284_nm <dbl> -0.037695, -0.031097, -0.036513, -0.032641, -0.0406…\n$ x1286_nm <dbl> -0.036906, -0.029836, -0.036003, -0.032724, -0.0402…\n$ x1288_nm <dbl> -0.037094, -0.029678, -0.036171, -0.033748, -0.0403…\n$ x1290_nm <dbl> -0.036752, -0.028968, -0.035911, -0.034132, -0.0398…\n$ x1292_nm <dbl> -0.037211, -0.029269, -0.036362, -0.035013, -0.0398…\n$ x1294_nm <dbl> -0.037308, -0.029262, -0.036318, -0.035620, -0.0397…\n$ x1296_nm <dbl> -0.037673, -0.029877, -0.036997, -0.036407, -0.0400…\n$ x1298_nm <dbl> -0.037877, -0.030030, -0.037092, -0.036595, -0.0400…\n$ x1300_nm <dbl> -0.038212, -0.030565, -0.037218, -0.036742, -0.0400…\n$ x1302_nm <dbl> -0.038426, -0.031404, -0.037411, -0.037076, -0.0401…\n$ x1304_nm <dbl> -0.038534, -0.032320, -0.037429, -0.037230, -0.0403…\n$ x1306_nm <dbl> -0.039103, -0.033693, -0.038010, -0.037877, -0.0411…\n$ x1308_nm <dbl> -0.038933, -0.034155, -0.037523, -0.037530, -0.0406…\n$ x1310_nm <dbl> -0.039611, -0.035496, -0.038127, -0.038428, -0.0415…\n$ x1312_nm <dbl> -0.039168, -0.035629, -0.037544, -0.037948, -0.0407…\n$ x1314_nm <dbl> -0.038598, -0.035932, -0.037255, -0.037560, -0.0402…\n$ x1316_nm <dbl> -0.037733, -0.035289, -0.036148, -0.036566, -0.0390…\n$ x1318_nm <dbl> -0.037027, -0.034887, -0.035291, -0.035762, -0.0379…\n$ x1320_nm <dbl> -0.035891, -0.034467, -0.034674, -0.035156, -0.0374…\n$ x1322_nm <dbl> -0.034290, -0.033427, -0.033358, -0.033697, -0.0361…\n$ x1324_nm <dbl> -0.033224, -0.032634, -0.032758, -0.032653, -0.0353…\n$ x1326_nm <dbl> -0.031475, -0.030815, -0.031228, -0.030570, -0.0334…\n$ x1328_nm <dbl> -0.030071, -0.029493, -0.030097, -0.028914, -0.0324…\n$ x1330_nm <dbl> -0.028086, -0.027298, -0.027881, -0.026203, -0.0302…\n$ x1332_nm <dbl> -0.026779, -0.026081, -0.026697, -0.024685, -0.0290…\n$ x1334_nm <dbl> -0.024450, -0.024199, -0.024809, -0.022470, -0.0267…\n$ x1336_nm <dbl> -0.022709, -0.022546, -0.022950, -0.020930, -0.0245…\n$ x1338_nm <dbl> -0.020088, -0.020154, -0.020248, -0.018910, -0.0215…\n$ x1340_nm <dbl> -0.016814, -0.017032, -0.016845, -0.016121, -0.0179…\n$ x1342_nm <dbl> -0.013638, -0.013717, -0.013423, -0.013279, -0.0139…\n$ x1344_nm <dbl> -0.008755, -0.008264, -0.007559, -0.008334, -0.0080…\n$ x1346_nm <dbl> -0.003402, -0.002064, -0.000991, -0.003372, -0.0018…\n$ x1348_nm <dbl> 0.004585, 0.007304, 0.008786, 0.004753, 0.007269, 0…\n$ x1350_nm <dbl> 0.015708, 0.020121, 0.022739, 0.016161, 0.019779, 0…\n$ x1352_nm <dbl> 0.032342, 0.039083, 0.043209, 0.033429, 0.038201, 0…\n$ x1354_nm <dbl> 0.055075, 0.064193, 0.069797, 0.056411, 0.062477, 0…\n$ x1356_nm <dbl> 0.083261, 0.095086, 0.102595, 0.085082, 0.092206, 0…\n$ x1358_nm <dbl> 0.114275, 0.128027, 0.137954, 0.116397, 0.123556, 0…\n$ x1360_nm <dbl> 0.142234, 0.158800, 0.169100, 0.145471, 0.151961, 0…\n$ x1362_nm <dbl> 0.165117, 0.183265, 0.194530, 0.168918, 0.174343, 0…\n$ x1364_nm <dbl> 0.181014, 0.201249, 0.212629, 0.185935, 0.189944, 0…\n$ x1366_nm <dbl> 0.194140, 0.216701, 0.227677, 0.200819, 0.202222, 0…\n$ x1368_nm <dbl> 0.207728, 0.232922, 0.243204, 0.215868, 0.215936, 0…\n$ x1370_nm <dbl> 0.225084, 0.252380, 0.262941, 0.234213, 0.233155, 0…\n$ x1372_nm <dbl> 0.245205, 0.274944, 0.285764, 0.255771, 0.254445, 0…\n$ x1374_nm <dbl> 0.267331, 0.298853, 0.309733, 0.279190, 0.277749, 0…\n$ x1376_nm <dbl> 0.290558, 0.322715, 0.333065, 0.303732, 0.301495, 0…\n$ x1378_nm <dbl> 0.311237, 0.342958, 0.352530, 0.325323, 0.320695, 0…\n$ x1380_nm <dbl> 0.328961, 0.360426, 0.369372, 0.344871, 0.336187, 0…\n$ x1382_nm <dbl> 0.341901, 0.372804, 0.380216, 0.359555, 0.345160, 0…\n$ x1384_nm <dbl> 0.350544, 0.381960, 0.387849, 0.370893, 0.350677, 0…\n$ x1386_nm <dbl> 0.357751, 0.389191, 0.394677, 0.378461, 0.355522, 0…\n$ x1388_nm <dbl> 0.364172, 0.397112, 0.402078, 0.386247, 0.362320, 0…\n$ x1390_nm <dbl> 0.370471, 0.403062, 0.408582, 0.392268, 0.368313, 0…\n$ x1392_nm <dbl> 0.373154, 0.406006, 0.410932, 0.393815, 0.370052, 0…\n$ x1394_nm <dbl> 0.368176, 0.401295, 0.406170, 0.388471, 0.365085, 0…\n$ x1396_nm <dbl> 0.356007, 0.389064, 0.393839, 0.376881, 0.352979, 0…\n$ x1398_nm <dbl> 0.341120, 0.373591, 0.376215, 0.360814, 0.337215, 0…\n$ x1400_nm <dbl> 0.325587, 0.356204, 0.357096, 0.344653, 0.321339, 0…\n$ x1402_nm <dbl> 0.313571, 0.342050, 0.340687, 0.331744, 0.308459, 0…\n$ x1404_nm <dbl> 0.306847, 0.332780, 0.329190, 0.323812, 0.299508, 0…\n$ x1406_nm <dbl> 0.305310, 0.329005, 0.322999, 0.321500, 0.295653, 0…\n$ x1408_nm <dbl> 0.304471, 0.328951, 0.321164, 0.323119, 0.295204, 0…\n$ x1410_nm <dbl> 0.304125, 0.328051, 0.319110, 0.322928, 0.292860, 0…\n$ x1412_nm <dbl> 0.298698, 0.322452, 0.311924, 0.317972, 0.286862, 0…\n$ x1414_nm <dbl> 0.285589, 0.308979, 0.298671, 0.305534, 0.274217, 0…\n$ x1416_nm <dbl> 0.267933, 0.290704, 0.280373, 0.288311, 0.257335, 0…\n$ x1418_nm <dbl> 0.247573, 0.269241, 0.258819, 0.267978, 0.237130, 0…\n$ x1420_nm <dbl> 0.229774, 0.249374, 0.238800, 0.248524, 0.219594, 0…\n$ x1422_nm <dbl> 0.215690, 0.233860, 0.222677, 0.232934, 0.205760, 0…\n$ x1424_nm <dbl> 0.207192, 0.224821, 0.213798, 0.223277, 0.197859, 0…\n$ x1426_nm <dbl> 0.202051, 0.220504, 0.209824, 0.217777, 0.195243, 0…\n$ x1428_nm <dbl> 0.201819, 0.220685, 0.210649, 0.216610, 0.196015, 0…\n$ x1430_nm <dbl> 0.202353, 0.222163, 0.212908, 0.216723, 0.197585, 0…\n$ x1432_nm <dbl> 0.201612, 0.222347, 0.214331, 0.216083, 0.198133, 0…\n$ x1434_nm <dbl> 0.199291, 0.221434, 0.214048, 0.214255, 0.196885, 0…\n$ x1436_nm <dbl> 0.193770, 0.215752, 0.209266, 0.208305, 0.190623, 0…\n$ x1438_nm <dbl> 0.186669, 0.209041, 0.202558, 0.201097, 0.184152, 0…\n$ x1440_nm <dbl> 0.178426, 0.200297, 0.193675, 0.192070, 0.175524, 0…\n$ x1442_nm <dbl> 0.171231, 0.192172, 0.185359, 0.183795, 0.167989, 0…\n$ x1444_nm <dbl> 0.164144, 0.183974, 0.176963, 0.176042, 0.159801, 0…\n$ x1446_nm <dbl> 0.157259, 0.175816, 0.169181, 0.168643, 0.152625, 0…\n$ x1448_nm <dbl> 0.149194, 0.167254, 0.159964, 0.160181, 0.144600, 0…\n$ x1450_nm <dbl> 0.139789, 0.156358, 0.149142, 0.150664, 0.134160, 0…\n$ x1452_nm <dbl> 0.130173, 0.146486, 0.139082, 0.141494, 0.125008, 0…\n$ x1454_nm <dbl> 0.120745, 0.137275, 0.129739, 0.133234, 0.115873, 0…\n$ x1456_nm <dbl> 0.113733, 0.130180, 0.122022, 0.126515, 0.109185, 0…\n$ x1458_nm <dbl> 0.107519, 0.123365, 0.116297, 0.120700, 0.103171, 0…\n$ x1460_nm <dbl> 0.103765, 0.119438, 0.113173, 0.117645, 0.100027, 0…\n$ x1462_nm <dbl> 0.101175, 0.116286, 0.110954, 0.115052, 0.098121, 0…\n$ x1464_nm <dbl> 0.100106, 0.114482, 0.110095, 0.113629, 0.097745, 0…\n$ x1466_nm <dbl> 0.099068, 0.112444, 0.109058, 0.112397, 0.097652, 0…\n$ x1468_nm <dbl> 0.097315, 0.108714, 0.106665, 0.108961, 0.096193, 0…\n$ x1470_nm <dbl> 0.095724, 0.106003, 0.104405, 0.106436, 0.095738, 0…\n$ x1472_nm <dbl> 0.093093, 0.102389, 0.100941, 0.102798, 0.093557, 0…\n$ x1474_nm <dbl> 0.090002, 0.098235, 0.097474, 0.098874, 0.090709, 0…\n$ x1476_nm <dbl> 0.086612, 0.093959, 0.093497, 0.094944, 0.087039, 0…\n$ x1478_nm <dbl> 0.084922, 0.091774, 0.091673, 0.092621, 0.085000, 0…\n$ x1480_nm <dbl> 0.080780, 0.087636, 0.087493, 0.088422, 0.080375, 0…\n$ x1482_nm <dbl> 0.077849, 0.084646, 0.084593, 0.084868, 0.077113, 0…\n$ x1484_nm <dbl> 0.074536, 0.082097, 0.081474, 0.081632, 0.073986, 0…\n$ x1486_nm <dbl> 0.070910, 0.079089, 0.078340, 0.078056, 0.069980, 0…\n$ x1488_nm <dbl> 0.068700, 0.077735, 0.076682, 0.076356, 0.067515, 0…\n$ x1490_nm <dbl> 0.064584, 0.074444, 0.073916, 0.072892, 0.064092, 0…\n$ x1492_nm <dbl> 0.061726, 0.071813, 0.071351, 0.069921, 0.060768, 0…\n$ x1494_nm <dbl> 0.057395, 0.068586, 0.068287, 0.066060, 0.057105, 0…\n$ x1496_nm <dbl> 0.053704, 0.065580, 0.065568, 0.062555, 0.054163, 0…\n$ x1498_nm <dbl> 0.049329, 0.061501, 0.061216, 0.057895, 0.050508, 0…\n$ x1500_nm <dbl> 0.044695, 0.056740, 0.055821, 0.052995, 0.045830, 0…\n$ x1502_nm <dbl> 0.039937, 0.051068, 0.049529, 0.047908, 0.040213, 0…\n$ x1504_nm <dbl> 0.034641, 0.045104, 0.043150, 0.042637, 0.034258, 0…\n$ x1506_nm <dbl> 0.030164, 0.040192, 0.037184, 0.038187, 0.029202, 0…\n$ x1508_nm <dbl> 0.024898, 0.034145, 0.030828, 0.032880, 0.023555, 0…\n$ x1510_nm <dbl> 0.021588, 0.029970, 0.026308, 0.029345, 0.019996, 0…\n$ x1512_nm <dbl> 0.017102, 0.024131, 0.020805, 0.024026, 0.015186, 0…\n$ x1514_nm <dbl> 0.014860, 0.020670, 0.016907, 0.020904, 0.012201, 0…\n$ x1516_nm <dbl> 0.010854, 0.016931, 0.013194, 0.017431, 0.008836, 0…\n$ x1518_nm <dbl> 0.008205, 0.013083, 0.009759, 0.014136, 0.005561, 0…\n$ x1520_nm <dbl> 0.005869, 0.010199, 0.007271, 0.011749, 0.003463, 0…\n$ x1522_nm <dbl> 0.003920, 0.007926, 0.004796, 0.009634, 0.001051, 0…\n$ x1524_nm <dbl> 0.002107, 0.005733, 0.002720, 0.007699, -0.001164, …\n$ x1526_nm <dbl> -0.000203, 0.002833, 0.000243, 0.005545, -0.003490,…\n$ x1528_nm <dbl> -0.001072, 0.001596, -0.001196, 0.004693, -0.004459…\n$ x1530_nm <dbl> -0.003325, -0.000928, -0.003965, 0.002553, -0.00740…\n$ x1532_nm <dbl> -0.004707, -0.002545, -0.005704, 0.001494, -0.00901…\n$ x1534_nm <dbl> -0.006635, -0.003656, -0.007156, -0.000102, -0.0101…\n$ x1536_nm <dbl> -0.008403, -0.006217, -0.009860, -0.003056, -0.0127…\n$ x1538_nm <dbl> -0.009317, -0.006885, -0.010959, -0.004391, -0.0138…\n$ x1540_nm <dbl> -0.011351, -0.008307, -0.012194, -0.006868, -0.0151…\n$ x1542_nm <dbl> -0.012280, -0.009142, -0.013605, -0.008665, -0.0158…\n$ x1544_nm <dbl> -0.013474, -0.010409, -0.014564, -0.010355, -0.0171…\n$ x1546_nm <dbl> -0.014453, -0.010983, -0.015313, -0.011261, -0.0175…\n$ x1548_nm <dbl> -0.015262, -0.012033, -0.015723, -0.011896, -0.0181…\n$ x1550_nm <dbl> -0.015588, -0.012020, -0.015263, -0.011357, -0.0179…\n$ x1552_nm <dbl> -0.016349, -0.013442, -0.016537, -0.012562, -0.0191…\n$ x1554_nm <dbl> -0.016028, -0.012709, -0.015581, -0.011408, -0.0183…\n$ x1556_nm <dbl> -0.016209, -0.011563, -0.014147, -0.010164, -0.0173…\n$ x1558_nm <dbl> -0.015985, -0.010432, -0.013280, -0.009194, -0.0168…\n$ x1560_nm <dbl> -0.015483, -0.010091, -0.013291, -0.009642, -0.0167…\n$ x1562_nm <dbl> -0.014216, -0.007960, -0.012268, -0.008700, -0.0155…\n$ x1564_nm <dbl> -0.013790, -0.006697, -0.011176, -0.007899, -0.0145…\n$ x1566_nm <dbl> -0.013844, -0.005367, -0.010897, -0.006986, -0.0146…\n$ x1568_nm <dbl> -0.011773, -0.003432, -0.009383, -0.005475, -0.0131…\n$ x1570_nm <dbl> -0.010432, -0.002665, -0.008983, -0.004838, -0.0130…\n$ x1572_nm <dbl> -0.009606, -0.001307, -0.007542, -0.003785, -0.0112…\n$ x1574_nm <dbl> -0.008209, -0.000406, -0.005716, -0.002646, -0.0101…\n$ x1576_nm <dbl> -0.006732, 0.001300, -0.003888, -0.001373, -0.00784…\n$ x1578_nm <dbl> -0.004573, 0.003107, -0.001115, 0.000559, -0.005406…\n$ x1580_nm <dbl> -0.003433, 0.003075, 0.000176, 0.001422, -0.004259,…\n$ x1582_nm <dbl> -0.001200, 0.004513, 0.002464, 0.003357, -0.002039,…\n$ x1584_nm <dbl> -0.000066, 0.005336, 0.003951, 0.005481, -0.000655,…\n$ x1586_nm <dbl> 0.002759, 0.008496, 0.008541, 0.010048, 0.003930, 0…\n$ x1588_nm <dbl> 0.004036, 0.009262, 0.009676, 0.011528, 0.004579, 0…\n$ x1590_nm <dbl> 0.006620, 0.010982, 0.012153, 0.014357, 0.007207, 0…\n$ x1592_nm <dbl> 0.008709, 0.012330, 0.013639, 0.015642, 0.008612, 0…\n$ x1594_nm <dbl> 0.011810, 0.015301, 0.016898, 0.018304, 0.012129, 0…\n$ x1596_nm <dbl> 0.014426, 0.018270, 0.019847, 0.020612, 0.015541, 0…\n$ x1598_nm <dbl> 0.017254, 0.020912, 0.022470, 0.022399, 0.018535, 0…\n$ x1600_nm <dbl> 0.020633, 0.024617, 0.026069, 0.025038, 0.022522, 0…\n$ x1602_nm <dbl> 0.025032, 0.028188, 0.029714, 0.027860, 0.026739, 0…\n$ x1604_nm <dbl> 0.029496, 0.033321, 0.034281, 0.032481, 0.032223, 0…\n$ x1606_nm <dbl> 0.034242, 0.037755, 0.039133, 0.036772, 0.037103, 0…\n$ x1608_nm <dbl> 0.039735, 0.043515, 0.045228, 0.042282, 0.043411, 0…\n$ x1610_nm <dbl> 0.046803, 0.050530, 0.051833, 0.049100, 0.051336, 0…\n$ x1612_nm <dbl> 0.053743, 0.058033, 0.059036, 0.056030, 0.059517, 0…\n$ x1614_nm <dbl> 0.062788, 0.067009, 0.068655, 0.065351, 0.069594, 0…\n$ x1616_nm <dbl> 0.073531, 0.077781, 0.079319, 0.075811, 0.081145, 0…\n$ x1618_nm <dbl> 0.087246, 0.091191, 0.092339, 0.089083, 0.096240, 0…\n$ x1620_nm <dbl> 0.104000, 0.106815, 0.107952, 0.105323, 0.112862, 0…\n$ x1622_nm <dbl> 0.124210, 0.123946, 0.123435, 0.123006, 0.131164, 0…\n$ x1624_nm <dbl> 0.147924, 0.146013, 0.143631, 0.145753, 0.153401, 0…\n$ x1626_nm <dbl> 0.175024, 0.171884, 0.166498, 0.173142, 0.178326, 0…\n$ x1628_nm <dbl> 0.202711, 0.197074, 0.188171, 0.200956, 0.202094, 0…\n$ x1630_nm <dbl> 0.232547, 0.224837, 0.210848, 0.231804, 0.228229, 0…\n$ x1632_nm <dbl> 0.259828, 0.250088, 0.232041, 0.259687, 0.252492, 0…\n$ x1634_nm <dbl> 0.282117, 0.270191, 0.249246, 0.281929, 0.273077, 0…\n$ x1636_nm <dbl> 0.299937, 0.284011, 0.261166, 0.297769, 0.290969, 0…\n$ x1638_nm <dbl> 0.313416, 0.290728, 0.271317, 0.308358, 0.307760, 0…\n$ x1640_nm <dbl> 0.322870, 0.294182, 0.276957, 0.311430, 0.321736, 0…\n$ x1642_nm <dbl> 0.327217, 0.293247, 0.279662, 0.311149, 0.331647, 0…\n$ x1644_nm <dbl> 0.331907, 0.289916, 0.280366, 0.308663, 0.337756, 0…\n$ x1646_nm <dbl> 0.336522, 0.291554, 0.284626, 0.308939, 0.344823, 0…\n$ x1648_nm <dbl> 0.341847, 0.293929, 0.289691, 0.311825, 0.352189, 0…\n$ x1650_nm <dbl> 0.350585, 0.301816, 0.297987, 0.319857, 0.362563, 0…\n$ x1652_nm <dbl> 0.368032, 0.316670, 0.312909, 0.335700, 0.379202, 0…\n$ x1654_nm <dbl> 0.393494, 0.336533, 0.333932, 0.355742, 0.404428, 0…\n$ x1656_nm <dbl> 0.427370, 0.363209, 0.362284, 0.384754, 0.441534, 0…\n$ x1658_nm <dbl> 0.472357, 0.399271, 0.400352, 0.424769, 0.494011, 0…\n$ x1660_nm <dbl> 0.528721, 0.443964, 0.445686, 0.472003, 0.557655, 0…\n$ x1662_nm <dbl> 0.593168, 0.499024, 0.504338, 0.531050, 0.635493, 0…\n$ x1664_nm <dbl> 0.666647, 0.559683, 0.567039, 0.596606, 0.721107, 0…\n$ x1666_nm <dbl> 0.743945, 0.624577, 0.635874, 0.662997, 0.803044, 0…\n$ x1668_nm <dbl> 0.833972, 0.697450, 0.711690, 0.744126, 0.893778, 0…\n$ x1670_nm <dbl> 0.909368, 0.770433, 0.792939, 0.817398, 0.968985, 0…\n$ x1672_nm <dbl> 0.985332, 0.848769, 0.872021, 0.895422, 1.050312, 0…\n$ x1674_nm <dbl> 1.051159, 0.916590, 0.938915, 0.959761, 1.104573, 1…\n$ x1676_nm <dbl> 1.107395, 0.975048, 1.004690, 1.021595, 1.152964, 1…\n$ x1678_nm <dbl> 1.158488, 1.036499, 1.067934, 1.082497, 1.192131, 1…\n$ x1680_nm <dbl> 1.174795, 1.085067, 1.108813, 1.117447, 1.219254, 1…\n$ x1682_nm <dbl> 1.198461, 1.128877, 1.147964, 1.160089, 1.252712, 1…\n$ x1684_nm <dbl> 1.224243, 1.148342, 1.167798, 1.169350, 1.238013, 1…\n$ x1686_nm <dbl> 1.242645, 1.189116, 1.198287, 1.201066, 1.259616, 1…\n$ x1688_nm <dbl> 1.250789, 1.223242, 1.237383, 1.233299, 1.273713, 1…\n$ x1690_nm <dbl> 1.246626, 1.253306, 1.260979, 1.262966, 1.296524, 1…\n$ x1692_nm <dbl> 1.250985, 1.282889, 1.276677, 1.272709, 1.299507, 1…\n$ x1694_nm <dbl> 1.264189, 1.215065, 1.218871, 1.211068, 1.226448, 1…\n$ x1696_nm <dbl> 1.244678, 1.225211, 1.223132, 1.215044, 1.230718, 1…\n$ x1698_nm <dbl> 1.245913, 1.227985, 1.230321, 1.232655, 1.232864, 1…\n$ x1700_nm <dbl> 1.221135, 1.198851, 1.208742, 1.206696, 1.202926, 1…\n\nEDA\nCheck for missing values using purrr::map()\n\n\ngasoline_tidy %>% \n  purrr::map(is.na) %>% \n  map_df(sum) %>% \n  tidy() %>% \n  dplyr::select(column, mean) %>% \n  as_tibble() %>% \n  filter(mean>0) # no missing values\n\n\n# A tibble: 0 x 2\n# … with 2 variables: column <chr>, mean <dbl>\n\nFor other datasets, one should check the correlation again, but in this case, as this is a NIR dataset, I will not do this step.\nSplit dataset\n\n\n# initial split\nset.seed(20210308)\ngasoline_split <- initial_split(gasoline_tidy, prop = 0.8)\n\ngasoline_training <- gasoline_split %>% \n  training()\n\ngasoline_testing <- gasoline_split %>% \n  testing()\n\ngasoline_cv <- vfold_cv(gasoline_training) # to tune number of components later\n\n\n\nModelling\n\n\n# recipe\n\ngasoline_reciped <- recipe(octane ~ ., data = gasoline_training) %>% \n  update_role(octane, new_role = \"outcome\") %>% \n  step_normalize(all_predictors())\n\ngasoline_reciped\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor        401\n\nOperations:\n\nCentering and scaling for all_predictors()\n\n# fit model\n\npls_model <- plsmod::pls(num_comp = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"mixOmics\")\n\n# put into workflow\n\npls_workflow <- workflow() %>% \n  add_recipe(gasoline_reciped) %>% \n  add_model(pls_model)\n\n# create grid\npls_grid <- expand.grid(num_comp = seq (from = 1, to = 20, by = 1))\n\ntuned_pls_results <- pls_workflow %>% \n  tune_grid(resamples = gasoline_cv,\n            grid = pls_grid,\n            metrics = metric_set(mae, rmse, rsq))\n\n(model_results <- tuned_pls_results %>% \n  collect_metrics())\n\n\n# A tibble: 60 x 7\n   num_comp .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1        1 mae     standard   1.15     10 0.122   Preprocessor1_Mode…\n 2        1 rmse    standard   1.29     10 0.137   Preprocessor1_Mode…\n 3        1 rsq     standard   0.371    10 0.104   Preprocessor1_Mode…\n 4        2 mae     standard   0.573    10 0.0745  Preprocessor1_Mode…\n 5        2 rmse    standard   0.691    10 0.0876  Preprocessor1_Mode…\n 6        2 rsq     standard   0.716    10 0.0874  Preprocessor1_Mode…\n 7        3 mae     standard   0.212    10 0.0280  Preprocessor1_Mode…\n 8        3 rmse    standard   0.259    10 0.0289  Preprocessor1_Mode…\n 9        3 rsq     standard   0.972    10 0.00967 Preprocessor1_Mode…\n10        4 mae     standard   0.187    10 0.0177  Preprocessor1_Mode…\n# … with 50 more rows\n\nVisualize:\n\n\n# plot\n\ntuned_pls_results %>% \n  collect_metrics() %>% \n  ggplot(aes(num_comp, mean, col = .metric)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(n.breaks = 20) +\n  labs(x = \"Number of components\",\n       y = \"Indicator\",\n       title = \"Plot of MAE, RMSE and R-SQ vs number of components for TRAINING dataset, with 10-fold repeated cross validation\",\n       subtitle = \"Optimal number of components is 3\") +\n facet_grid(.metric ~.) +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n\nCheck against numerical values:\n\n\n# check against numerical values\ntuned_best <- tuned_pls_results %>% \n  select_best(\"rsq\") \n\ntuned_best\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1        4 Preprocessor1_Model04\n\n# check with other indicators\ntuned_pls_results %>% \n  select_best(\"mae\") \n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1        4 Preprocessor1_Model04\n\ntuned_pls_results %>% \n  select_best(\"rmse\")\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1        4 Preprocessor1_Model04\n\nIn this case, I will go for num_comp = 4. The aim of PLS is to reduce number of variables, but there would be a slight trade-off in terms of accuracy/error in favor of simplicity of model when determining the number of components.\nUpdate model and workflow:\n\n\nupdated_pls_model <-  plsmod::pls(num_comp = 4) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"mixOmics\")\n\nupdated_workflow <- pls_workflow %>% \n  update_model(updated_pls_model)\n  \n\npls_fit <- updated_workflow %>% \n  fit(data = gasoline_training)\n\n\n\nCheck the most important X variables for the updated model:\n\n\n# check the most important predictors\n\ntidy_pls <- pls_fit %>% \n  pull_workflow_fit() %>% \n  tidy()\n\n\n# variable importance\ntidy_pls %>% \n  filter(term != \"Y\", # outcome variable col name\n         component == c(1:4)) %>% \n  group_by(component) %>% \n  slice_max(abs(value), n = 20) %>% \n  ungroup() %>% \n  ggplot(aes(value, fct_reorder(term, value), fill = factor(component))) +\n  geom_col(show.legend = F) +\n  facet_wrap( ~ component, scales = \"free_y\") +\n  labs( y = NULL) +\n  theme_few()\n\n\n\n\nAssess\n\n\n# results_train\npls_fit %>% \n  predict(new_data = gasoline_training) %>% \n  mutate(truth = gasoline_training$octane) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TRAINING dataset\",\n       x = \"Actual Octane Number\",\n       y = \"Predicted Octane Number\") +\n  theme_few()\n\n\n\npls_fit %>% \n  predict(new_data = gasoline_training) %>% \n  mutate(truth = gasoline_training$octane) %>% \n  metrics(truth = truth, estimate = .pred) %>% \n  as_tibble() # rsq is 0.981 for training dataset\n\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.207\n2 rsq     standard       0.981\n3 mae     standard       0.168\n\n# results_test\npls_fit %>% \n  predict(new_data = gasoline_testing) %>% \n  mutate(truth = gasoline_testing$octane) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point(col = \"deepskyblue3\") +\n  geom_abline(col = \"deepskyblue3\") +\n  labs(title = \"Actual vs Predicted for TESTING dataset\",\n       x = \"Actual Octane Number\",\n       y = \"Predicted Octane Number\") +\n  theme_few()\n\n\n\npls_fit %>% \n  predict(new_data = gasoline_testing) %>% \n  mutate(truth = gasoline_testing$octane) %>% \n  metrics(truth = truth, estimate = .pred) %>% \n  as_tibble() # r-sq is 0.989 for testing dataset\n\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.222\n2 rsq     standard       0.983\n3 mae     standard       0.151\n\nTo predict future data\nIn this case, I will just take the first dataset and check if the predicted value is the actual value. This should not be done, but I did it just to understand how the workflow is.\n\n\ntrial_data <- gasoline_tidy %>% \n  head((1)) %>% \n  dplyr::select(-octane) # octane = 85.3\n\npls_fit %>% \n  predict(trial_data) # 85.3\n\n\n# A tibble: 1 x 1\n  .pred\n  <dbl>\n1  85.2\n\n# actual value = predicted value\n\n\n\nReflections\nThrough this exercise, I learnt:\nhow to format the matrix data into a tibble dataframe. I have not worked with AsIs structure before and had to look up stacksoverflow to understand how to code to get a plot for NIR spectra\nhow to apply tidymodels for pls modelling. I also learnt that there were different types of pls algorithms. Perhaps I would compare the different model algorithms for my next practice.\nhow to extract variable importance\nhow to predict Y variable using a new dataset, although in this case I did not use new data\nI would like to learn how to model for multi-outcome data in the future.\nReferences\nhttps://stackoverflow.com/questions/64254295/predictor-importance-for-pls-model-trained-with-tidymodels\nhttps://rpubs.com/RandallThompson15/HW10_624\nhttps://www.tidyverse.org/blog/2020/04/parsnip-adjacent/\nApplied Predictive Modelling, Max Kuhn and Kjell Johnson, Chapter 6. http://appliedpredictivemodeling.com/\n\n\n\n",
    "preview": "posts/20210309_tidy models plsr gasoline/Tidy-Models---Partial-Least-Squares-Regression---gasoline_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-10T11:55:27+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210301_tidy models regression 1/",
    "title": "Tidy Models - Regression",
    "description": "Predicting numerical outcomes using linear regression and random forest",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-03-01",
    "categories": [],
    "contents": "\nOverview\nThis is an exercise for me to revise the tidymodels way of carrying out regression using linear regression (OLS) and random forest models. Only after going through the readings, my course materials, and actually trying to work through a dataset myself do I really appreciate what my Prof was trying to teach during his machine learning course..\nI will be working on a dataset familiar to me - the white wine dataset.\nMachine learning can be divided into supervised and unsupervised machine learning - the differentiating point is whether you know the outcome. In this case, I want to apply what I have learnt on predicting a known numerical outcome - this would be regression. If I want to predict a known categorical outcome - for eg whether the wine is red or white, then that would be classification. If I am unsure what are the types of wine, and just want to group them, then that would be clustering.\nFor regression, I could work on predicting the quality of wine from the white wine dataset, the quality of wine from the red wine dataset, or the quality of wine from both the red and white wine dataset.\nAs a start, let me try to predict the quality score of white wine from the various attributes.\nGeneral Workflow for Predicting Numerical Outcome using Linear Regression and Random Forest\nMost of the points mentioned below were taken from: https://jhudatascience.org/tidyversecourse/model.html#summary-of-tidymodels.\nThis was a great read for me to frame my learning and see the whole picture\nThe general steps are outlined below:\n1. Import the data\nI will import the white wine dataset from https://archive.ics.uci.edu/ml/datasets/wine+quality. The attributes are:\nFeatures/X variables\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nOutcome/Y variable: 12 - quality (score between 0 and 10)\n2. Define the question you want to ask the data\nCan I predict the quality score (Y variable), based on the physicochemical attributes of white wine (X variables)?\n3. Clean and transform the data\nIn this case, the data is already in a tidy format. As for transformations required, I will leave it to the pre-processing step later.\n4. Exploratory data analysis (EDA)\nWhat do you look out for when doing EDA?\nShape of data: How many Y and how many X variables are there? How many observations are there? The number of observations (n) and number of parameters (p) may affect your choice of models.\nType of variables: are they numerical or categorical? Or are they numerical, but actually can be transformed into categorical (eg month of the year), or are they categorical, but should be transformed into dummy variables (eg for regression)?\nAre there any missing values? This may affect the modelling as some models cannot handle missing values.\nWithin each variable, what is the min, max, mean, median, range? Are the datapoints normally distributed, or skewed? This affects whether modelling, for eg OLS regression can be carried out, or should further transformations be carried out first?\nHow are the X variables related to each other? Are they correlated? What is the strength of correlation? Is there a multi-collinearity issue? These are points to be addressed for linear regression.\n5. Preprocessing the data\nAfter identifying all the “touch-up” points required for successful modelling, the data may be pre-processed using the recipes package. This is really a powerful package that can transform the data the way you want, using single-line commands (as compared to multiple clicks of the mouse). However, it requires the user to know what steps are to be taken.\nA list of preprocessig steps is given below:\nhttps://recipes.tidymodels.org/reference/index.html#section-basic-functions\nLike cooking, this is part art part science. If I want to do missing values imputation, which kind of imputation do I use? I am still learning as I go along for this step..\nIn a way, this preprocessing step helps you to zoom in razor sharp to the important X variables that can be used to predict Y. These X variables may exist as hidden variables that need carving out and polishing/transformation. There may be X variables that are of not much importance, so it is important to extract relevant information and keep the number of variables as small as possible without compromising on accuracy. In other words, that is called feature engineering.\n6. Carry out modelling on the training dataset\nSplit dataset into training, testing and cross-validation. The training dataset is for you to train models with. The cross-validation dataset should be a subset of training dataset, for tuning different parameters of the model to make it an even better model. Cross-validation is useful when the number of observations isn’t big enough to split into three different datasets for training, validation and testing. Instead, the data is randomly partitioned into subsamples to be used as training dataset for one partition, and the same subsample would be used as test dataset for another partition. In repeated cross-validation, the cross-validation is repeated many times, giving random partitions of the original sample. The test dataset is for testing the trained model to see if the model is able to deliver predictive results.\nUsually, you will train more than one model, and then compare the results. The choice of model could span over simple, easy to understand linear models, or difficult to interpret but accurate models (eg neural networks which is like a blackbox). The results of the trained dataset will usually not fare too badly, since the model was built using the training dataset. Different models may require different preprocessing and different types of parameter tuning.\n7. Assessing the test dataset.\nA litmus test of whether the model works is to look at how well the model performs its predictive task when a set of completely new data is provided to the model. Models may be assessed in terms of r-sq, root mean square error or mean absolute error to judge the performance.\nThe indicators above give us an understanding of how accurate the model is in terms of predicting new data. A model that is over-fitted fits the training dataset well, but is unable to predict the test dataset well. A model that can fit the test dataset well, may not be accurate enough to give good predictions. This is known as the bias-variance tradeoff.\n8. Communicate the modelling results\nResults should be shown as visualizations/data tables to communicate the findings.\nLoad required packages\nLoad required packages:\n\n\nlibrary(pacman)\np_load(tidyverse, janitor, GGally, skimr, ggthemes, ggsci,\n       broom, gvlma, ggfortify, jtools, car, huxtable, sandwich,\n       tidymodels, parsnip, vip)\n\n\n\nImport\n\n\nwhite_rawdata <- read.table(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", \n                            strip.white = T, sep = \";\", header = T) %>% \n  as_tibble() \n\n# save as another variable \ndata_white <- white_rawdata\n\n\n\nExploratory data analysis\n\n\nglimpse(data_white) # 12 variables, all numerical\n\n\nRows: 4,898\nColumns: 12\n$ fixed.acidity        <dbl> 7.0, 6.3, 8.1, 7.2, 7.2, 8.1, 6.2, 7.0,…\n$ volatile.acidity     <dbl> 0.27, 0.30, 0.28, 0.23, 0.23, 0.28, 0.3…\n$ citric.acid          <dbl> 0.36, 0.34, 0.40, 0.32, 0.32, 0.40, 0.1…\n$ residual.sugar       <dbl> 20.70, 1.60, 6.90, 8.50, 8.50, 6.90, 7.…\n$ chlorides            <dbl> 0.045, 0.049, 0.050, 0.058, 0.058, 0.05…\n$ free.sulfur.dioxide  <dbl> 45, 14, 30, 47, 47, 30, 30, 45, 14, 28,…\n$ total.sulfur.dioxide <dbl> 170, 132, 97, 186, 186, 97, 136, 170, 1…\n$ density              <dbl> 1.0010, 0.9940, 0.9951, 0.9956, 0.9956,…\n$ pH                   <dbl> 3.00, 3.30, 3.26, 3.19, 3.19, 3.26, 3.1…\n$ sulphates            <dbl> 0.45, 0.49, 0.44, 0.40, 0.40, 0.44, 0.4…\n$ alcohol              <dbl> 8.8, 9.5, 10.1, 9.9, 9.9, 10.1, 9.6, 8.…\n$ quality              <int> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, …\n\nsummary(data_white) # scale and range of values are quite different\n\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 3.800   Min.   :0.0800   Min.   :0.0000   Min.   : 0.600  \n 1st Qu.: 6.300   1st Qu.:0.2100   1st Qu.:0.2700   1st Qu.: 1.700  \n Median : 6.800   Median :0.2600   Median :0.3200   Median : 5.200  \n Mean   : 6.855   Mean   :0.2782   Mean   :0.3342   Mean   : 6.391  \n 3rd Qu.: 7.300   3rd Qu.:0.3200   3rd Qu.:0.3900   3rd Qu.: 9.900  \n Max.   :14.200   Max.   :1.1000   Max.   :1.6600   Max.   :65.800  \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide\n Min.   :0.00900   Min.   :  2.00      Min.   :  9.0       \n 1st Qu.:0.03600   1st Qu.: 23.00      1st Qu.:108.0       \n Median :0.04300   Median : 34.00      Median :134.0       \n Mean   :0.04577   Mean   : 35.31      Mean   :138.4       \n 3rd Qu.:0.05000   3rd Qu.: 46.00      3rd Qu.:167.0       \n Max.   :0.34600   Max.   :289.00      Max.   :440.0       \n    density             pH          sulphates         alcohol     \n Min.   :0.9871   Min.   :2.720   Min.   :0.2200   Min.   : 8.00  \n 1st Qu.:0.9917   1st Qu.:3.090   1st Qu.:0.4100   1st Qu.: 9.50  \n Median :0.9937   Median :3.180   Median :0.4700   Median :10.40  \n Mean   :0.9940   Mean   :3.188   Mean   :0.4898   Mean   :10.51  \n 3rd Qu.:0.9961   3rd Qu.:3.280   3rd Qu.:0.5500   3rd Qu.:11.40  \n Max.   :1.0390   Max.   :3.820   Max.   :1.0800   Max.   :14.20  \n    quality     \n Min.   :3.000  \n 1st Qu.:5.000  \n Median :6.000  \n Mean   :5.878  \n 3rd Qu.:6.000  \n Max.   :9.000  \n\nskim(data_white) # no missing values, probably need to normalize data\n\n\nTable 1: Data summary\nName\ndata_white\nNumber of rows\n4898\nNumber of columns\n12\n_______________________\n\nColumn type frequency:\n\nnumeric\n12\n________________________\n\nGroup variables\nNone\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nfixed.acidity\n0\n1\n6.85\n0.84\n3.80\n6.30\n6.80\n7.30\n14.20\n▁▇▁▁▁\nvolatile.acidity\n0\n1\n0.28\n0.10\n0.08\n0.21\n0.26\n0.32\n1.10\n▇▅▁▁▁\ncitric.acid\n0\n1\n0.33\n0.12\n0.00\n0.27\n0.32\n0.39\n1.66\n▇▆▁▁▁\nresidual.sugar\n0\n1\n6.39\n5.07\n0.60\n1.70\n5.20\n9.90\n65.80\n▇▁▁▁▁\nchlorides\n0\n1\n0.05\n0.02\n0.01\n0.04\n0.04\n0.05\n0.35\n▇▁▁▁▁\nfree.sulfur.dioxide\n0\n1\n35.31\n17.01\n2.00\n23.00\n34.00\n46.00\n289.00\n▇▁▁▁▁\ntotal.sulfur.dioxide\n0\n1\n138.36\n42.50\n9.00\n108.00\n134.00\n167.00\n440.00\n▂▇▂▁▁\ndensity\n0\n1\n0.99\n0.00\n0.99\n0.99\n0.99\n1.00\n1.04\n▇▂▁▁▁\npH\n0\n1\n3.19\n0.15\n2.72\n3.09\n3.18\n3.28\n3.82\n▁▇▇▂▁\nsulphates\n0\n1\n0.49\n0.11\n0.22\n0.41\n0.47\n0.55\n1.08\n▃▇▂▁▁\nalcohol\n0\n1\n10.51\n1.23\n8.00\n9.50\n10.40\n11.40\n14.20\n▃▇▆▃▁\nquality\n0\n1\n5.88\n0.89\n3.00\n5.00\n6.00\n6.00\n9.00\n▁▅▇▃▁\n\ndata_white %>% \n  ggpairs() # distribution of X is quite skewed, except maybe for pH.\n\n\n\n            # outcome is not unimodal --> ? \n\ndata_white %>% \n  ggcorr(label = T, label_alpha = T, label_round = 2) # some collinearity issues?\n\n\n\n\nSeems like OLS linear model isn’t really the best option for this case, since Y is multi-modal. It may be more suitable for classification to predict Y instead.\nNevertheless, let me compare the performance of OLS linear model with random forest, just for me to familiarise myself with the workflow for regression.\nTidymodels\nSplitting the dataset into training and testing datasets\n\n\nset.seed(202102212)\nwhitewine_split <- initial_split(data_white, prop = 0.8)\n\nwhitewine_train <- training(whitewine_split)\nwhitewine_test <- testing(whitewine_split)\n\n# split training dataset for cross-validation\nset.seed(20210228)\nwhite_cv <- vfold_cv(whitewine_train) # split training dataset for tuning mtry later\n\n\n\nSplitting the dataset into a training and testing dataset helps to minimise over-fitting of the model. Over-fitting the model would mean that the model fits the existing data very well, but is unable to predict for new data accurately.\nPreprocessing\nThe aim of preprocessing would be to solve the multi-collinearity issue, transform the data so that the distribution is not skewed, as well as to normalize the data.\n\n\nwhitewine_reciped <- whitewine_train %>% \n  recipe(quality ~., .) %>% \n  step_log(all_predictors(), -pH, offset = 1) %>% # do not use all numeric since will affect Y (outcome)\n  step_corr(all_predictors(), threshold = 0.5) %>%  # remove variables with r-sq > 0.5\n  step_normalize(all_predictors()) # means centering and scaling\n\nwhitewine_reciped \n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         11\n\nOperations:\n\nLog transformation on all_predictors(), -pH\nCorrelation filter on all_predictors()\nCentering and scaling for all_predictors()\n\nTrain the data recipe\n\n\nwhitewine_preprocessed <- prep(whitewine_reciped, verbose = T)\n\n\noper 1 step log [training] \noper 2 step corr [training] \noper 3 step normalize [training] \nThe retained training set is ~ 0.29 Mb  in memory.\n\nww_transformed <- whitewine_preprocessed %>% bake(new_data = NULL) # see preprocessed data\nww_transformed %>%  as_tibble() %>% round(., digits = 3) %>%  head()\n\n\n┌────────────────────────────────────────────────────────────────── │ fixed.ac volatile citric.a residual chloride free.sul\n│ idity .acidity cid .sugar s fur.diox\n│ ide\n├────────────────────────────────────────────────────────────────── │ 0.226 -0.042 0.264 1.83  -0.023 0.682\n│ -0.643 0.27  0.093 -1.1   0.171 -1.47 \n│ 0.46  -0.471 -0.08  0.691 0.604 0.763\n│ 0.46  -0.471 -0.08  0.691 0.604 0.763\n│ 1.45  0.063 0.598 0.436 0.219 -0.075\n│ 0.226 -0.042 0.264 1.83  -0.023 0.682\n└──────────────────────────────────────────────────────────────────\nColumn names: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, pH, sulphates, alcohol, quality\n6/10 columns shown.\n\n# check for missing values \nww_transformed %>% \n  map(is.na) %>%  \n  map_df(sum) %>% \n  tidy() %>% \n  select(column, mean) %>%  # no missing values\n  as_tibble()\n\n\n               ┌──────────────────────────────┐\n               │ column                  mean │\n               ├──────────────────────────────┤\n               │ fixed.acidity              0 │\n               │ volatile.acidity           0 │\n               │ citric.acid                0 │\n               │ residual.sugar             0 │\n               │ chlorides                  0 │\n               │ free.sulfur.dioxide        0 │\n               │ pH                         0 │\n               │ sulphates                  0 │\n               │ alcohol                    0 │\n               │ quality                    0 │\n               └──────────────────────────────┘\nColumn names: column, mean\n\nSpecify the models\nLinear regression (OLS)\n\n\nww_lr_model <- linear_reg() %>% \n  set_engine(\"lm\") %>% # there are other options available, eg glmnet\n  set_mode(\"regression\") # could also be classification for certain models, so just specify as best practice to be clear\n\nww_lr_workflow <- workflow() %>% \n  add_recipe(whitewine_reciped) %>% \n  add_model(ww_lr_model)\n\n#  fit model to training data, and get predicted values\nfinal_ww_lm_model_fit <- fit(ww_lr_workflow, whitewine_train)\n\n# understanding the lm model\n\nlm_fit_output <- final_ww_lm_model_fit %>% \n  pull_workflow_fit() %>% \n  tidy() %>% \n  as_tibble()\n\nlm_fit_output\n\n\n┌────────────────────────────────────────────────────────────┐\n│ term          estimate   std.error   statistic     p.value │\n├────────────────────────────────────────────────────────────┤\n│ (Intercept    5.88          0.012     488        0         │\n│ )                                                          │\n│ fixed.acid   -0.0459        0.0139     -3.3      0.000983  │\n│ ity                                                        │\n│ volatile.a   -0.199         0.0126    -15.8      2.61e-54  │\n│ cidity                                                     │\n│ citric.aci   -0.000888      0.0129     -0.0686   0.945     │\n│ d                                                          │\n│ residual.s    0.121         0.0142      8.53     1.98e-17  │\n│ ugar                                                       │\n│ chlorides    -0.0194        0.0133     -1.45     0.147     │\n│ free.sulfu    0.125         0.0131      9.61     1.31e-21  │\n│ r.dioxide                                                  │\n│ pH            0.0165        0.0139      1.19     0.234     │\n│ sulphates     0.0465        0.0123      3.78     0.000161  │\n│ alcohol       0.458         0.0147     31.1      4.63e-190 │\n└────────────────────────────────────────────────────────────┘\nColumn names: term, estimate, std.error, statistic, p.value\n\nlm_fit <- final_ww_lm_model_fit %>% \n  pull_workflow_fit()\n\nlm_fit\n\n\nparsnip model object\n\nFit time:  8ms \n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n        (Intercept)        fixed.acidity     volatile.acidity  \n          5.8775198           -0.0459302           -0.1989870  \n        citric.acid       residual.sugar            chlorides  \n         -0.0008882            0.1212696           -0.0193643  \nfree.sulfur.dioxide                   pH            sulphates  \n          0.1254698            0.0165026            0.0465193  \n            alcohol  \n          0.4576764  \n\n# Looking at the fitted values:\nlm_fitted_values <- lm_fit$fit$fitted.values\n\n# another way, from workflow\nlm_wf_fitted_values <- \n  broom::augment(lm_fit$fit, data = whitewine_train) %>% \n  select(quality, .fitted: .std.resid)\n\nglimpse(lm_wf_fitted_values)\n\n\nRows: 3,919\nColumns: 6\n$ quality    <int> 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 6, 8, 5, 8…\n$ .fitted    <dbl> 5.469029, 5.165576, 5.863142, 5.863142, 5.686045,…\n$ .hat       <dbl> 0.0016112016, 0.0019222325, 0.0009142831, 0.00091…\n$ .sigma     <dbl> 0.7532844, 0.7532139, 0.7533292, 0.7533292, 0.753…\n$ .cooksd    <dbl> 8.032116e-05, 2.368036e-04, 3.023818e-06, 3.02381…\n$ .std.resid <dbl> 0.705488442, 1.108851543, 0.181776968, 0.18177696…\n\n# looking at variable importance\nvip_lm <- final_ww_lm_model_fit %>% \n  pull_workflow_fit() %>% # extracts the model information\n  vip(num_features = 10, \n      aesthetics = list(fill = \"deepskyblue4\")) + # most important factor is alcohol +\n  labs(title = \"Variable Importance: Linear Regression\") +\n  theme_few() +\n  theme(axis.text = element_text(face = \"bold\", size = 14))\n\nvip_lm\n\n\n\n\nRandom forest model\n\n\nrf_model <- rand_forest() %>% \n  set_args(mtry = tune()) %>% \n  set_mode(mode = \"regression\") %>% \n  set_engine(engine = \"ranger\", importance = \"impurity\")\n\nrf_model\n\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\nrf_workflow <- workflow() %>% \n  add_recipe(whitewine_reciped) %>% \n  add_model(rf_model)\n\nrf_grid <- expand.grid(mtry = 3:7) # choose sqrt(no. of variables) usually\n\ntuned_rf_results <- rf_workflow %>% \n  tune_grid(resamples = white_cv, # using cv dataset from training dataset\n            grid = rf_grid,\n            metrics = metric_set(rmse, rsq, mae))\n\nmodel_results <- tuned_rf_results %>% \n  collect_metrics()\n\nfinalized_rf_param <- tuned_rf_results %>% \n  select_best(metric = \"rmse\") %>% \n  as_tibble()\n\nfinalized_rf_param #M TRY = 3\n\n\n              ┌───────────────────────────────┐\n              │   mtry   .config              │\n              ├───────────────────────────────┤\n              │      3   Preprocessor1_Model1 │\n              └───────────────────────────────┘\nColumn names: mtry, .config\n\nrf_model_b <- rand_forest() %>% \n  set_args(mtry = 3) %>% \n  set_engine(engine = \"ranger\", importance = \"impurity\") %>% \n  set_mode(mode = \"regression\")\n\nrf_workflow_b <- workflow() %>% \n  add_recipe(whitewine_reciped) %>% \n  add_model(rf_model_b)\n\nfinal_ww_rf_model_fit <- fit(rf_workflow_b, whitewine_train)\n\n# understanding the rf model\n\n# for random forest, need to set importance = impurity in set_engine() to extract this\nvip_rf <- final_ww_rf_model_fit %>% \n  pull_workflow_fit() %>% # extracts the model information\n  vip(num_features = 10, \n      aesthetics = list(fill = \"darkorange\"))+ # most important factor is alcohol\n  labs(title = \"Variable Importance: Random Forest\") +\n  theme_few() +\n  theme(axis.text = element_text(face = \"bold\", size = 14))\n\nvip_rf\n\n\n\n\nComparing Linear Regression (OLS) vs Random Forest (RF)\n\n\ngridExtra::grid.arrange(vip_lm, vip_rf, nrow = 2)\n\n\n\n\nAlcohol content was the most important variable for both OLS and random forest models.\nAssessing on test data\n\n\nresults_train <- final_ww_lm_model_fit %>% \n  predict(new_data = whitewine_train) %>%  # use actual train data, not preprocessed data\n  mutate(truth = whitewine_train$quality,\n         model = \"lm\") %>% \n  bind_rows(final_ww_rf_model_fit %>% \n              predict(new_data = whitewine_train) %>% \n              mutate(truth = whitewine_train$quality,\n                     model = \"rf\")) \n  \nresults_train %>% \n  group_by(model) %>% \n  metrics(truth = truth, estimate = .pred) %>% \n  as_tibble()\n\n\n         ┌──────────────────────────────────────────┐\n         │ model   .metric   .estimator   .estimate │\n         ├──────────────────────────────────────────┤\n         │ lm      rmse      standard         0.752 │\n         │ rf      rmse      standard         0.277 │\n         │ lm      rsq       standard         0.282 │\n         │ rf      rsq       standard         0.934 │\n         │ lm      mae       standard         0.584 │\n         │ rf      mae       standard         0.198 │\n         └──────────────────────────────────────────┘\nColumn names: model, .metric, .estimator, .estimate\n\nresults_test <- final_ww_lm_model_fit %>% \n  predict(new_data = whitewine_test) %>% \n  mutate(truth = whitewine_test$quality,\n         model = \"lm\") %>% \n  bind_rows(final_ww_rf_model_fit %>% \n              predict(new_data = whitewine_test) %>% \n              mutate(truth = whitewine_test$quality,\n                     model = \"rf\")) \n\nresults_test %>% \n  group_by(model) %>% \n  metrics(truth = truth, estimate = .pred) %>% \n  as_tibble()\n\n\n         ┌──────────────────────────────────────────┐\n         │ model   .metric   .estimator   .estimate │\n         ├──────────────────────────────────────────┤\n         │ lm      rmse      standard         0.737 │\n         │ rf      rmse      standard         0.589 │\n         │ lm      rsq       standard         0.295 │\n         │ rf      rsq       standard         0.56  │\n         │ lm      mae       standard         0.581 │\n         │ rf      mae       standard         0.435 │\n         └──────────────────────────────────────────┘\nColumn names: model, .metric, .estimator, .estimate\n\nWhen comparing rmse, rf has lower rmse in training dataset but the rmse value increased in the test dataset –> overfitting and cannot predict as well.This was the same for other indicators rsq and mean absolute error.\nBear in mind that in the first place, the outcome variable Y was multi-modal. This may be the reason why OLS wasn’t a suitable learner.\nVisualizing the assessment\n\n\nresults_train %>% \n  ggplot(aes(x =  truth, y = .pred)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  geom_abline(col = \"darkorange\") +\n  labs(x = \"actual values (truth)\",\n       y = \"predicted values\",\n       title = \"Training dataset\") +\n  scale_x_continuous(breaks = c(1:10)) +\n  facet_wrap( model ~ ., ncol = 2) +\n  theme_few()\n\n\n\nresults_test %>% \n  ggplot(aes(x =  truth, y = .pred)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  geom_abline(col = \"darkorange\") +\n  labs(x = \"actual values (truth)\",\n       y = \"predicted values\",\n       title = \"Testing dataset\") +\n  scale_x_continuous(breaks = c(1:10)) +\n  facet_wrap( model ~ ., ncol = 2) +\n  theme_few()\n\n\n\n\nLearning points\nIt took me a while to piece different pieces of the jigsaw together to see the whole picture for machine learning. Initially, I will be carrying out EDA blindly, simply using skim() because it is a convenient function, but not fully understanding what I should be looking out for. I would be doing pre-processing steps at random, depending on what I saw from other websites. Finally, I saw the light that the purpose of doing EDA was to understand what I should be doing for preprocessing!\nIt is always good to start with the simple OLS when I am learning regression. There are assumptions that must be met before doing OLS – these could be checked using the gvlma package, and you can carry out the necessary transformations before doing OLS. There are other types of linear regression, for example generalized linear model (GLM), which I should try as well.\nThe order of carrying out preprocessing steps matter!\nThe choice of all_numerical, all_predictors in the recipe step matters! In this case, all_numerical includes the Y variable. Although Y is multimodal, it is not skewed, so I should not log transform it (which is what would happen if I were to use step_log(all_numerical())). If I log-transformed Y, I would run into errors further along the script, as there are some bugs regarding predict function if Y is transformed. The OLS model performed relatively consistently in both training and test dataset. However, the RF model performed better in the training dataset, but performance was poorer in the test dataset. This suggested that the RF model, in this case, had over-fitting issues.\nNext steps:\nTry out regression on a dataset in which Y is suitable for regression analysis\nTry out classification on wine dataset.\nTry out step_dummy, which creates numerical variables out of categorical variables\nTry out different algorithms and their tuning parameters\n“There is only one corner of the universe you can be certain of improving, and that’s your own self.” - ― Aldous Huxley\nThis is just the beginning of my learning journey!\nReferences\nhttps://www.tmwr.org/\nhttps://online.stat.psu.edu/stat508/lesson/1a\nhttps://semba-blog.netlify.app/05/11/2020/regression-with-tidymodels/\nhttps://stackoverflow.com/questions/63239600/how-to-make-predictions-in-tidymodels-r-when-the-data-has-been-preprocessed\nhttps://stackoverflow.com/questions/13956435/setting-values-for-ntree-and-mtry-for-random-forest-regression-model\nhttps://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/\nhttps://jhudatascience.org/tidyversecourse/model.html\nhttps://koalaverse.github.io/vip/articles/vip.html\nhttp://rstudio-pubs-static.s3.amazonaws.com/565136_b4395e2500ec4c129ab776b9e8dd24de.html#results\n\n\n\n",
    "preview": "posts/20210301_tidy models regression 1/Tidy-Models---Regression-v2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-03T22:04:39+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210219_color calculations (juice)/",
    "title": "Color Analysis in Juices",
    "description": "Using R for color calculations and data visualization",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-02-19",
    "categories": [],
    "contents": "\nIntroduction\nWhy are food scientists interested in color analysis? Color is a visual quality attribute that determines food acceptance (Wrolstad and Smith (2017)). Instrumental color analysis is carried out in most commercial research and development laboratories to assess color stability, and in turn, shelf life of food products. The Hunter L a b color space is commonly used in the food industry, and was first published in 1942. Improvements were made to this system, to give more uniform color spacing, and in 1976, the CIELAB L* a* b* system was introduced. Chroma and Hue could be calculated from the a* and b* values. What do all these terms mean?\nL* : lightness (0 being black and 100 being white)\na*: red (+) and green (-)\nb*: yellow (+) and blue (-)\nChroma: a measure of how vivid/dull the color is. Chroma increases with increasing pigment concentration, and then decreases as the sample becomes darker.\nHue (in radian): the type of color - where red is defined as 0/360deg, yellow is defined as 90 deg, green is defined as 180 deg and blue is defined as 270 deg.\nThe calculations for chroma and hue are give below:\nChroma = sqrt(aˆ2 + bˆ2)\nHue is expressed in radians (multiply by 180/pi). However, this equation is only for the first quadrant. Other quadrants need to be handled so that a 360deg representation is accomodated (Mclellan, Lind, and Kime (1995)).\nFirst quadrant [+a, +b] : Hue = arc tan (b/a)\nSecond quadrant [‐a, +b] and third quadrant [−a, −b] calculations should be: hue = 180+Arc tan(b/a).\nFourth quadrant [+a, −b] calculations should be: hue = 360+ Arc tan(b/a).\nThe L* C* H* color space is more useful than just looking at L* a* b*, since it takes into account human perception of color, rather than just looking at redness/greenness and yellowness/blueness individually.\nUltimately, when color is measured, other than having an objective set of numbers to describe colors, it is also of interest to assess if there is any color difference between a reference sample and a test sample, and to peg a number to this color difference and immediately tell if the color difference is visually obvious to people.\nThere are different equations for assessing color difference:\ndeltaE-1976: the first internationally endorced color difference equation. However, it does not take into account that the human eye is more sensitive to small color differences in some regions of the color wheel but less sensitive to others.\ndeltaE-1994: Improvements were made to the 1976 equation, but it lacked accuracy in the blue-violet region of the color space.\ndeltaE-2000: Corrected for the 1994 equation to improve accuracy in the blue-violet space. This is the most updated and accurate representation of total color difference so far, and the equation is given in: http://www2.ece.rochester.edu/~gsharma/ciede2000/ciede2000noteCRNA.pdf.\nPreviously, I worked with Excel spreadsheets to carry out color data calculations. It was a nightmare when I tried to calculate hue using Excel, as the equations used for +a/-a/+b/-b could be different and it was problematic when I was trying to fill an equation down for my shelf life study. DE2000 was complicated and I tried to use a spreadsheet that I downloaded off the Internet, but I had to copy my data over to the template spreadsheet and it was a lot of copying and pasting.\nAll I want, is a workflow that can house all my data in 1 place, and automatically apply calculations with minimal manual input.\nI am so glad that I found R, and that there is an inbuilt package spacesXYZ, that can calculate the different variants of total color difference.\nObjective\nTo develop a workflow for automatic color calculations. This include writing my own function for calculating chroma, hue, and using inbuilt functions within spacesXYZ package to calculate de2000.\nTo visualize data and derive insights from shelf life data clearly.\nData\nThe data I am using is from a paper by Porto et al: https://www.mdpi.com/2306-5710/3/3/36. In this paper, the color data for five types of juices were given, and color changes were assessed in terms of L* a* b*, chroma and de1976. I went on further to look at hue, and de2000.\nWorkflow\nLoad data\nTransform data (in wide format) - calculate chroma and hue\nAdd in new columns with initial L* a* b* to facilitate calculation for de2000\nExtract initial L* a* b* values as matrix form\nExtract measured L* a* b* values as matrix form\nCalculate de2000 using spacesXYZ, as the function requires input to be in matrix form\nCalculate change in L* a* b* chroma and hue\nTransform data into long format for data visualization\nPlot de2000, L* a* b* chroma and hue, as well as change in L* a* b* chroma and hue to derive insights.\nLoad packages\n\n\nlibrary(pacman)\np_load(tidyverse, spacesXYZ, ggthemes, gridExtra, ggsci)\n\n\n\nImport Data\nThe five samples tested were:\nBJ: raw beet juice\nPBJ: pasteurized beet juice\nPOJ: pasteurized orange juice\nBOMJ_1: pasteurized beet and orange mix juice (1:1 v/v)\nBOMJ-2: pasteurized beet and orange mix juice (1:2, v/v)\n\n\ndata_l <- tribble(\n  ~Juices, ~L_d0, ~L_d5, ~L_d10, ~L_d15, ~L_d30,\n  #-------/------/-----/--------/------/-------\n  \"BJ\",     22.45, 22.87, 22.31, 22.37, 24.16 , \n  \"PBJ\",    22.37, 22.71, 22.34, 22.23, 23.72,\n  \"POJ\",    33.61, 38.18, 36.73, 37.04, 42.42,\n  \"BOMJ_1\", 23.21, 23.76, 23.15, 23.14, 24.78,\n  \"BOMJ_2\", 23.77, 24.33, 23.81, 24.15, 26.18\n)\n\n\ndata_a <- tribble(\n  ~Juices, ~a_d0, ~a_d5, ~a_d10, ~a_d15, ~a_d30,\n  #-------/------/-----/--------/------/-------\n  \"BJ\",     0.78, 0.68, 0.74, 0.81, 1.07,\n  \"PBJ\",    0.95, 0.82, 0.87, 0.91, 1.19,\n  \"POJ\",    -2.49, -2.87, -2.51, -2.63, -3.64,\n  \"BOMJ_1\", 4.80, 5.18, 4.78, 4.59, 6.13,\n  \"BOMJ_2\", 6.65, 7.07, 6.65, 6.68, 8.76\n)\n\n\ndata_b <- tribble(\n  ~Juices, ~b_d0, ~b_d5, ~b_d10, ~b_d15, ~b_d30,\n  #-------/------/-----/--------/------/-------\n  \"BJ\",    1.56, 1.52, 1.56, 1.57, 0.97, \n  \"PBJ\",   1.67, 1.61, 1.63, 1.64, 1.21,  \n  \"POJ\",  16.34, 17.03, 16.46, 15.95, 18.34,\n  \"BOMJ_1\", 2.39, 2.38, 2.35, 2.19, 2.23,\n  \"BOMJ_2\", 2.75, 2.82, 2.68, 2.26, 2.47\n)\n\n# Transform #####\ndata <- bind_cols(data_l, data_a, data_b, .name_repair = \"unique\") %>% \n  select(-Juices...7, -Juices...13) %>% \n  rename(juices = Juices...1)\n\n\n\nTransform\n\n\ndata_reshape_L <- data %>% \n  pivot_longer(cols = starts_with(\"L\"),\n               names_to = \"days_L\",\n               values_to = \"L_av\") %>% \n  select(juices, days_L, L_av)\n\n\ndata_reshape_a <- data %>% \n  pivot_longer(cols = starts_with(\"a\"),\n               names_to = \"days_a\",\n               values_to = \"a_av\") %>% \n  select(juices, days_a, a_av)\n  \ndata_reshape_b <- data %>%   \n  pivot_longer(cols = starts_with(\"b\"),\n               names_to = \"days_b\",\n               values_to = \"b_av\") %>% \n  select(juices, days_b, b_av)\n\n\ndata_reshaped <- bind_cols(data_reshape_L, data_reshape_a, data_reshape_b) %>% \n  mutate(days = parse_number(days_L)) %>% \n  select(juices...1, days, L_av, a_av, b_av) %>% \n  rename(juices = juices...1)\n\ndata_reshaped\n\n\n# A tibble: 25 x 5\n   juices  days  L_av  a_av  b_av\n   <chr>  <dbl> <dbl> <dbl> <dbl>\n 1 BJ         0  22.4  0.78  1.56\n 2 BJ         5  22.9  0.68  1.52\n 3 BJ        10  22.3  0.74  1.56\n 4 BJ        15  22.4  0.81  1.57\n 5 BJ        30  24.2  1.07  0.97\n 6 PBJ        0  22.4  0.95  1.67\n 7 PBJ        5  22.7  0.82  1.61\n 8 PBJ       10  22.3  0.87  1.63\n 9 PBJ       15  22.2  0.91  1.64\n10 PBJ       30  23.7  1.19  1.21\n# … with 15 more rows\n\nColor calculations\nWriting functions to calculate chroma and hue\n\n\ncal_chroma <- function (a_av, b_av) {\n  \n  a_sq = a_av^2\n  b_sq = b_av^2\n  chroma = sqrt(a_sq + b_sq)\n  \n}\n\ncal_hue <- function (a_av, b_av) {\n  \n  if(a_av > 0 & b_av > 0) {  # a pos, b pos\n    hue = 180*(atan(b_av/a_av)/pi)\n    \n    \n  }   else if (a_av<0 & b_av > 0) {  # a neg, b pos\n    hue = 180 + 180*(atan(b_av/a_av)/pi)\n    \n    \n  } else if (a_av<0 & b_av<0) {   # a neg, b neg\n    hue = 180 + 180*(atan(b_av/a_av)/pi)\n    \n    \n  } else {    # a pos, b neg\n    hue = 360 + 180*(atan(b_av/a_av)/pi)\n    \n  }\n  \n}\n\n\n\nAdding calculated chroma and hue columns to tibble\n\n\ndata_transformed <- data_reshaped %>% \n  mutate(chroma = map2_dbl(.x = a_av,\n                           .y = b_av,\n                           .f = cal_chroma),\n         hue = map2_dbl(.x = a_av,\n                        .y = b_av,\n                        .f = cal_hue))\n\nglimpse(data_transformed) # compares well with table\n\n\nRows: 25\nColumns: 7\n$ juices <chr> \"BJ\", \"BJ\", \"BJ\", \"BJ\", \"BJ\", \"PBJ\", \"PBJ\", \"PBJ\", \"…\n$ days   <dbl> 0, 5, 10, 15, 30, 0, 5, 10, 15, 30, 0, 5, 10, 15, 30…\n$ L_av   <dbl> 22.45, 22.87, 22.31, 22.37, 24.16, 22.37, 22.71, 22.…\n$ a_av   <dbl> 0.78, 0.68, 0.74, 0.81, 1.07, 0.95, 0.82, 0.87, 0.91…\n$ b_av   <dbl> 1.56, 1.52, 1.56, 1.57, 0.97, 1.67, 1.61, 1.63, 1.64…\n$ chroma <dbl> 1.744133, 1.665173, 1.726615, 1.766635, 1.444230, 1.…\n$ hue    <dbl> 63.43495, 65.89777, 64.62226, 62.70972, 42.19363, 60…\n\nCreating initial values tibble dataframe to calculate dE2000 later\n\n\ninitial <-  data_transformed %>% \n                      filter(days == 0) %>% \n                      select(L_av, a_av, b_av, chroma, hue) %>% \n                      rename(ini_L = L_av,\n                             ini_a = a_av,\n                             ini_b = b_av,\n                             ini_chroma = chroma,\n                             ini_hue = hue)\n\ninitial\n\n\n# A tibble: 5 x 5\n  ini_L ini_a ini_b ini_chroma ini_hue\n  <dbl> <dbl> <dbl>      <dbl>   <dbl>\n1  22.4  0.78  1.56       1.74    63.4\n2  22.4  0.95  1.67       1.92    60.4\n3  33.6 -2.49 16.3       16.5     98.7\n4  23.2  4.8   2.39       5.36    26.5\n5  23.8  6.65  2.75       7.20    22.5\n\nAdding the initial L* a* b* values to tibble\n\n\ndata_transformed_b<- data_transformed %>% \n  group_by(juices) %>% \n  nest() %>% \n  bind_cols(initial) %>% \n  unnest(cols = c(data))\n\ndata_transformed_b\n\n\n# A tibble: 25 x 12\n# Groups:   juices [5]\n   juices  days  L_av  a_av  b_av chroma   hue ini_L ini_a ini_b\n   <chr>  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 BJ         0  22.4  0.78  1.56   1.74  63.4  22.4  0.78  1.56\n 2 BJ         5  22.9  0.68  1.52   1.67  65.9  22.4  0.78  1.56\n 3 BJ        10  22.3  0.74  1.56   1.73  64.6  22.4  0.78  1.56\n 4 BJ        15  22.4  0.81  1.57   1.77  62.7  22.4  0.78  1.56\n 5 BJ        30  24.2  1.07  0.97   1.44  42.2  22.4  0.78  1.56\n 6 PBJ        0  22.4  0.95  1.67   1.92  60.4  22.4  0.95  1.67\n 7 PBJ        5  22.7  0.82  1.61   1.81  63.0  22.4  0.95  1.67\n 8 PBJ       10  22.3  0.87  1.63   1.85  61.9  22.4  0.95  1.67\n 9 PBJ       15  22.2  0.91  1.64   1.88  61.0  22.4  0.95  1.67\n10 PBJ       30  23.7  1.19  1.21   1.70  45.5  22.4  0.95  1.67\n# … with 15 more rows, and 2 more variables: ini_chroma <dbl>,\n#   ini_hue <dbl>\n\nCalculating de2000\n\n\n# calculate de2000 using spacesXYZ package, input must be as matrix\n\nlab_meas <- as.matrix(data_transformed_b[, c(\"L_av\", \"a_av\", \"b_av\")])\nlab_ini <- as.matrix(data_transformed_b[, c(\"ini_L\", \"ini_a\", \"ini_b\")])\n\ndata_de <- spacesXYZ::DeltaE(lab_ini, lab_meas, metric = 2000)\n  \n\ndata_transformed_c <- data_transformed_b %>% \n  bind_cols(data_de) %>% \n  rename(de2000 = ...13) %>% \n  ungroup() # remove group by juices\n\n# round off to 2 digits\ndata_transformed_c$de2000 <- round(data_transformed_c$de2000, digits = 2)\n\nglimpse(data_transformed_c)\n\n\nRows: 25\nColumns: 13\n$ juices     <chr> \"BJ\", \"BJ\", \"BJ\", \"BJ\", \"BJ\", \"PBJ\", \"PBJ\", \"PBJ…\n$ days       <dbl> 0, 5, 10, 15, 30, 0, 5, 10, 15, 30, 0, 5, 10, 15…\n$ L_av       <dbl> 22.45, 22.87, 22.31, 22.37, 24.16, 22.37, 22.71,…\n$ a_av       <dbl> 0.78, 0.68, 0.74, 0.81, 1.07, 0.95, 0.82, 0.87, …\n$ b_av       <dbl> 1.56, 1.52, 1.56, 1.57, 0.97, 1.67, 1.61, 1.63, …\n$ chroma     <dbl> 1.744133, 1.665173, 1.726615, 1.766635, 1.444230…\n$ hue        <dbl> 63.43495, 65.89777, 64.62226, 62.70972, 42.19363…\n$ ini_L      <dbl> 22.45, 22.45, 22.45, 22.45, 22.45, 22.37, 22.37,…\n$ ini_a      <dbl> 0.78, 0.78, 0.78, 0.78, 0.78, 0.95, 0.95, 0.95, …\n$ ini_b      <dbl> 1.56, 1.56, 1.56, 1.56, 1.56, 1.67, 1.67, 1.67, …\n$ ini_chroma <dbl> 1.744133, 1.744133, 1.744133, 1.744133, 1.744133…\n$ ini_hue    <dbl> 63.43495, 63.43495, 63.43495, 63.43495, 63.43495…\n$ de2000     <dbl> 0.00, 0.33, 0.11, 0.07, 1.42, 0.00, 0.31, 0.12, …\n\nThe perceptible difference is defined theoretically as de2000 being greater than 2 http://zschuessler.github.io/DeltaE/learn/. Which samples have de2000 >2?\n\n\n# threshold is de2000>2\n\nabove_threshold <- data_transformed_c %>% \n  filter(de2000>2) %>% \n  select(juices, days, de2000)\n\nabove_threshold  # POJ\n\n\n# A tibble: 5 x 3\n  juices  days de2000\n  <chr>  <dbl>  <dbl>\n1 POJ        5   3.84\n2 POJ       10   2.57\n3 POJ       15   2.85\n4 POJ       30   7.7 \n5 BOMJ_2    30   2.76\n\nColor difference was already perceptible for pasteurized orange juice from day 5. For beet and orange mixed juice (1:2 v/v), the color difference was perceptibely at day 30, at the end of shelf life.\nVisualization\n\n\n# Reshape data to make it suitable for facetting \n\ndata_viz_long <- data_transformed_c %>% \n  mutate(delta_L = L_av - ini_L,\n         delta_a = a_av - ini_a,\n         delta_b = b_av - ini_b,\n         delta_chroma = chroma - ini_chroma,\n         delta_hue = hue - ini_hue) %>% \n  select(juices, days, L_av:delta_hue) %>% \n  pivot_longer(cols = c(L_av:delta_hue),\n               names_to = \"parameters\",\n               values_to = \"readings\")\n  \n\ndata_viz_long\n\n\n# A tibble: 400 x 4\n   juices  days parameters readings\n   <chr>  <dbl> <chr>         <dbl>\n 1 BJ         0 L_av          22.4 \n 2 BJ         0 a_av           0.78\n 3 BJ         0 b_av           1.56\n 4 BJ         0 chroma         1.74\n 5 BJ         0 hue           63.4 \n 6 BJ         0 ini_L         22.4 \n 7 BJ         0 ini_a          0.78\n 8 BJ         0 ini_b          1.56\n 9 BJ         0 ini_chroma     1.74\n10 BJ         0 ini_hue       63.4 \n# … with 390 more rows\n\nde2000\n\n\ndata_viz_long %>% \n  filter(parameters == \"de2000\") %>% \n  ggplot(aes(days, readings)) +\n  geom_point(aes(col = juices), size = 2) +\n  geom_line(aes(col = juices), size = 1) +\n  scale_color_lancet() +\n  geom_hline(yintercept = 2, col = \"grey77\", lty = 2) +\n  labs(title = \"Comparison of Total Color Difference (dE2000) when stored at 4degC for 30 days\",\n       x = \"Days\",\n       y = \"Calc. dE2000\",\n       subtitle = \"Pure Orange Juice (POJ) had the greatest change in color. Addition of beet juice decreases change in color difference.\",\n       caption = \"Source: Porto et al, 2017\") +\n  facet_wrap(~juices, ncol = 3) +\n  theme_few() +\n  theme(title = element_text(face = \"bold\", size = 16),\n        legend.position = \"none\",\n        strip.text = element_text(face = \"bold\", size = 14))\n\n\n\n\nWhilst we know that pasteurized orange juice had perceptible color difference, what exactly was the difference due to? To answer this question, we will have to look at individual parameters (L* a* b* chroma and hue).\nUnderstanding each color parameter\n\n\nviz_absolute <- data_viz_long %>% \n  filter(parameters %in% c(\"L_av\", \"a_av\", \"b_av\", \"chroma\", \"hue\")) %>%\n  mutate(parameters_fct = factor(parameters,\n                                 levels = c(\"L_av\", \"a_av\", \"b_av\", \"chroma\", \"hue\"))) %>% \n  ggplot(aes(days, readings, group = juices)) +\n  geom_point(aes(col = juices), size = 2) +\n  geom_line(aes(col = juices), size = 1) +\n  scale_color_lancet() +\n  labs(title = \"Change in color over shelf life period\",\n       caption = \"Source: Porto et al, 2017\",\n       col = \"Juices\") +\n  facet_wrap( ~ parameters_fct, ncol = 5, scales = \"free\") +\n  theme_few()+\n  theme(title = element_text(face = \"bold\", size = 20),\n        strip.text = element_text(face = \"bold\", size = 16),\n        axis.text = element_text(size = 14),\n        legend.text = element_text(size = 14),\n        legend.position = \"top\")\n\nviz_change <- data_viz_long %>% \n  filter(parameters %in% c(\"delta_L\", \"delta_a\", \"delta_b\", \"delta_chroma\", \"delta_hue\")) %>% \n  mutate(parameters_fct = factor(parameters, \n                                 levels = c(\"delta_L\", \"delta_a\", \"delta_b\", \"delta_chroma\", \"delta_hue\"))) %>% \n  ggplot(aes(days, readings, group = juices)) +\n  geom_point(aes(col = juices), size = 2) +\n  geom_line(aes(col = juices), size = 1) +\n  scale_color_lancet() +\n  labs(title = \"Change in color over shelf life period\",\n       caption = \"Source: Porto et al, 2017\",\n       col = \"Juices\") +\n  facet_wrap( ~ parameters_fct, ncol = 5) +\n  theme_few()+\n  theme(title = element_text(face = \"bold\", size = 20),\n        strip.text = element_text(face = \"bold\", size = 16),\n        axis.text = element_text(size = 14),\n        legend.text = element_text(size = 14),\n        legend.position = \"top\")\n\ngrid.arrange(viz_absolute, viz_change, nrow = 2)\n\n\n\n\nInterpretation\nBeet juice is red in color and orange juice is orange-yellow in color. If we look at the L* a* b* values, from the onset, POJ had higher values for L* (ie more dark), lower a* (ie less red) and higher b* (ie more yellow). This is more easily understood by looking at the hue values, which describes the type of color (0 = red, 90 = yellow). In terms of color vividness, POJ was relatively more vivid than the other samples, and BJ and PBJ has the “dullest” color.\nHowever, for total color difference, we would be more interested in the change in each parameter. POJ had a relatively large increase in L* (ie more darkening of color). For hue, there was a slight increase for POJ, but it was less in magnitude as compared to BJ and PBJ.\n\n\ndata_viz_long %>% \n  filter(juices %in% c(\"BJ\", \"PBJ\", \"POJ\"),\n         days == 30,\n         parameters == \"delta_hue\") \n\n\n# A tibble: 3 x 4\n  juices  days parameters readings\n  <chr>  <dbl> <chr>         <dbl>\n1 BJ        30 delta_hue    -21.2 \n2 PBJ       30 delta_hue    -14.9 \n3 POJ       30 delta_hue      2.56\n\nBJ and PBJ had a decrease in hue of 21 units and 15 units. This meant that the color became less orange-red and more red. However, the change in de2000 was probably attributed to the change in L* for POJ. Even though there was a difference in hue, the total color difference was below threshold of 2 for BJ and PBJ. Color instability was mostly attributed to pasteurized orange juice, and beet juice was relatively more stable.\nBetalains were responsible for the red color in beet, and carotenoids are responsible for the orange color in oranges (Tanaka, Sasaki, and Ohmiya (2008)). Fun fact: betalains color are not pH-dependent like anthocyanins, and they do not co-exist in plants.\nReflections\nI am happy that I managed to write a function for hue calculation, and use existing functions to calculate de2000. The calculations were really cumbersome when done in excel.\nWhen looking at color difference, it is important to look at both absolute readings and change in parameter readings to get the whole picture. The former allows you to understand what the starting point was, and the latter zooms in to the change between the start and at the end of shelf life. Although the data could be expressed in numerical form in tables, properly drawn graphs give more intuitive understanding of the data. I really like the faceting function in R, as it allows me to see all the types of juices and different color parameters clearly. In addition, the grid.arrange function allows me to display more than one graph.\nIn this shelf life study, only one temperature condition was studied. What if more than one temperature/product were looked at? In such cases, repetitive color calculations may be made more efficient by using purrr.\nLinks\nhttps://www.mdpi.com/2306-5710/3/3/36 https://www.xrite.com/blog/lab-color-space https://sensing.konicaminolta.us/us/blog/identifying-color-differences-using-l-a-b-or-l-c-h-coordinates/ https://www.konicaminolta.com/instruments/knowledge/color/pdf/color_communication.pdf https://www.hdm-stuttgart.de/international_circle/circular/issues/13_01/ICJ_06_2013_02_069.pdf http://zschuessler.github.io/DeltaE/learn/\n\n\n\nMclellan, M. R., L. R. Lind, and R. W. Kime. 1995. “HUE ANGLE DETERMINATIONS AND STATISTICAL ANALYSIS FOR MULTIQUADRANT HUNTER l,a,b DATA.” Journal of Food Quality 18 (3): 235–40. https://doi.org/https://doi.org/10.1111/j.1745-4557.1995.tb00377.x.\n\n\nTanaka, Yoshikazu, Nobuhiro Sasaki, and Akemi Ohmiya. 2008. “Biosynthesis of Plant Pigments: Anthocyanins, Betalains and Carotenoids.” The Plant Journal 54 (4): 733–49. https://doi.org/https://doi.org/10.1111/j.1365-313X.2008.03447.x.\n\n\nWrolstad, Ronald E., and Daniel E. Smith. 2017. “Color Analysis.” In Food Analysis, edited by S. Suzanne Nielsen, 545–55. Food Science Text Series. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-45776-5_31.\n\n\n\n\n",
    "preview": "posts/20210219_color calculations (juice)/Color-analysis-for-Juices_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2021-02-21T15:51:34+08:00",
    "input_file": {},
    "preview_width": 3072,
    "preview_height": 2304
  },
  {
    "path": "posts/20210216_durian volatiles/",
    "title": "Comparison of volatiles in Durians",
    "description": "Data visualization for volatiles in different durian varieties",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-02-16",
    "categories": [],
    "contents": "\nIntroduction\nDurian is a tropical fruit that is either much loved or much hated in Singapore. There are different varieties of durians, and the top durians such as Mao Shan Wang can command prices of around $20-30 per kg. The price depends on the supply, the quality, and of course the demand.\nTeh et al. (2017) mentioned that the durian aroma comes mainly from the sulfur compounds, which gives it the characteristic pungent smell; as well as esters, which contributes to the fruity character.\nThe data below is from the work done by Chin et al. (2007). A total of 39 volatiles were identified in three varieties of durian: D2, D24 and D101. In the paper, PCA was carried out to distinguish between the three varieties.\nObjective\nData visualization for top 10 volatile compounds (by concentration) in three different durian varieties: D2, D24 and D101\nLoad packages\n\n\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\nImport\nThe file was saved on my working directory and I imported it into R\n\n\ndurian <- read_csv(\"Durian.csv\") %>% \n  clean_names() \n\n\n\nData visualization\n\n\nd101 <- durian %>% \n  select(-peak_no, -odor_description, - category) %>% \n  pivot_longer(cols = starts_with(\"d\"),\n               names_to = \"variety\",\n               values_to = \"concentration\") %>% \n  filter(variety == \"d101\") %>% \n  top_n(10, concentration) %>% \n  ggplot(aes(fct_reorder(compound, concentration), concentration)) +\n  geom_col(fill = \"goldenrod\") +\n  labs(x = NULL,\n       title = \"D101\",\n       x = \"Relative Concentration (ug/g)\",\n       caption = \"Chin et al, 2007\") +\n  coord_flip() +\n  theme_classic() +\n  theme(title = element_text(face = \"bold\", size = 16),\n        axis.text = element_text(size = 14))\n  \nd2 <- durian %>% \n  select(-peak_no, -odor_description, - category) %>% \n  pivot_longer(cols = starts_with(\"d\"),\n               names_to = \"variety\",\n               values_to = \"concentration\") %>% \n  filter(variety == \"d2\") %>% \n  top_n(10, concentration) %>% \n  ggplot(aes(fct_reorder(compound, concentration), concentration)) +\n  geom_col(fill = \"forestgreen\") +\n  labs(x = NULL,\n       title = \"D2\",\n       x = \"Relative Concentration (ug/g)\",\n       caption = \"Chin et al, 2007\") +\n  coord_flip() +\n  theme_classic() +\n  theme(title = element_text(face = \"bold\", size = 16),\n        axis.text = element_text(size = 14))\n\nd24 <- durian %>% \n  select(-peak_no, -odor_description, - category) %>% \n  pivot_longer(cols = starts_with(\"d\"),\n               names_to = \"variety\",\n               values_to = \"concentration\") %>% \n  filter(variety == \"d24\") %>% \n  top_n(10, concentration) %>% \n  ggplot(aes(fct_reorder(compound, concentration), concentration)) +\n  geom_col(fill = \"darkorange2\") +\n  labs(x = NULL,\n       title = \"D24\",\n       x = \"Relative Concentration (ug/g)\",\n       caption = \"Chin et al, 2007\") +\n  coord_flip() +\n  theme_classic() +\n  theme(title = element_text(face = \"bold\", size = 16),\n        axis.text = element_text(size = 14))\n\n\ngridExtra::grid.arrange(d101, d2, d24, ncol = 3,\n                        top = \"Comparison of top volatiles found in different durian varieties\")\n\n\n\ndurian %>% \n  select(-peak_no, -odor_description) %>% \n  pivot_longer(cols = starts_with(\"d\"),\n               names_to = \"variety\",\n               values_to = \"concentration\") %>% \n  group_by(category, variety) %>% \n  summarize(sum_conc = sum(concentration)) %>% \n  arrange(desc(sum_conc))\n\n\n# A tibble: 9 x 3\n# Groups:   category [3]\n  category         variety sum_conc\n  <chr>            <chr>      <dbl>\n1 Ester            d2         54.7 \n2 Ester            d101       54.6 \n3 Sulfur-compounds d24        47.4 \n4 Sulfur-compounds d2         46.5 \n5 Sulfur-compounds d101       36.5 \n6 Ester            d24        30.5 \n7 Alcohol          d2          1.09\n8 Alcohol          d101        0.72\n9 Alcohol          d24         0.56\n\nInterpretation\nFrom the plot above, half of the top ten volatile compounds in D24 were sulfur-containing compounds, and the most abundant volatile was diethyl disulfide (18.76 ug/g). The odor description for diethyl disulfide is “Sulfury, roasty, cabbage-like odor”.\nFor D101, the top two most abundant volatile compounds were esters: ethyl 2-methylbutanoate (21.89 ug/g) (poweful green, fruity, apple-like odor) and propyl 2-methylbutanoate (12.67 ug/g), followed by sulfur compounds diethyl disulfide (12.42ug/g) and diethyl trisulfide (5.97ug/g).\nFor D2, ethyl 2-methylbutanoate (29.68 ug/g) was relatively higher than in D101.\nIf we look at the total concentration of esters and sulfur compounds, D24 has the highest concentration of sulfur compounds (in line with the plot above). Comparing D2 and D101, the concentration of esters is about the same, but D2 has higher concentration of sulfur-containing compounds than D101. According to Takeoka et al. (1995), branched chain esters have lower odor thresholds than their straight chain counterparts. It appeared that D101, with slightly lower concentration of sulfur-compounds, would be perceived as more fruity. However, the authors found that D2 was perceived to have a stronger sweet and fruity odor; and that D101 was perceived to have a well-balanced aroma. I’m not quite sure why, I guess I would need to taste in person to find out!\nPCA\nI attempted to do PCA with the data provided, but it was a bit silly as n = 3, as I did not have the raw data with me. In addition, the assumptions for KMO and Bartlett’s tests were not met.\nDue to the very small number of observations, I ran into this error: Error in comps[, 1:object$num_comp, drop = FALSE] : subscript out of bounds\nAfter specifying that num_comp = 3, I did not receive this error message again.\nThe script below shows my attempt to reproduce the PCA variable loadings plot. I managed to get the same plot as the authors, so probably if I have raw data with me, that would be great. Note that I did not show the scree plot, eigenvalues and variance explained plot, as n=3 is really very small and PCA should not even be conducted. Nevertheless, it was an exercise in attempting to understand the conclusions drawn by the authors.\n\n\n# PACKAGES ####\nlibrary(pacman)\np_load(tidyverse, janitor, skimr, psych, tidymodels, learntidymodels)\n\n# IMPORT ####\n\ndurian <- read_csv(\"Durian.csv\") %>% \n  clean_names() %>% \n  mutate(peak_no_2 = paste( \"peak\", peak_no, sep = \"_\")) %>% \n  select(-peak_no) %>% \n  rename(peak_no = peak_no_2) %>% \n  select(peak_no, everything())\n\nglimpse(durian)\n\n\nRows: 39\nColumns: 7\n$ peak_no          <chr> \"peak_3\", \"peak_4\", \"peak_7\", \"peak_8\", \"p…\n$ compound         <chr> \"Ethyl acetate\", \"Methyl propanoate\", \"Eth…\n$ category         <chr> \"Ester\", \"Ester\", \"Ester\", \"Ester\", \"Ester…\n$ d101             <dbl> 0.28, 0.97, 3.11, 0.46, 0.19, 0.30, 4.07, …\n$ d2               <dbl> 0.61, 0.88, 1.85, 0.51, 0.09, 0.45, 2.33, …\n$ d24              <dbl> 0.93, 0.71, 2.53, 0.52, 0.56, 0.00, 2.29, …\n$ odor_description <chr> \"Pleasant, ethereal, fruity, brandy-like o…\n\n# so that can pivot longer later\n# durian$d101 <- as.character(durian$d101)\n# durian$d2 <- as.character(durian$d2)\ndurian$d24 <- as.numeric(durian$d24)\n\n# TRANSFORM #####\n\ndurian_reshape <- durian %>% \n  \n  # remove unnecessary columns\n  select(-category, -odor_description, -compound) %>% \n  # pivot longer for variety\n  pivot_longer(cols = starts_with(\"d\"),\n               names_to = \"variety\",\n               values_to = \"concentration\") %>% \n  \n  pivot_wider(names_from = peak_no,\n              values_from = concentration) %>% \n  \n  clean_names() %>% \n\n  # pivot wider for compound names as (X)/Features\n  dplyr::group_by(variety) %>% \n  dplyr::summarize_all(sum, na.rm = T)\n\n  \nglimpse(durian_reshape)  # 40 variables: 1Y and 39 X\n\n\nRows: 3\nColumns: 40\n$ variety <chr> \"d101\", \"d2\", \"d24\"\n$ peak_3  <dbl> 0.28, 0.61, 0.93\n$ peak_4  <dbl> 0.97, 0.88, 0.71\n$ peak_7  <dbl> 3.11, 1.85, 2.53\n$ peak_8  <dbl> 0.46, 0.51, 0.52\n$ peak_9  <dbl> 0.19, 0.09, 0.56\n$ peak_10 <dbl> 0.30, 0.45, 0.00\n$ peak_11 <dbl> 4.07, 2.33, 2.29\n$ peak_12 <dbl> 0.85, 2.22, 0.04\n$ peak_13 <dbl> 4.63, 1.74, 3.81\n$ peak_14 <dbl> 21.89, 29.68, 4.97\n$ peak_15 <dbl> 0.32, 0.22, 0.22\n$ peak_17 <dbl> 0.95, 0.63, 0.95\n$ peak_18 <dbl> 12.67, 4.77, 11.30\n$ peak_19 <dbl> 0.19, 0.00, 0.38\n$ peak_20 <dbl> 0.00, 0.14, 0.00\n$ peak_22 <dbl> 0.32, 1.70, 0.00\n$ peak_23 <dbl> 0.73, 0.00, 0.60\n$ peak_26 <dbl> 1.17, 5.52, 0.00\n$ peak_28 <dbl> 0.58, 0.45, 0.31\n$ peak_29 <dbl> 0.15, 0.25, 0.15\n$ peak_32 <dbl> 0.22, 0.10, 0.00\n$ peak_33 <dbl> 0.55, 0.55, 0.26\n$ peak_6  <dbl> 0.72, 1.09, 0.56\n$ peak_1  <dbl> 5.48, 4.26, 3.55\n$ peak_2  <dbl> 5.00, 2.72, 5.77\n$ peak_5  <dbl> 0.27, 0.00, 0.13\n$ peak_16 <dbl> 0.34, 0.00, 0.31\n$ peak_21 <dbl> 0.09, 0.06, 0.32\n$ peak_24 <dbl> 12.42, 15.85, 18.76\n$ peak_25 <dbl> 0.00, 0.00, 0.33\n$ peak_27 <dbl> 3.63, 3.35, 9.04\n$ peak_30 <dbl> 0.66, 0.14, 0.66\n$ peak_31 <dbl> 0.20, 0.11, 1.03\n$ peak_34 <dbl> 5.97, 14.68, 2.52\n$ peak_35 <dbl> 0.86, 1.73, 0.68\n$ peak_36 <dbl> 0.47, 1.46, 1.74\n$ peak_37 <dbl> 0.59, 1.47, 1.71\n$ peak_38 <dbl> 0.12, 0.16, 0.11\n$ peak_39 <dbl> 0.42, 0.49, 0.71\n\ndurian_reshape$variety <- factor(durian_reshape$variety)\n\n# EDA\nskim(durian_reshape)\n\n\nTable 1: Data summary\nName\ndurian_reshape\nNumber of rows\n3\nNumber of columns\n40\n_______________________\n\nColumn type frequency:\n\nfactor\n1\nnumeric\n39\n________________________\n\nGroup variables\nNone\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\nvariety\n0\n1\nFALSE\n3\nd10: 1, d2: 1, d24: 1\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\npeak_3\n0\n1\n0.61\n0.33\n0.28\n0.44\n0.61\n0.77\n0.93\n▇▁▇▁▇\npeak_4\n0\n1\n0.85\n0.13\n0.71\n0.79\n0.88\n0.92\n0.97\n▇▁▁▇▇\npeak_7\n0\n1\n2.50\n0.63\n1.85\n2.19\n2.53\n2.82\n3.11\n▇▁▇▁▇\npeak_8\n0\n1\n0.50\n0.03\n0.46\n0.48\n0.51\n0.52\n0.52\n▃▁▁▁▇\npeak_9\n0\n1\n0.28\n0.25\n0.09\n0.14\n0.19\n0.38\n0.56\n▇▇▁▁▇\npeak_10\n0\n1\n0.25\n0.23\n0.00\n0.15\n0.30\n0.38\n0.45\n▇▁▁▇▇\npeak_11\n0\n1\n2.90\n1.02\n2.29\n2.31\n2.33\n3.20\n4.07\n▇▁▁▁▃\npeak_12\n0\n1\n1.04\n1.10\n0.04\n0.44\n0.85\n1.54\n2.22\n▇▇▁▁▇\npeak_13\n0\n1\n3.39\n1.49\n1.74\n2.78\n3.81\n4.22\n4.63\n▇▁▁▇▇\npeak_14\n0\n1\n18.85\n12.63\n4.97\n13.43\n21.89\n25.78\n29.68\n▇▁▁▇▇\npeak_15\n0\n1\n0.25\n0.06\n0.22\n0.22\n0.22\n0.27\n0.32\n▇▁▁▁▃\npeak_17\n0\n1\n0.84\n0.18\n0.63\n0.79\n0.95\n0.95\n0.95\n▃▁▁▁▇\npeak_18\n0\n1\n9.58\n4.22\n4.77\n8.04\n11.30\n11.98\n12.67\n▃▁▁▁▇\npeak_19\n0\n1\n0.19\n0.19\n0.00\n0.10\n0.19\n0.29\n0.38\n▇▁▇▁▇\npeak_20\n0\n1\n0.05\n0.08\n0.00\n0.00\n0.00\n0.07\n0.14\n▇▁▁▁▃\npeak_22\n0\n1\n0.67\n0.90\n0.00\n0.16\n0.32\n1.01\n1.70\n▇▁▁▁▃\npeak_23\n0\n1\n0.44\n0.39\n0.00\n0.30\n0.60\n0.66\n0.73\n▃▁▁▁▇\npeak_26\n0\n1\n2.23\n2.91\n0.00\n0.58\n1.17\n3.34\n5.52\n▇▇▁▁▇\npeak_28\n0\n1\n0.45\n0.14\n0.31\n0.38\n0.45\n0.52\n0.58\n▇▁▇▁▇\npeak_29\n0\n1\n0.18\n0.06\n0.15\n0.15\n0.15\n0.20\n0.25\n▇▁▁▁▃\npeak_32\n0\n1\n0.11\n0.11\n0.00\n0.05\n0.10\n0.16\n0.22\n▇▁▇▁▇\npeak_33\n0\n1\n0.45\n0.17\n0.26\n0.41\n0.55\n0.55\n0.55\n▃▁▁▁▇\npeak_6\n0\n1\n0.79\n0.27\n0.56\n0.64\n0.72\n0.90\n1.09\n▇▇▁▁▇\npeak_1\n0\n1\n4.43\n0.98\n3.55\n3.90\n4.26\n4.87\n5.48\n▇▇▁▁▇\npeak_2\n0\n1\n4.50\n1.59\n2.72\n3.86\n5.00\n5.38\n5.77\n▇▁▁▇▇\npeak_5\n0\n1\n0.13\n0.14\n0.00\n0.06\n0.13\n0.20\n0.27\n▇▁▇▁▇\npeak_16\n0\n1\n0.22\n0.19\n0.00\n0.16\n0.31\n0.32\n0.34\n▃▁▁▁▇\npeak_21\n0\n1\n0.16\n0.14\n0.06\n0.07\n0.09\n0.21\n0.32\n▇▁▁▁▃\npeak_24\n0\n1\n15.68\n3.17\n12.42\n14.13\n15.85\n17.30\n18.76\n▇▁▇▁▇\npeak_25\n0\n1\n0.11\n0.19\n0.00\n0.00\n0.00\n0.16\n0.33\n▇▁▁▁▃\npeak_27\n0\n1\n5.34\n3.21\n3.35\n3.49\n3.63\n6.33\n9.04\n▇▁▁▁▃\npeak_30\n0\n1\n0.49\n0.30\n0.14\n0.40\n0.66\n0.66\n0.66\n▃▁▁▁▇\npeak_31\n0\n1\n0.45\n0.51\n0.11\n0.16\n0.20\n0.62\n1.03\n▇▁▁▁▃\npeak_34\n0\n1\n7.72\n6.27\n2.52\n4.24\n5.97\n10.32\n14.68\n▇▇▁▁▇\npeak_35\n0\n1\n1.09\n0.56\n0.68\n0.77\n0.86\n1.29\n1.73\n▇▁▁▁▃\npeak_36\n0\n1\n1.22\n0.67\n0.47\n0.96\n1.46\n1.60\n1.74\n▇▁▁▇▇\npeak_37\n0\n1\n1.26\n0.59\n0.59\n1.03\n1.47\n1.59\n1.71\n▇▁▁▇▇\npeak_38\n0\n1\n0.13\n0.03\n0.11\n0.11\n0.12\n0.14\n0.16\n▇▁▁▁▃\npeak_39\n0\n1\n0.54\n0.15\n0.42\n0.45\n0.49\n0.60\n0.71\n▇▇▁▁▇\n\n# no missing values\n# should do auto-scale and means centering later\n\n# Check assumptions for EDA\n\ndurian_no_y <- durian_reshape %>% \n  dplyr::select(-variety)\n\n# KMO test\ndurian_no_y %>% \n  cor() %>% \n  KMO() # overall MSA = 0.5\n\n\nError in solve.default(r) : \n  system is computationally singular: reciprocal condition number = 2.35978e-20\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = .)\nOverall MSA =  0.5\nMSA for each item = \n peak_3  peak_4  peak_7  peak_8  peak_9 peak_10 peak_11 peak_12 \n    0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5 \npeak_13 peak_14 peak_15 peak_17 peak_18 peak_19 peak_20 peak_22 \n    0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5 \npeak_23 peak_26 peak_28 peak_29 peak_32 peak_33  peak_6  peak_1 \n    0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5 \n peak_2  peak_5 peak_16 peak_21 peak_24 peak_25 peak_27 peak_30 \n    0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5 \npeak_31 peak_34 peak_35 peak_36 peak_37 peak_38 peak_39 \n    0.5     0.5     0.5     0.5     0.5     0.5     0.5 \n\n# Bartlett \n\ndurian_no_y %>% \n  cor() %>% \n  cortest.bartlett(., n = 3) # p = 1, by right not suitable for PCA\n\n\n$chisq\n[1] -Inf\n\n$p.value\n[1] 1\n\n$df\n[1] 741\n\n# 3 observations - not really ok for PCA\n\n# PCA ####\nglimpse(durian_reshape)\n\n\nRows: 3\nColumns: 40\n$ variety <fct> d101, d2, d24\n$ peak_3  <dbl> 0.28, 0.61, 0.93\n$ peak_4  <dbl> 0.97, 0.88, 0.71\n$ peak_7  <dbl> 3.11, 1.85, 2.53\n$ peak_8  <dbl> 0.46, 0.51, 0.52\n$ peak_9  <dbl> 0.19, 0.09, 0.56\n$ peak_10 <dbl> 0.30, 0.45, 0.00\n$ peak_11 <dbl> 4.07, 2.33, 2.29\n$ peak_12 <dbl> 0.85, 2.22, 0.04\n$ peak_13 <dbl> 4.63, 1.74, 3.81\n$ peak_14 <dbl> 21.89, 29.68, 4.97\n$ peak_15 <dbl> 0.32, 0.22, 0.22\n$ peak_17 <dbl> 0.95, 0.63, 0.95\n$ peak_18 <dbl> 12.67, 4.77, 11.30\n$ peak_19 <dbl> 0.19, 0.00, 0.38\n$ peak_20 <dbl> 0.00, 0.14, 0.00\n$ peak_22 <dbl> 0.32, 1.70, 0.00\n$ peak_23 <dbl> 0.73, 0.00, 0.60\n$ peak_26 <dbl> 1.17, 5.52, 0.00\n$ peak_28 <dbl> 0.58, 0.45, 0.31\n$ peak_29 <dbl> 0.15, 0.25, 0.15\n$ peak_32 <dbl> 0.22, 0.10, 0.00\n$ peak_33 <dbl> 0.55, 0.55, 0.26\n$ peak_6  <dbl> 0.72, 1.09, 0.56\n$ peak_1  <dbl> 5.48, 4.26, 3.55\n$ peak_2  <dbl> 5.00, 2.72, 5.77\n$ peak_5  <dbl> 0.27, 0.00, 0.13\n$ peak_16 <dbl> 0.34, 0.00, 0.31\n$ peak_21 <dbl> 0.09, 0.06, 0.32\n$ peak_24 <dbl> 12.42, 15.85, 18.76\n$ peak_25 <dbl> 0.00, 0.00, 0.33\n$ peak_27 <dbl> 3.63, 3.35, 9.04\n$ peak_30 <dbl> 0.66, 0.14, 0.66\n$ peak_31 <dbl> 0.20, 0.11, 1.03\n$ peak_34 <dbl> 5.97, 14.68, 2.52\n$ peak_35 <dbl> 0.86, 1.73, 0.68\n$ peak_36 <dbl> 0.47, 1.46, 1.74\n$ peak_37 <dbl> 0.59, 1.47, 1.71\n$ peak_38 <dbl> 0.12, 0.16, 0.11\n$ peak_39 <dbl> 0.42, 0.49, 0.71\n\n# recipe\ndurian_recipe <- recipe(~ ., data = durian_reshape) %>% \n  update_role(variety, new_role = \"id\") %>%  \n  # step_naomit(all_predictors()) %>% \n  step_normalize(all_predictors()) %>% \n  step_pca(all_predictors(), id = \"pca\", num_comp = 3)\n\n\n# need to specify num_comp = 3 if not will have error\n# Error in comps[, 1:object$num_comp, drop = FALSE] : \n# subscript out of bounds\n\n\n# prep: estimate the required parameters from a training set\n# that can be later applied to other data sets\n# returns an updated recipe with its estimates\n\ndurian_prep <- prep(durian_recipe)\n\ntidy_pca_loadings <- durian_prep %>% \n  tidy(id = \"pca\")\n\n\n# bake\n\ndurian_bake <- bake(durian_prep, durian_reshape)\n\n\n# plot loadings for top 8\n\nloadings_top_8 <- tidy_pca_loadings %>% \n  group_by(component) %>% \n  top_n(8, abs(value)) %>% \n  ungroup() %>% \n  mutate(terms = tidytext::reorder_within(terms, abs(value), component)) %>% \n  ggplot(aes(abs(value), terms, fill = value>0)) +\n  geom_col() +\n  facet_wrap(~component, scales = \"free_y\") +\n  tidytext::scale_y_reordered() +\n  ggthemes::scale_fill_few() +\n  theme_minimal()\n\n\njuice(durian_prep) %>% \n  ggplot(aes(PC1, PC2, label = variety)) +\n  geom_point(aes(col = variety), show.legend = F) +\n  geom_text() +\n  labs(x = \"PC1\",\n       y = \"PC2\") +\n  theme_classic()\n\n\n\n# loadings only\n\n# define arrow style\narrow_style <- arrow(angle = 30,\n                     length = unit(0.02, \"inches\"),\n                     type = \"closed\")\n\n# get pca loadings into wider format\npca_loadings_wider <- tidy_pca_loadings%>% \n  pivot_wider(names_from = component, id_cols = terms)\n\n\npca_loadings_only <- pca_loadings_wider %>% \n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_segment(aes(xend = PC1, yend = PC2),\n               x = 0, \n               y = 0,\n               arrow = arrow_style) +\n  ggrepel::geom_text_repel(aes(x = PC1, y = PC2, label = terms),\n                           hjust = 0, \n                           vjust = 1,\n                           size = 4,\n                           color = \"deepskyblue4\") +\n  labs(title = \"Loadings on PCs 1 and 2 for normalized data\") +\n  theme_classic()\n\n\n\n# check raw data\n\n# PC 1\npc1_raw <- durian %>% \n  filter(peak_no %in% c(\"peak_19\",\n                        \"peak_12\",\n                        \"peak_6\",\n                        \"peak_34\",\n                        \"peak_2\",\n                        \"peak_10\",\n                        \"peak_14\",\n                        \"peak_26\"))\n\n\n\n# PC 2\npc2_raw <- durian %>% \n  filter(peak_no %in% c(\"peak_11\",\n                        \"peak_15\",\n                        \"peak_8\",\n                        \"peak_37\",\n                        \"peak_36\",\n                        \"peak_1\",\n                        \"peak_32\",\n                        \"peak_24\"))\n\n\npc1_raw %>% arrange(peak_no)\n\n\n# A tibble: 8 x 7\n  peak_no compound    category   d101    d2   d24 odor_description    \n  <chr>   <chr>       <chr>     <dbl> <dbl> <dbl> <chr>               \n1 peak_10 Methyl but… Ester      0.3   0.45  0    Apple-like odor     \n2 peak_12 Ethyl buta… Ester      0.85  2.22  0.04 Fruity odor with pi…\n3 peak_14 Ethyl 2-me… Ester     21.9  29.7   4.97 Powerful green, fru…\n4 peak_19 Propyl 3-m… Ester      0.19  0     0.38 Fruity odor         \n5 peak_2  Propanethi… Sulfur-c…  5     2.72  5.77 Cabbage, sweet onio…\n6 peak_26 Ethyl hexa… Ester      1.17  5.52  0    Powerful fruity odo…\n7 peak_34 Diethyl tr… Sulfur-c…  5.97 14.7   2.52 Sweet alliaceous od…\n8 peak_6  Ethanol     Alcohol    0.72  1.09  0.56 <NA>                \n\npc2_raw %>%  arrange(peak_no)\n\n\n# A tibble: 8 x 7\n  peak_no compound      category   d101    d2   d24 odor_description  \n  <chr>   <chr>         <chr>     <dbl> <dbl> <dbl> <chr>             \n1 peak_1  Ethanethiol   Sulfur-c…  5.48  4.26  3.55 Onion, rubber odor\n2 peak_11 Methyl 2-but… Ester      4.07  2.33  2.29 Sweet fruity, app…\n3 peak_15 Ethyl 3-meth… Ester      0.32  0.22  0.22 Fruity odor remin…\n4 peak_24 Diethyl disu… Sulfur-c… 12.4  15.8  18.8  Sulfury, roasty, …\n5 peak_32 Methyl octan… Ester      0.22  0.1   0    Powerful winey, f…\n6 peak_36 3,5-dimethyl… Sulfur-c…  0.47  1.46  1.74 Sulfury, heavy, c…\n7 peak_37 3,5-dimethyl… Sulfur-c…  0.59  1.47  1.71 Sulfury, onion od…\n8 peak_8  Ethyl 2-meth… Ester      0.46  0.51  0.52 Fruity aromatic o…\n\npca_loadings_only\n\n\n\nloadings_top_8\n\n\n\n\nLearning pointers\nI feel that data visualization is a very important data exploratory tool to better understand your data. After data visualization, PCA can be performed to further explore your data and uncover latent structures. Together with the insights from earlier visualizations, the findings of PCA could be better interpreted.\nThe number of observations should not be so small until it is a bit meaningless to carry out PCA. This, was due to me carrying out analysis on aggregated data. I would need to remember to carry out more replicates if I am doing this experiment in the lab.\nWhat I like about the paper was that there was proper documentation on how extraction efficiency was optimised through sample size, vial size, the use of salting out, as well as equilibration time. The use of salting out is rather controversial as salt alters the equilibrium space between SPME fiber coatings and headspace. The results with and without addition of salt should always be compared to understand the effect of salt addition.\nIn addition, internal standard was used as a semi-quantitative analysis for relative concentration of volatile compounds. This would be better than just comparing percentage area of compounds because it gives the concentration in “absolute” value. However, it is still a semi-quantitative method as the IS cannot correct for differences in ionization during analysis, but it is better than nothing.\nFlavor analysis is not straightforward as numbers used to describe concentration do not indicate odor threshold and intensity perceived. They also do not descripe the type of odor. I wonder if text analysis could be applied to odor descriptions in flavor analysis? Odor threshold is further influenced by chemical structure, and extraction efficiency is also affected by sample matrix and volatility of compound when SPME is used as extraction. SPME offers a snapshot of flavor of food, but it would be more robust to compare against other extraction techniques as well. The ideal extraction method should not introduce artefacts (high temperature extraction, use of solvents etc), and requires high-end techniques. Alas, not every lab is that well-equipped. However, we should always make sure that our data is “clean,” so that our insights are factually correct and not contaminated by errors in extraction. The most advanced data analytics cannot correct for erroneous data, and any further analysis on such data carries no meaning.\n\n\n\nChin, S. T., S. A. H. Nazimah, S. Y. Quek, Y. B. Che Man, R. Abdul Rahman, and D. Mat Hashim. 2007. “Analysis of Volatile Compounds from Malaysian Durians (Durio Zibethinus) Using Headspace SPME Coupled to Fast GC-MS.” Journal of Food Composition and Analysis 20 (1): 31–44. https://doi.org/10.1016/j.jfca.2006.04.011.\n\n\nTakeoka, Gary R., Ron G. Buttery, Jean G. Turnbaugh, and Mabry Benson. 1995. “Odor Thresholds of Various Branched Esters.” LWT - Food Science and Technology 28 (1): 153–56. https://doi.org/10.1016/S0023-6438(95)80028-X.\n\n\nTeh, Bin Tean, Kevin Lim, Chern Han Yong, Cedric Chuan Young Ng, Sushma Ramesh Rao, Vikneswari Rajasegaran, Weng Khong Lim, et al. 2017. “The Draft Genome of Tropical Fruit Durian ( Durio Zibethinus ).” Nature Genetics 49 (11): 1633–41. https://doi.org/10.1038/ng.3972.\n\n\n\n\n",
    "preview": "posts/20210216_durian volatiles/Durian-Volatiles_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-02-16T23:53:30+08:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1152
  },
  {
    "path": "posts/20210209_tidy_tuesday_coffee_ratings/",
    "title": "Tidy Tuesday on Coffee Ratings Dataset",
    "description": "Exploratory Data Analysis on Coffee Ratings",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-02-09",
    "categories": [],
    "contents": "\nObjective\nTo practice data transformation and visualization on a tidytuesday dataset that is relatable to food (since I am a food science graduate).\nThe main areas that I will focus on would be the scoring differences between types of coffee (Arabica vs Robusta), processing methods (Wet vs Dry), country of origin/companies (top 6 by score), as well as varieties (top 6 by count).\nLoad packages\n\n\nlibrary(pacman)\np_load(tidyverse,skimr,tidytuesdayR, ggthemes, GGally, broom)\n\n\n\nImport\n\n\ntuesdata <- tidytuesdayR::tt_load(2020, week = 28)\n\n\n\n    Downloading file 1 of 1: `coffee_ratings.csv`\n\ncoffee_ratings <- tuesdata$coffee_ratings\n\n\n\nUnderstanding the data\nSkimming the data using the skimr package.\n\n\nskim(coffee_ratings)\n\n\nTable 1: Data summary\nName\ncoffee_ratings\nNumber of rows\n1339\nNumber of columns\n43\n_______________________\n\nColumn type frequency:\n\ncharacter\n24\nnumeric\n19\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nspecies\n0\n1.00\n7\n7\n0\n2\n0\nowner\n7\n0.99\n3\n50\n0\n315\n0\ncountry_of_origin\n1\n1.00\n4\n28\n0\n36\n0\nfarm_name\n359\n0.73\n1\n73\n0\n571\n0\nlot_number\n1063\n0.21\n1\n71\n0\n227\n0\nmill\n315\n0.76\n1\n77\n0\n460\n0\nico_number\n151\n0.89\n1\n40\n0\n847\n0\ncompany\n209\n0.84\n3\n73\n0\n281\n0\naltitude\n226\n0.83\n1\n41\n0\n396\n0\nregion\n59\n0.96\n2\n76\n0\n356\n0\nproducer\n231\n0.83\n1\n100\n0\n691\n0\nbag_weight\n0\n1.00\n1\n8\n0\n56\n0\nin_country_partner\n0\n1.00\n7\n85\n0\n27\n0\nharvest_year\n47\n0.96\n3\n24\n0\n46\n0\ngrading_date\n0\n1.00\n13\n20\n0\n567\n0\nowner_1\n7\n0.99\n3\n50\n0\n319\n0\nvariety\n226\n0.83\n4\n21\n0\n29\n0\nprocessing_method\n170\n0.87\n5\n25\n0\n5\n0\ncolor\n218\n0.84\n4\n12\n0\n4\n0\nexpiration\n0\n1.00\n13\n20\n0\n566\n0\ncertification_body\n0\n1.00\n7\n85\n0\n26\n0\ncertification_address\n0\n1.00\n40\n40\n0\n32\n0\ncertification_contact\n0\n1.00\n40\n40\n0\n29\n0\nunit_of_measurement\n0\n1.00\n1\n2\n0\n2\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\ntotal_cup_points\n0\n1.00\n82.09\n3.50\n0\n81.08\n82.50\n83.67\n90.58\n▁▁▁▁▇\nnumber_of_bags\n0\n1.00\n154.18\n129.99\n0\n14.00\n175.00\n275.00\n1062.00\n▇▇▁▁▁\naroma\n0\n1.00\n7.57\n0.38\n0\n7.42\n7.58\n7.75\n8.75\n▁▁▁▁▇\nflavor\n0\n1.00\n7.52\n0.40\n0\n7.33\n7.58\n7.75\n8.83\n▁▁▁▁▇\naftertaste\n0\n1.00\n7.40\n0.40\n0\n7.25\n7.42\n7.58\n8.67\n▁▁▁▁▇\nacidity\n0\n1.00\n7.54\n0.38\n0\n7.33\n7.58\n7.75\n8.75\n▁▁▁▁▇\nbody\n0\n1.00\n7.52\n0.37\n0\n7.33\n7.50\n7.67\n8.58\n▁▁▁▁▇\nbalance\n0\n1.00\n7.52\n0.41\n0\n7.33\n7.50\n7.75\n8.75\n▁▁▁▁▇\nuniformity\n0\n1.00\n9.83\n0.55\n0\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\n0\n1.00\n9.84\n0.76\n0\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\n0\n1.00\n9.86\n0.62\n0\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\ncupper_points\n0\n1.00\n7.50\n0.47\n0\n7.25\n7.50\n7.75\n10.00\n▁▁▁▇▁\nmoisture\n0\n1.00\n0.09\n0.05\n0\n0.09\n0.11\n0.12\n0.28\n▃▇▅▁▁\ncategory_one_defects\n0\n1.00\n0.48\n2.55\n0\n0.00\n0.00\n0.00\n63.00\n▇▁▁▁▁\nquakers\n1\n1.00\n0.17\n0.83\n0\n0.00\n0.00\n0.00\n11.00\n▇▁▁▁▁\ncategory_two_defects\n0\n1.00\n3.56\n5.31\n0\n0.00\n2.00\n4.00\n55.00\n▇▁▁▁▁\naltitude_low_meters\n230\n0.83\n1750.71\n8669.44\n1\n1100.00\n1310.64\n1600.00\n190164.00\n▇▁▁▁▁\naltitude_high_meters\n230\n0.83\n1799.35\n8668.81\n1\n1100.00\n1350.00\n1650.00\n190164.00\n▇▁▁▁▁\naltitude_mean_meters\n230\n0.83\n1775.03\n8668.63\n1\n1100.00\n1310.64\n1600.00\n190164.00\n▇▁▁▁▁\n\nThis is an incomplete dataset. I am not familiar with all the terms, such as ICO number, altitude, certification details.\nTo address my focal questions, I would need to take note that there are missing values in:\ncountry of origin\nvariety\nprocessing method\nThe distribution for scoring criteria is quite right-skewed. The total cup points is also very right skewed, most of the coffee graded are probably good coffee, so this may not be a representative dataset since it only contains information on above average coffee, but does not show data for average and sub-par coffee.\nSpecies: Arabica vs Robusta\n\n\ncoffee_ratings %>% \n  select(species) %>% \n  count(species) %>% # equivalent to df %>% group_by(a, b) %>% summarise(n = n()).\n  mutate(percentage = n/sum(n)*100) %>%  # need not group by first\n  ggplot(aes(species,percentage)) +\n  geom_col(aes(fill = species)) +\n  scale_fill_few() + # ggthemes: Color scales from Few's \"Practical Rules for Using Color in Charts\"\n  labs(title = \"Distribution of Arabica and Robusta coffe\",\n       subtitle = \"Most of the coffee graded are Arabica coffee\",\n       x = \"Species\",\n       y = \"Percentage of samples\",\n       caption = \"Source: Coffee Quality Institute\") +\n  theme_clean()\n\n\n\n\nEven though there is very little representation from Robusta coffee, which is considered to be a more inferior type, out of curiosity and for data exploratory purposes, I will look at the averate total cup score. Personally, I prefer the Robusta type of coffee unique to Singapore and Malaysia because of the way coffee beans are fried with butter and sugar, which gives it a unique aromatic taste.\n\n\ncoffee_ratings %>% \n  select(species,total_cup_points) %>% \n  group_by(species) %>% \n  summarise(mean = mean(total_cup_points)) %>% \n  ggplot(aes(x = species, y = mean, label = round(mean,1))) +\n  geom_col(aes(fill = species)) +\n  geom_text(aes(label = round(mean,1)), vjust = -0.5) +\n  scale_fill_few() +\n  labs(title = \"Mean Total Cup Points for Arabica and Robusta\",\n       subtitle = \"Arabica has higher mean score than Robusta\",\n       caption  = \"Source: Coffee Quality Institute\") +\n  ylim(0,100) +\n  theme_clean()\n\n\n\n\nProcessing method\nTo compare like with like, I will look the effect of processing methods on scores for Arabica coffee only.\n\n\narabica <- coffee_ratings %>% \n  filter(species == \"Arabica\")\n\n\n\nThe plot below shows what the commonly used processing methods are.\n\n\narabica %>% \n  filter(!is.na(processing_method)) %>% \n  count(processing_method) %>% \n  mutate(percentage = n/sum(n)*100) %>% \n  arrange(desc(percentage)) %>% \n  ggplot(aes(reorder(processing_method, percentage),percentage),\n         label = round(percentage,1)) +\n  geom_col(aes(fill = processing_method), width = 0.75) +\n  scale_color_few() +\n  geom_text(aes(label = round(percentage,1), hjust = -0.15)) +\n  labs(title = \"Distribution by Processing Method\",\n       subtitle = \"Most of the Arabica Coffee were either Wet or Dry Processed\",\n       caption = \"Source: Coffee Quality Institute\",\n       x = NULL) +\n  coord_flip() +\n  theme_clean() +\n  theme(legend.position = \"none\")\n\n\n\n\nI did some reading online (see Reference section below), and found that there were three main types of processing methods:\nWet/Washed: Most specialty coffees are washed, and the fruit flesh is removed from the bean before the beans are dried. There should be enough inherently present natural sugars in the bean so that sweetness will not be compromised.\nDry/Natural: The fruit remains on the bean and dries undisturbed. This is considered to be a lower quality method that may lead to inconsistent flavors due to unripe fruit drying and turning brown alongside ripe fruits.\nHoney: Often has a rounded acidity than washed coffees, with intense sweetness and complex mouthfeel.\nOthers: May include anaerobic processing, carbonic maceration etc.\nFor the purpose of comparing the scores across processing methods, I will just look at Wet vs Dry processing.\nHowever, it is important to compare like with like for different processing methods. What does the total cup points mean? The total cup points could be used as a classifier:\n95 - 100: Super Premium Specialty\n90 - 94: Premium Specialty\n85 - 89: Specialty\n80 - 84: Premium\n75 - 79: Usual Good Quality\n70 - 74: Average Quality\n60 - 70: Exchange grade\n50 - 60: Commercial grade\nI will add in the class into the dataset to compare effect of processing method in the class with the most datapoints.\nEDA on total cup points\n\n\nsensory <- coffee_ratings %>% \n  select(total_cup_points, species, country_of_origin,\n         processing_method:category_two_defects)\n\nsensory %>% \n  ggplot(aes(total_cup_points)) +\n  geom_histogram(fill = \"chocolate4\") +\n  theme_few()\n\n\n\nmin(sensory$total_cup_points)  # 0 : has missing values\n\n\n[1] 0\n\ntable(sensory$total_cup_points) # 1 missing value, lowest is 59.83\n\n\n\n    0 59.83 63.08 67.92 68.33 69.17 69.33 70.67 70.75    71 71.08 \n    1     1     1     1     1     2     1     1     1     1     1 \n71.75 72.33 72.58 72.83 72.92 73.42  73.5 73.67 73.75 73.83    74 \n    1     1     1     1     1     1     1     1     1     1     1 \n74.33 74.42 74.67 74.75 74.83 74.92    75 75.08 75.17  75.5 75.58 \n    2     1     1     2     1     1     1     1     3     1     2 \n75.67 75.83    76 76.08 76.17 76.25 76.33 76.42  76.5 76.75 76.83 \n    1     1     1     1     3     1     2     1     1     1     1 \n   77 77.17 77.25 77.33 77.42  77.5 77.58 77.67 77.83 77.92    78 \n    1     2     2     3     1     1     1     1     3     3     8 \n78.08 78.17 78.25 78.33 78.42  78.5 78.58 78.67 78.75 78.83 78.92 \n    2     1     2     5     2     3     7     2     6     1     2 \n   79 79.08 79.17 79.25 79.33 79.42  79.5 79.58 79.67 79.75 79.83 \n    6     6     8     2     6     3     5     4     8    13     5 \n79.92    80 80.08 80.17 80.25 80.33 80.42  80.5 80.58 80.67 80.75 \n    9     8     8    11    11     8     7    12     9    11    12 \n80.83 80.92    81 81.08 81.17 81.25 81.33 81.42  81.5 81.58 81.67 \n    7    18    15    12    15    10    12    17    26    17    25 \n81.75 81.83 81.92    82 82.08 82.17 82.25 82.33 82.42  82.5 82.58 \n   12    26    18    21    17    21    22    29    32    23    21 \n82.67 82.75 82.83 82.92    83 83.08 83.17 83.25 83.33 83.38 83.42 \n   26    30    19    26    39    18    38    25    20     1    20 \n 83.5 83.58 83.67 83.75 83.83 83.92    84 84.08 84.13 84.17 84.25 \n   25    16    21    20    21    16    18     8     1    21    19 \n84.33 84.42  84.5 84.58 84.67 84.75 84.83 84.92    85 85.08 85.17 \n   12     8    13    14    19     5     5     9    10     8     2 \n85.25 85.33 85.42  85.5 85.58 85.75 85.83 85.92    86 86.08 86.17 \n    3     8     5     5     3     3     4     3     6     3     4 \n86.25 86.33 86.42  86.5 86.58 86.67 86.83 86.92 87.08 87.17 87.25 \n    5     1     1     1     2     1     1     2     2     2     3 \n87.33 87.42 87.58 87.83 87.92 88.08 88.25 88.42 88.67 88.75 88.83 \n    1     1     1     1     3     1     1     1     1     1     2 \n   89 89.75 89.92 90.58 \n    1     1     1     1 \n\nCreating a classification variable\n\n\nsensory_with_category <- sensory %>% \n  filter(total_cup_points != 0) %>% # remove zero score\n  mutate(classification = ifelse(total_cup_points > 95, \"Super Premium Specialty\",\n                                 ifelse(total_cup_points >90, \"Premium Specialty\",\n                                        ifelse(total_cup_points >85, \"Specialty\",\n                                               ifelse(total_cup_points >80, \"Premium\",\n                                                      ifelse(total_cup_points >75, \"Usual Good Quality\",\n                                                             ifelse(total_cup_points >70, \"Average Quality\",\n                                                                    ifelse(total_cup_points >60, \"Exchange grade\",\n                                                                           \"Commercial grade\"))))))))\n\n\n\nUnderstanding the coffee with the highest score:\n\n\nsensory_with_category %>% \n  select(total_cup_points, classification) %>% \n  arrange(desc(total_cup_points))\n\n\n# A tibble: 1,338 x 2\n   total_cup_points classification   \n              <dbl> <chr>            \n 1             90.6 Premium Specialty\n 2             89.9 Specialty        \n 3             89.8 Specialty        \n 4             89   Specialty        \n 5             88.8 Specialty        \n 6             88.8 Specialty        \n 7             88.8 Specialty        \n 8             88.7 Specialty        \n 9             88.4 Specialty        \n10             88.2 Specialty        \n# … with 1,328 more rows\n\nmin(coffee_ratings$total_cup_points)\n\n\n[1] 0\n\n# which coffee had the highest score?\ncoffee_ratings %>% \n  filter(total_cup_points == max(coffee_ratings$total_cup_points)) %>% \n  t() # transpose\n\n\n                      [,1]                                      \ntotal_cup_points      \"90.58\"                                   \nspecies               \"Arabica\"                                 \nowner                 \"metad plc\"                               \ncountry_of_origin     \"Ethiopia\"                                \nfarm_name             \"metad plc\"                               \nlot_number            NA                                        \nmill                  \"metad plc\"                               \nico_number            \"2014/2015\"                               \ncompany               \"metad agricultural developmet plc\"       \naltitude              \"1950-2200\"                               \nregion                \"guji-hambela\"                            \nproducer              \"METAD PLC\"                               \nnumber_of_bags        \"300\"                                     \nbag_weight            \"60 kg\"                                   \nin_country_partner    \"METAD Agricultural Development plc\"      \nharvest_year          \"2014\"                                    \ngrading_date          \"April 4th, 2015\"                         \nowner_1               \"metad plc\"                               \nvariety               NA                                        \nprocessing_method     \"Washed / Wet\"                            \naroma                 \"8.67\"                                    \nflavor                \"8.83\"                                    \naftertaste            \"8.67\"                                    \nacidity               \"8.75\"                                    \nbody                  \"8.5\"                                     \nbalance               \"8.42\"                                    \nuniformity            \"10\"                                      \nclean_cup             \"10\"                                      \nsweetness             \"10\"                                      \ncupper_points         \"8.75\"                                    \nmoisture              \"0.12\"                                    \ncategory_one_defects  \"0\"                                       \nquakers               \"0\"                                       \ncolor                 \"Green\"                                   \ncategory_two_defects  \"0\"                                       \nexpiration            \"April 3rd, 2016\"                         \ncertification_body    \"METAD Agricultural Development plc\"      \ncertification_address \"309fcf77415a3661ae83e027f7e5f05dad786e44\"\ncertification_contact \"19fef5a731de2db57d16da10287413f5f99bc2dd\"\nunit_of_measurement   \"m\"                                       \naltitude_low_meters   \"1950\"                                    \naltitude_high_meters  \"2200\"                                    \naltitude_mean_meters  \"2075\"                                    \n\n# which coffee had the lowest score?\ncoffee_ratings %>% \n  filter(total_cup_points == 59.83) %>% \n  t() # transpose\n\n\n                      [,1]                                      \ntotal_cup_points      \"59.83\"                                   \nspecies               \"Arabica\"                                 \nowner                 \"juan luis alvarado romero\"               \ncountry_of_origin     \"Guatemala\"                               \nfarm_name             \"finca el limon\"                          \nlot_number            NA                                        \nmill                  \"beneficio serben\"                        \nico_number            \"11/853/165\"                              \ncompany               \"unicafe\"                                 \naltitude              \"4650\"                                    \nregion                \"nuevo oriente\"                           \nproducer              \"WILLIAM ESTUARDO MARTINEZ PACHECO\"       \nnumber_of_bags        \"275\"                                     \nbag_weight            \"1 kg\"                                    \nin_country_partner    \"Asociacion Nacional Del Café\"            \nharvest_year          \"2012\"                                    \ngrading_date          \"May 24th, 2012\"                          \nowner_1               \"Juan Luis Alvarado Romero\"               \nvariety               \"Catuai\"                                  \nprocessing_method     \"Washed / Wet\"                            \naroma                 \"7.5\"                                     \nflavor                \"6.67\"                                    \naftertaste            \"6.67\"                                    \nacidity               \"7.67\"                                    \nbody                  \"7.33\"                                    \nbalance               \"6.67\"                                    \nuniformity            \"8\"                                       \nclean_cup             \"1.33\"                                    \nsweetness             \"1.33\"                                    \ncupper_points         \"6.67\"                                    \nmoisture              \"0.1\"                                     \ncategory_one_defects  \"0\"                                       \nquakers               \"0\"                                       \ncolor                 \"Green\"                                   \ncategory_two_defects  \"4\"                                       \nexpiration            \"May 24th, 2013\"                          \ncertification_body    \"Asociacion Nacional Del Café\"            \ncertification_address \"b1f20fe3a819fd6b2ee0eb8fdc3da256604f1e53\"\ncertification_contact \"724f04ad10ed31dbb9d260f0dfd221ba48be8a95\"\nunit_of_measurement   \"ft\"                                      \naltitude_low_meters   \"1417.32\"                                 \naltitude_high_meters  \"1417.32\"                                 \naltitude_mean_meters  \"1417.32\"                                 \n\n# min score is actually 0, which is a missing datapoint.\n\n\n\n\n\n# distribution of types of coffee\nsensory_with_category %>% \n  filter(species == \"Arabica\",\n         processing_method %in% c(\"Natural / Dry\", \"Washed / Wet\")) %>% \n  count(classification, processing_method) %>% \n  ggplot(aes(fct_reorder(classification, n), n, label = n)) + \n  geom_col(aes(fill = classification)) +\n  scale_color_few() +\n  labs(title = \"Distribution of types of Arabica coffees, by processing method\",\n       subtitle = \"Most of the premium coffee (with cup scores 80 - 84) are processed by Washed/Wet method.\",\n       caption = \"Source: Coffee Quality Institute\") +\n  facet_grid(processing_method ~. ) +\n  theme_clean() +\n  coord_flip() +\n  theme(legend.position = \"none\")\n\n\n\n\nThe Premium category has the most number of datapoints, and I will focus on this category for analysis.\n\n\nplot_sensory_total_boxplot <- sensory_with_category %>% \n  filter(classification == \"Premium\",\n         species == \"Arabica\",\n         processing_method %in% c(\"Natural / Dry\", \"Washed / Wet\")) %>% \n  mutate(processing_mtd_fct = ifelse(processing_method == c(\"Natural / Dry\"), \"Dry\",\n                                     \"Wet\")) %>% \n  select(total_cup_points, processing_mtd_fct) %>% \n  ggplot(aes(x = processing_mtd_fct, y = total_cup_points)) +\n  geom_boxplot(aes(col = processing_mtd_fct),notch = T) +\n  stat_summary(fun.data = \"mean_cl_normal\",\n           geom = \"errorbar\",\n           fun.args = (conf.int = 0.95),\n           color = \"forestgreen\") +\n  geom_jitter(aes(col = processing_mtd_fct), alpha = 0.3) +\n  scale_color_manual(values = c(\"Dry\" = \"chocolate4\",\n                                \"Wet\" = \"cadetblue4\")) +\n  labs(title = \"Comparison of Mean Total Cup Points for Dry vs Wet Processing in Arabica Coffee\",\n       subtitle = \"The Mean Total Cup Points are very similar for both processing methods\",\n       caption = \"Source: Coffee Quality Institute\",\n       x = \"Processing Method\",\n       y = \"Total Cup Points\") +\n  theme_few() +\n  theme(legend.position = \"none\")\n\nplot_sensory_total_boxplot \n\n\n\n\n\n\nplot_sensory_boxplot <- sensory_with_category %>% \n  filter(classification == \"Premium\",\n         species == \"Arabica\",\n         processing_method %in% c(\"Natural / Dry\", \"Washed / Wet\")) %>% \n  mutate(processing_mtd_fct = ifelse(processing_method == c(\"Natural / Dry\"), \"Dry\",\n                                     \"Wet\")) %>% \n  select(-quakers, -color, - category_one_defects, \n         - category_two_defects, - processing_method) %>% \n  pivot_longer(cols = aroma:cupper_points,\n               names_to = \"parameters\",\n               values_to = \"score\") %>% \n  mutate(parameters_fct = factor(parameters,\n                                 levels = c(\"acidity\", \"aroma\", \"clean_cup\",\n                                            \"sweetness\", \"uniformity\", \"aftertaste\",\n                                            \"balance\", \"body\", \"cupper_points\", \"flavor\"\n                                 ))) %>% \n  ggplot(aes(x = processing_mtd_fct, y = score)) +\n  geom_boxplot(aes(col = processing_mtd_fct), notch = T, size = 1) +\n  geom_jitter(aes(col = processing_mtd_fct), alpha = 0.1) +\n  scale_color_manual(values = c(\"Dry\" = \"chocolate4\",\n                                \"Wet\" = \"cadetblue4\")) +\n  facet_wrap(vars(parameters_fct), scales = \"free\", ncol= 5) +\n  labs(x = NULL,\n       title = \"Comparison of mean score for Arabica coffee: Dry vs Wet Processing\",\n       subtitle = \"Wet processed coffee has higher average scores for acidity, aroma, clean_cup, sweetness, uniformity.\",\n       caption = \"Source: Coffee Quality Institute\") +\n  theme_few() +\n  theme(legend.position = \"none\")\n\nplot_sensory_boxplot\n\n\n\n\nCountry of origin/Owner\n\n\n# plot to see which countries are above/below mean rating\n\narabica_dotplot <- arabica %>% \n  filter(!is.na(country_of_origin)) %>% # 1 missing value\n  group_by(country_of_origin) %>% \n  summarise(mean_rating = mean(total_cup_points)) %>% \n  mutate(above_below_mean = as.factor(ifelse(mean_rating > mean(arabica$total_cup_points),\n                                             \"above_mean\", \"below_mean\"))) %>% \n  ggplot(aes(x = reorder(country_of_origin, mean_rating), \n             y = mean_rating, \n             col = above_below_mean,\n             label = round(mean_rating,1))) +\n  geom_point(aes(col = above_below_mean), stat = \"identity\", size = 9) +\n  scale_color_few() +\n  geom_text(col = \"black\", size = 4) +\n  geom_hline(aes(yintercept = mean(arabica$total_cup_points)), size = 2,\n             col = \"grey\")+\n  labs(title = \"Dot plot for Arabica Coffee Ratings\",\n       subtitle = \"Countries with ratings above mean values are coloured blue,\\nand countries below mean values are colored orange.\",\n       x =  \"Country of Origin\",\n       y = \"Mean Rating\",\n       caption = \"Source: Coffee Quality Institute\") +\n  coord_flip() +\n  theme_clean() +\n  theme(legend.position = \"none\",\n        axis.title = element_text(size = 16, face = \"bold\"),\n        axis.text = element_text(size = 14),\n        title = element_text(size = 20, face = \"bold\"))\n\narabica_dotplot\n\n\n\n\n\n\n# sensory scores for arabica coffee, top scorers for sensory\n\nsensory_by_country <- coffee_ratings %>% \n  filter(species == \"Arabica\",\n         !total_cup_points %in% 0,\n         !is.na(country_of_origin),\n         !is.na(owner)) %>% \n  select(country_of_origin, owner, \n         total_cup_points, aroma:cupper_points)\n\n\n\n\n\nskim(sensory_by_country)\n\n\nTable 2: Data summary\nName\nsensory_by_country\nNumber of rows\n1302\nNumber of columns\n13\n_______________________\n\nColumn type frequency:\n\ncharacter\n2\nnumeric\n11\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\ncountry_of_origin\n0\n1\n4\n28\n0\n36\n0\nowner\n0\n1\n3\n50\n0\n305\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\ntotal_cup_points\n0\n1\n82.18\n2.69\n59.83\n81.17\n82.50\n83.67\n90.58\n▁▁▁▇▁\naroma\n0\n1\n7.57\n0.32\n5.08\n7.42\n7.58\n7.75\n8.75\n▁▁▂▇▁\nflavor\n0\n1\n7.52\n0.34\n6.08\n7.33\n7.58\n7.75\n8.83\n▁▂▇▃▁\naftertaste\n0\n1\n7.40\n0.35\n6.17\n7.25\n7.42\n7.58\n8.67\n▁▃▇▂▁\nacidity\n0\n1\n7.54\n0.32\n5.25\n7.33\n7.50\n7.75\n8.75\n▁▁▃▇▁\nbody\n0\n1\n7.52\n0.29\n5.25\n7.33\n7.50\n7.67\n8.58\n▁▁▁▇▁\nbalance\n0\n1\n7.52\n0.35\n6.08\n7.33\n7.50\n7.75\n8.75\n▁▂▇▃▁\nuniformity\n0\n1\n9.84\n0.49\n6.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\n0\n1\n9.84\n0.72\n0.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\n0\n1\n9.91\n0.46\n1.33\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\ncupper_points\n0\n1\n7.50\n0.43\n5.17\n7.25\n7.50\n7.75\n10.00\n▁▂▇▁▁\n\nLooking at the coffee with clean cup score = 0: Is it really that the coffee had a score of 0? Or was it a data entry mistake?\n\n\n# there is one datapoint in which clean_cup score = 0\ncoffee_ratings %>% \n  filter(clean_cup == 0) %>% \n  t() # transpose\n\n\n                      [,1]                                                  \ntotal_cup_points      \"68.33\"                                               \nspecies               \"Arabica\"                                             \nowner                 \"juan carlos garcia lopez\"                            \ncountry_of_origin     \"Mexico\"                                              \nfarm_name             \"el centenario\"                                       \nlot_number            NA                                                    \nmill                  \"la esperanza, municipio juchique de ferrer, veracruz\"\nico_number            \"1104328663\"                                          \ncompany               \"terra mia\"                                           \naltitude              \"900\"                                                 \nregion                \"juchique de ferrer\"                                  \nproducer              \"JUAN CARLOS GARCÍA LOPEZ\"                            \nnumber_of_bags        \" 12\"                                                 \nbag_weight            \"1 kg\"                                                \nin_country_partner    \"AMECAFE\"                                             \nharvest_year          \"2012\"                                                \ngrading_date          \"September 17th, 2012\"                                \nowner_1               \"JUAN CARLOS GARCIA LOPEZ\"                            \nvariety               \"Bourbon\"                                             \nprocessing_method     \"Washed / Wet\"                                        \naroma                 \"7.08\"                                                \nflavor                \"6.83\"                                                \naftertaste            \"6.25\"                                                \nacidity               \"7.42\"                                                \nbody                  \"7.25\"                                                \nbalance               \"6.75\"                                                \nuniformity            \"10\"                                                  \nclean_cup             \"0\"                                                   \nsweetness             \"10\"                                                  \ncupper_points         \"6.75\"                                                \nmoisture              \"0.11\"                                                \ncategory_one_defects  \"0\"                                                   \nquakers               \"0\"                                                   \ncolor                 \"None\"                                                \ncategory_two_defects  \"20\"                                                  \nexpiration            \"September 17th, 2013\"                                \ncertification_body    \"AMECAFE\"                                             \ncertification_address \"59e396ad6e22a1c22b248f958e1da2bd8af85272\"            \ncertification_contact \"0eb4ee5b3f47b20b049548a2fd1e7d4a2b70d0a7\"            \nunit_of_measurement   \"m\"                                                   \naltitude_low_meters   \" 900\"                                                \naltitude_high_meters  \" 900\"                                                \naltitude_mean_meters  \" 900\"                                                \n                      [,2]                                      \ntotal_cup_points      \" 0.00\"                                   \nspecies               \"Arabica\"                                 \nowner                 \"bismarck castro\"                         \ncountry_of_origin     \"Honduras\"                                \nfarm_name             \"los hicaques\"                            \nlot_number            \"103\"                                     \nmill                  \"cigrah s.a de c.v.\"                      \nico_number            \"13-111-053\"                              \ncompany               \"cigrah s.a de c.v\"                       \naltitude              \"1400\"                                    \nregion                \"comayagua\"                               \nproducer              \"Reinerio Zepeda\"                         \nnumber_of_bags        \"275\"                                     \nbag_weight            \"69 kg\"                                   \nin_country_partner    \"Instituto Hondureño del Café\"            \nharvest_year          \"2017\"                                    \ngrading_date          \"April 28th, 2017\"                        \nowner_1               \"Bismarck Castro\"                         \nvariety               \"Caturra\"                                 \nprocessing_method     NA                                        \naroma                 \"0.00\"                                    \nflavor                \"0.00\"                                    \naftertaste            \"0.00\"                                    \nacidity               \"0.00\"                                    \nbody                  \"0.00\"                                    \nbalance               \"0.00\"                                    \nuniformity            \" 0\"                                      \nclean_cup             \"0\"                                       \nsweetness             \" 0\"                                      \ncupper_points         \"0.00\"                                    \nmoisture              \"0.12\"                                    \ncategory_one_defects  \"0\"                                       \nquakers               \"0\"                                       \ncolor                 \"Green\"                                   \ncategory_two_defects  \" 2\"                                      \nexpiration            \"April 28th, 2018\"                        \ncertification_body    \"Instituto Hondureño del Café\"            \ncertification_address \"b4660a57e9f8cc613ae5b8f02bfce8634c763ab4\"\ncertification_contact \"7f521ca403540f81ec99daec7da19c2788393880\"\nunit_of_measurement   \"m\"                                       \naltitude_low_meters   \"1400\"                                    \naltitude_high_meters  \"1400\"                                    \naltitude_mean_meters  \"1400\"                                    \n\n# one is missing value, already filtered out for total_cup_points = 0\n# the remaining one looks like it really has 0 for clean cup score\n\n7.08 + 6.83 + 6.25 + 7.42 + 7.25 + 6.75 + 10 + 10  + 6.75 # 68.33\n\n\n[1] 68.33\n\nIt turned out that total cup points is a summation of scores for aroma, flavor, aftertaste, acidity, body, balance, uniformity, clean_cup, sweetness and cupper_points.\n\n\ncountry_mean_score <- sensory_by_country %>% \n  group_by(country_of_origin, owner) %>% \n  summarise(mean_score = mean(total_cup_points)) %>% \n  arrange(desc(mean_score)) \n\ncountry_mean_score\n\n\n# A tibble: 350 x 3\n# Groups:   country_of_origin [36]\n   country_of_origin owner                              mean_score\n   <chr>             <chr>                                   <dbl>\n 1 Ethiopia          metad plc                                89.8\n 2 Guatemala         grounds for health admin                 89.8\n 3 Ethiopia          yidnekachew dabessa                      89  \n 4 Brazil            ji-ae ahn                                88.8\n 5 Peru              hugo valdivia                            88.8\n 6 Ethiopia          diamond enterprise plc                   88.2\n 7 Ethiopia          mohammed lalo                            88.1\n 8 Indonesia         grounds for health admin                 87.4\n 9 United States     cqi q coffee sample representative       87.3\n10 Mexico            roberto licona franco                    87.2\n# … with 340 more rows\n\nmin(country_mean_score$mean_score) # 68.33\n\n\n[1] 68.33\n\nmax(country_mean_score$mean_score) # 89.7767\n\n\n[1] 89.77667\n\nHow do the top 8 coffee owners by country compare against each other in terms of the ten scoring criteria?\n\n\n# plot profile for top 8 owners\n\ntop_owners_data <- sensory_by_country%>% \n  group_by(country_of_origin, owner) %>% \n  summarise_at(.vars = vars(total_cup_points:cupper_points),\n               .funs = c(mean = \"mean\")) %>% \n  ungroup() %>% \n  mutate(country_owner = str_c(country_of_origin, owner, sep = \",\"),\n         country_owner_fct = factor(country_owner, \n                                    levels =c(\"Ethiopia,metad plc\", \n                                             \"Guatemala,grounds for health admin\", \n                                             \"Ethiopia,yidnekachew dabessa\",\n                                             \"Brazil,ji-ae ahn\",\n                                             \"Peru,hugo valdivia\",\n                                             \"Ethiopia,diamond enterprise plc\",\n                                             \"Ethiopia,mohammed lalo\",\n                                             \"Indonesia,grounds for health admin\"))) %>% \n  group_by(country_owner_fct) %>% \n  arrange(desc(total_cup_points_mean)) %>% \n  ungroup() %>% \n  slice_max(total_cup_points_mean, n = 8) %>% \n  pivot_longer(cols = c(aroma_mean:cupper_points_mean),\n               names_to = \"parameters\",\n               values_to = \"score\") %>% \n\n  ggplot(aes(x = fct_rev(factor(parameters)), y = score, label = round(score, 1))) +\n  geom_point(stat = \"identity\", aes(col = factor(parameters)), size = 8) +\n  geom_text(col = \"black\", size = 4) +\n  facet_wrap(country_owner_fct~., scales = \"free_y\", ncol = 4) +\n  coord_flip() +\n  theme_few() +\n  theme(legend.position = \"none\")\n  \ntop_owners_data\n\n\n\n\nThe scores for clean_cup, sweetness, uniformity is a perfect 10 for all 8 owners. Slight differences were observed for mean scores for cupper_points, aftertaste and body. These were probably the distinguishing parameters.\nVariety\nThe first few sections above looked mainly at highly scored coffee. Would there be any differenced in scoring profile, if I were to look at different varieties of coffee?\n\n\nvariety_count <- coffee_ratings %>% \n  count(variety) %>% \n  arrange(desc(n)) # 30 observations\n\nhead(variety_count, 8) # NA: 226, Other: 226\n\n\n# A tibble: 8 x 2\n  variety            n\n  <chr>          <int>\n1 Caturra          256\n2 Bourbon          226\n3 <NA>             226\n4 Typica           211\n5 Other            110\n6 Catuai            74\n7 Hawaiian Kona     44\n8 Yellow Bourbon    35\n\ntail(variety_count)\n\n\n# A tibble: 6 x 2\n  variety                 n\n  <chr>               <int>\n1 Ethiopian Heirlooms     1\n2 Marigojipe              1\n3 Moka Peaberry           1\n4 Pache Comun             1\n5 Sulawesi                1\n6 Sumatra Lintong         1\n\ndata_variety <- coffee_ratings %>% \n  select(total_cup_points, species, owner, country_of_origin, processing_method,\n         variety, aroma:cupper_points, color) %>% \n  filter(variety %in% c(\"Caturra\", \"Bourbon\", \"Typica\", \"Catuai\", \n                        \"Hawaiian Kona\", \"Yellow Bourbon\")) %>% \n  group_by(variety)\n\nglimpse(data_variety)\n\n\nRows: 846\nColumns: 17\nGroups: variety [6]\n$ total_cup_points  <dbl> 89.75, 87.17, 86.92, 86.67, 86.42, 86.33,…\n$ species           <chr> \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica…\n$ owner             <chr> \"grounds for health admin\", \"the coffee s…\n$ country_of_origin <chr> \"Guatemala\", \"Costa Rica\", \"Brazil\", \"Hon…\n$ processing_method <chr> NA, \"Washed / Wet\", \"Natural / Dry\", NA, …\n$ variety           <chr> \"Bourbon\", \"Caturra\", \"Bourbon\", \"Caturra…\n$ aroma             <dbl> 8.42, 8.08, 8.50, 8.17, 8.50, 8.17, 8.08,…\n$ flavor            <dbl> 8.50, 8.25, 8.50, 8.08, 8.17, 7.83, 8.17,…\n$ aftertaste        <dbl> 8.42, 8.00, 8.00, 8.08, 8.00, 8.00, 8.00,…\n$ acidity           <dbl> 8.42, 8.17, 8.00, 8.00, 7.75, 8.08, 7.92,…\n$ body              <dbl> 8.33, 8.00, 8.00, 8.08, 8.00, 7.83, 7.92,…\n$ balance           <dbl> 8.42, 8.33, 8.00, 8.00, 8.00, 8.00, 7.83,…\n$ uniformity        <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ clean_cup         <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ sweetness         <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ cupper_points     <dbl> 9.25, 8.33, 7.92, 8.25, 8.00, 8.42, 8.33,…\n$ color             <chr> NA, \"Green\", \"Green\", \"Green\", \"Green\", \"…\n\n# Top 6 coffee by number of datapoints\n\ndata_variety %>% \n  count(species) # all arabica\n\n\n# A tibble: 6 x 3\n# Groups:   variety [6]\n  variety        species     n\n  <chr>          <chr>   <int>\n1 Bourbon        Arabica   226\n2 Catuai         Arabica    74\n3 Caturra        Arabica   256\n4 Hawaiian Kona  Arabica    44\n5 Typica         Arabica   211\n6 Yellow Bourbon Arabica    35\n\ndata_variety %>% \n  count(processing_method) # quite varied\n\n\n# A tibble: 30 x 3\n# Groups:   variety [6]\n   variety processing_method             n\n   <chr>   <chr>                     <int>\n 1 Bourbon Natural / Dry                38\n 2 Bourbon Other                         2\n 3 Bourbon Pulped natural / honey        2\n 4 Bourbon Semi-washed / Semi-pulped    11\n 5 Bourbon Washed / Wet                170\n 6 Bourbon <NA>                          3\n 7 Catuai  Natural / Dry                18\n 8 Catuai  Pulped natural / honey        2\n 9 Catuai  Semi-washed / Semi-pulped     6\n10 Catuai  Washed / Wet                 48\n# … with 20 more rows\n\ndata_variety %>% \n  ungroup() %>% \n  count(country_of_origin) %>% \n  arrange(desc(n))\n\n\n# A tibble: 25 x 2\n   country_of_origin          n\n   <chr>                  <int>\n 1 Mexico                   195\n 2 Guatemala                157\n 3 Colombia                 132\n 4 Brazil                    92\n 5 Taiwan                    66\n 6 Costa Rica                44\n 7 Honduras                  44\n 8 United States (Hawaii)    44\n 9 El Salvador               13\n10 Nicaragua                 13\n# … with 15 more rows\n\n\n\ndata_variety %>% \n  ungroup() %>% \n  group_by(variety) %>% \n  skim()\n\n\nTable 3: Data summary\nName\nPiped data\nNumber of rows\n846\nNumber of columns\n17\n_______________________\n\nColumn type frequency:\n\ncharacter\n5\nnumeric\n11\n________________________\n\nGroup variables\nvariety\nVariable type: character\nskim_variable\nvariety\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nspecies\nBourbon\n0\n1.00\n7\n7\n0\n1\n0\nspecies\nCatuai\n0\n1.00\n7\n7\n0\n1\n0\nspecies\nCaturra\n0\n1.00\n7\n7\n0\n1\n0\nspecies\nHawaiian Kona\n0\n1.00\n7\n7\n0\n1\n0\nspecies\nTypica\n0\n1.00\n7\n7\n0\n1\n0\nspecies\nYellow Bourbon\n0\n1.00\n7\n7\n0\n1\n0\nowner\nBourbon\n0\n1.00\n4\n50\n0\n44\n0\nowner\nCatuai\n2\n0.97\n5\n41\n0\n29\n0\nowner\nCaturra\n5\n0.98\n4\n45\n0\n45\n0\nowner\nHawaiian Kona\n0\n1.00\n15\n32\n0\n2\n0\nowner\nTypica\n0\n1.00\n8\n47\n0\n83\n0\nowner\nYellow Bourbon\n0\n1.00\n8\n25\n0\n6\n0\ncountry_of_origin\nBourbon\n0\n1.00\n6\n28\n0\n11\n0\ncountry_of_origin\nCatuai\n0\n1.00\n6\n10\n0\n8\n0\ncountry_of_origin\nCaturra\n0\n1.00\n4\n10\n0\n13\n0\ncountry_of_origin\nHawaiian Kona\n0\n1.00\n22\n22\n0\n1\n0\ncountry_of_origin\nTypica\n0\n1.00\n4\n11\n0\n9\n0\ncountry_of_origin\nYellow Bourbon\n0\n1.00\n6\n6\n0\n2\n0\nprocessing_method\nBourbon\n3\n0.99\n5\n25\n0\n5\n0\nprocessing_method\nCatuai\n0\n1.00\n12\n25\n0\n4\n0\nprocessing_method\nCaturra\n7\n0.97\n5\n25\n0\n5\n0\nprocessing_method\nHawaiian Kona\n0\n1.00\n12\n13\n0\n2\n0\nprocessing_method\nTypica\n3\n0.99\n5\n25\n0\n5\n0\nprocessing_method\nYellow Bourbon\n3\n0.91\n5\n25\n0\n5\n0\ncolor\nBourbon\n12\n0.95\n4\n12\n0\n4\n0\ncolor\nCatuai\n2\n0.97\n4\n12\n0\n4\n0\ncolor\nCaturra\n21\n0.92\n4\n12\n0\n4\n0\ncolor\nHawaiian Kona\n6\n0.86\n5\n12\n0\n3\n0\ncolor\nTypica\n27\n0.87\n4\n12\n0\n4\n0\ncolor\nYellow Bourbon\n3\n0.91\n4\n12\n0\n4\n0\nVariable type: numeric\nskim_variable\nvariety\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\ntotal_cup_points\nBourbon\n0\n1\n81.95\n2.54\n68.33\n81.00\n82.33\n83.40\n89.75\n▁▁▂▇▁\ntotal_cup_points\nCatuai\n0\n1\n81.30\n3.91\n59.83\n81.17\n81.88\n83.06\n85.83\n▁▁▁▁▇\ntotal_cup_points\nCaturra\n0\n1\n82.44\n5.59\n0.00\n82.00\n83.12\n83.77\n87.17\n▁▁▁▁▇\ntotal_cup_points\nHawaiian Kona\n0\n1\n81.58\n3.10\n72.58\n80.31\n82.62\n83.33\n86.25\n▁▁▃▇▃\ntotal_cup_points\nTypica\n0\n1\n81.02\n2.59\n67.92\n79.79\n81.50\n82.67\n85.33\n▁▁▁▇▇\ntotal_cup_points\nYellow Bourbon\n0\n1\n82.43\n1.58\n78.00\n81.54\n82.42\n83.16\n86.17\n▁▃▇▅▁\naroma\nBourbon\n0\n1\n7.56\n0.32\n6.17\n7.42\n7.58\n7.67\n8.50\n▁▁▆▇▁\naroma\nCatuai\n0\n1\n7.49\n0.30\n6.67\n7.33\n7.50\n7.67\n8.50\n▁▃▇▂▁\naroma\nCaturra\n0\n1\n7.58\n0.56\n0.00\n7.50\n7.67\n7.75\n8.25\n▁▁▁▁▇\naroma\nHawaiian Kona\n0\n1\n7.51\n0.24\n6.92\n7.33\n7.50\n7.67\n8.08\n▁▅▇▃▂\naroma\nTypica\n0\n1\n7.47\n0.28\n6.67\n7.25\n7.50\n7.67\n8.17\n▁▅▇▇▂\naroma\nYellow Bourbon\n0\n1\n7.50\n0.33\n6.92\n7.25\n7.42\n7.62\n8.42\n▂▇▃▂▂\nflavor\nBourbon\n0\n1\n7.50\n0.36\n6.08\n7.33\n7.50\n7.67\n8.50\n▁▁▇▇▁\nflavor\nCatuai\n0\n1\n7.43\n0.34\n6.17\n7.33\n7.50\n7.58\n8.00\n▁▁▂▇▃\nflavor\nCaturra\n0\n1\n7.53\n0.54\n0.00\n7.42\n7.58\n7.75\n8.33\n▁▁▁▁▇\nflavor\nHawaiian Kona\n0\n1\n7.53\n0.29\n6.92\n7.33\n7.50\n7.75\n8.17\n▃▆▆▇▁\nflavor\nTypica\n0\n1\n7.38\n0.34\n6.33\n7.17\n7.42\n7.58\n8.17\n▁▃▇▇▂\nflavor\nYellow Bourbon\n0\n1\n7.54\n0.24\n7.00\n7.38\n7.58\n7.67\n8.00\n▂▃▇▃▃\naftertaste\nBourbon\n0\n1\n7.32\n0.36\n6.17\n7.17\n7.33\n7.50\n8.42\n▁▂▇▂▁\naftertaste\nCatuai\n0\n1\n7.31\n0.35\n6.17\n7.17\n7.33\n7.50\n8.00\n▁▁▅▇▂\naftertaste\nCaturra\n0\n1\n7.44\n0.54\n0.00\n7.33\n7.50\n7.67\n8.08\n▁▁▁▁▇\naftertaste\nHawaiian Kona\n0\n1\n7.47\n0.31\n6.83\n7.33\n7.50\n7.69\n8.00\n▃▂▇▇▅\naftertaste\nTypica\n0\n1\n7.28\n0.33\n6.17\n7.08\n7.33\n7.50\n8.00\n▁▂▇▇▃\naftertaste\nYellow Bourbon\n0\n1\n7.40\n0.27\n6.83\n7.25\n7.42\n7.58\n8.00\n▂▃▇▅▁\nacidity\nBourbon\n0\n1\n7.55\n0.27\n6.83\n7.42\n7.54\n7.67\n8.42\n▁▅▇▂▁\nacidity\nCatuai\n0\n1\n7.49\n0.32\n6.50\n7.33\n7.50\n7.67\n8.33\n▁▂▇▃▁\nacidity\nCaturra\n0\n1\n7.52\n0.57\n0.00\n7.33\n7.58\n7.75\n8.25\n▁▁▁▁▇\nacidity\nHawaiian Kona\n0\n1\n7.59\n0.27\n6.92\n7.40\n7.58\n7.83\n8.00\n▁▅▂▇▇\nacidity\nTypica\n0\n1\n7.40\n0.27\n6.67\n7.25\n7.42\n7.58\n8.33\n▂▇▇▃▁\nacidity\nYellow Bourbon\n0\n1\n7.47\n0.23\n6.92\n7.33\n7.50\n7.67\n8.00\n▂▇▆▇▁\nbody\nBourbon\n0\n1\n7.50\n0.27\n6.33\n7.33\n7.50\n7.67\n8.33\n▁▁▇▆▁\nbody\nCatuai\n0\n1\n7.41\n0.28\n6.50\n7.27\n7.42\n7.58\n7.92\n▁▂▇▇▅\nbody\nCaturra\n0\n1\n7.54\n0.54\n0.00\n7.48\n7.58\n7.75\n8.17\n▁▁▁▁▇\nbody\nHawaiian Kona\n0\n1\n7.61\n0.26\n6.92\n7.42\n7.67\n7.83\n8.08\n▁▂▇▇▅\nbody\nTypica\n0\n1\n7.40\n0.25\n6.75\n7.25\n7.42\n7.50\n8.33\n▁▇▇▂▁\nbody\nYellow Bourbon\n0\n1\n7.57\n0.27\n6.92\n7.42\n7.50\n7.71\n8.33\n▁▅▇▁▁\nbalance\nBourbon\n0\n1\n7.47\n0.32\n6.50\n7.33\n7.50\n7.67\n8.42\n▁▃▇▅▁\nbalance\nCatuai\n0\n1\n7.40\n0.37\n6.17\n7.25\n7.42\n7.67\n8.00\n▁▁▃▇▆\nbalance\nCaturra\n0\n1\n7.57\n0.58\n0.00\n7.42\n7.58\n7.75\n8.58\n▁▁▁▁▇\nbalance\nHawaiian Kona\n0\n1\n7.64\n0.34\n6.83\n7.42\n7.67\n7.92\n8.25\n▁▃▇▅▅\nbalance\nTypica\n0\n1\n7.35\n0.31\n6.58\n7.17\n7.33\n7.50\n8.25\n▁▃▇▂▁\nbalance\nYellow Bourbon\n0\n1\n7.57\n0.24\n7.17\n7.42\n7.50\n7.67\n8.17\n▅▇▇▃▂\nuniformity\nBourbon\n0\n1\n9.87\n0.39\n8.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nuniformity\nCatuai\n0\n1\n9.85\n0.51\n8.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nuniformity\nCaturra\n0\n1\n9.89\n0.72\n0.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nuniformity\nHawaiian Kona\n0\n1\n9.47\n0.81\n6.67\n9.33\n10.00\n10.00\n10.00\n▁▁▁▅▇\nuniformity\nTypica\n0\n1\n9.75\n0.60\n6.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nuniformity\nYellow Bourbon\n0\n1\n9.83\n0.59\n6.67\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\nBourbon\n0\n1\n9.85\n0.80\n0.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\nCatuai\n0\n1\n9.77\n1.10\n1.33\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\nCaturra\n0\n1\n9.89\n0.79\n0.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\nHawaiian Kona\n0\n1\n9.53\n0.94\n6.67\n9.33\n10.00\n10.00\n10.00\n▁▁▁▂▇\nclean_cup\nTypica\n0\n1\n9.76\n0.89\n2.67\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\nYellow Bourbon\n0\n1\n9.96\n0.16\n9.33\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\nBourbon\n0\n1\n9.91\n0.35\n6.67\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\nCatuai\n0\n1\n9.75\n1.13\n1.33\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\nCaturra\n0\n1\n9.92\n0.71\n0.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\nHawaiian Kona\n0\n1\n9.67\n0.75\n6.67\n9.33\n10.00\n10.00\n10.00\n▁▁▁▂▇\nsweetness\nTypica\n0\n1\n9.94\n0.36\n6.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\nYellow Bourbon\n0\n1\n9.98\n0.11\n9.33\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\ncupper_points\nBourbon\n0\n1\n7.43\n0.41\n6.00\n7.25\n7.50\n7.67\n9.25\n▁▃▇▁▁\ncupper_points\nCatuai\n0\n1\n7.41\n0.36\n6.33\n7.27\n7.42\n7.58\n8.17\n▁▁▇▆▂\ncupper_points\nCaturra\n0\n1\n7.55\n0.57\n0.00\n7.42\n7.58\n7.75\n8.50\n▁▁▁▁▇\ncupper_points\nHawaiian Kona\n0\n1\n7.56\n0.35\n6.92\n7.33\n7.50\n7.83\n8.33\n▅▇▇▆▂\ncupper_points\nTypica\n0\n1\n7.30\n0.39\n5.25\n7.00\n7.33\n7.58\n8.17\n▁▁▃▇▃\ncupper_points\nYellow Bourbon\n0\n1\n7.60\n0.31\n7.00\n7.38\n7.58\n7.75\n8.25\n▅▇▇▃▃\n\n\n\ndata_variety %>% \n  select(variety, total_cup_points) %>% \n  filter(total_cup_points != 0) %>% \n  ggplot(aes(fct_reorder(variety, total_cup_points), total_cup_points)) +\n  geom_boxplot(aes(col = variety), show.legend = F) +\n  labs(title = \"Comparison of Total Cup Points across top 6 varieties \\n(by count)\",\n       subtitle = \"Caturra has the higest mean Total Cup Score. Catuai had a wider distribution of scores.\",\n       x = NULL,\n       y = \"Total Cup Points\",\n       caption = \"Source: Coffee Quality Institute\") +\n  geom_jitter(aes(col = variety), alpha = 0.2, show.legend = F) +\n  scale_color_few() +\n  coord_flip()  +\n  theme_few()\n\n\n\n\n\n\ndot_plot_variety <- data_variety %>% \n  filter(total_cup_points != 0) %>% \n  select(variety, aroma:cupper_points) %>% \n  group_by(variety) %>% \n  summarise(across(c(aroma:cupper_points), mean)) %>% \n  pivot_longer(cols = c(aroma:cupper_points),\n               names_to = \"parameters\",\n               values_to = \"score\") %>% \n  ggplot(aes(x = fct_reorder(factor(variety), score), y = score, label = round(score, 1))) +\n  geom_point(stat = \"identity\", aes(col = factor(variety)), size = 8) +\n  geom_text(col = \"black\", size = 4) +\n  facet_wrap(parameters~., scales = \"free\", ncol = 4) +\n  labs(title = \"Breakdown of scoring criteria for top 5 coffee (by count)\",\n       subtitle = \"Scores were quite close for all categories, within +/- 0.2. \nMain areas of differences were in balance, clean_cup, cupper_points, sweetness, uniformity\",\n       caption = \"Source: Coffee Quality Institute\",\n       x = \"Variety\",\n       y = \"Score\") +\n  coord_flip() +\n  theme_few() +\n  theme(legend.position = \"none\",\n        axis.title = element_text(face = \"bold\"))\n\ndot_plot_variety\n\n\n\n\nCanturra had an edge over Hawaiian Kona for aroma, clean_cup, sweetness and uniformity, resulting in higher mean total_cup_points. What is Canturra coffee? It’s actually a mutated type of Bourbon coffee that is known for great flavor.\nAs mentioned at the beginning, most of the coffee had very high scores in this dataset. Hence, the plots only show a snapshot of the flavor profiles of the scored coffee, but not all the coffee.\nMain Learning Pointers\nI am really glad to have found this #tidytuesday hashtag, which allows me to practice on readily available datasets and understand how different people in the community approach exploratory data analysis! I am really amazed that there is a dedicated package for loading the dataset with convenience, and this dataset even comes with a data dictionary to understand what each variable means. The R community is really committed to sharing and becoming better, together.\nThe process of EDA is about getting to know your dataset, through asking questions, which are to be answered by carrying out data transformations and creating data visualizations. One question often leads to another, and EDA is a repetitive process until you finish getting to know your data. There were several aspects that I did not look at, such as the effect of altitude, and the grading dates. I may have concentrated too much on the sensory aspect of coffee since that was the more familiar aspect to me, and should have also looked at geographical region and coffee varieties. As an initial learning exercise, I sharpened my focus and concentrated on the effect of species, variety, processing methods, country/owners.\nAs the total cup points is a summation of the scores for attributes such as aroma, flavor, etc, I think it is hard to do classification based on these scores. I would prefer to have physicochemical data as well so that differentiation is more objective and to better countercheck the sensorial data. However, this may be a personal bias as I work in the analytical chemistry field. :)\nI think coffee is really complex. You can have a poorer grade (Robusta), but the roasting process plays a very important role in flavor development. You can have a very good variety, but the processing method may spoil/enhance its flavor profile. You can have a very good farm/owner, but maybe the year of harvest was particularly good or bad. Hence, it is really important to consider all (both familar and unfamilar) aspects when carrying out data analysis, and this is one area I need to improve on.\nCoding wise, I got a chance to practice ggplots, data transformation, filtering and selecting rows and columns, as well as calculating means efficiently by using summarise(across, var, mean). I also managed to create new classifications using ifelse, and used fct_reorder to make my plot better. I like to use theme_clean and scale_color_few for my plots, making aesthetically pleasant plots are a breeze as compared to using Microsoft Excel.\nReferences\nhttps://perfectdailygrind.com/2016/07/washed-natural-honey-coffee-processing-101/ https://www.baristainstitute.com/blog/jori-korhonen/january-2020/coffee-processing-methods-drying-washing-or-honey https://www.coffeechemistry.com/cupping-fundamentals https://www.data-to-viz.com/caveat/spider.html https://www.javapresse.com/blogs/buying-coffee/beginners-guide-coffee-varieties\n\n\n\n",
    "preview": "posts/20210209_tidy_tuesday_coffee_ratings/Tidy-Tuesday---Coffee-Ratings_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-02-12T00:21:12+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210202_Clustering wine/",
    "title": "Clustering Analysis on Wine Dataset",
    "description": "A continuation from PCA analysis of wine dataset: k-means clustering and hierarchical clustering",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-02-02",
    "categories": [],
    "contents": "\nSummary\nPCA is used as an exploratory data analysis tool, and may be used for feature engineering and/or clustering. This is a continuation of clustering analysis on the wines dataset in the kohonen package, in which I carry out k-means clustering using the tidymodels framework, as well as hierarchical clustering using factoextra pacage.\nWorkflow\nImport data\nExploratory data analysis\nskim\nggcorr\nggpairs\nCheck assumptions on whether PCA can be carried out\nKMO\nBartlett\nCarry out PCA using tidymodels workflow\nAlways use only continuous variables, ensure that there are no missing data. Determine the number of components using eigenvalues, scree plots and parallel analysis.\nrecipe : preprocess the data (missing values, center and scale, ensuring that variables are continuous)\nprep : evaluate the data\nbake : get the PCA Scores results\nvisualize\ncommunicate results: show the scree plot, PCA loadings, variance explained by each component, loadings and score plot.\nThe loading shows the linear combinations of the original variables - ie the new dimension.\nThe scores show the coordinates of the individual wine samples in the new low-dimensional space.\nUse the loadings to carry out k-means clustering and hierarchical clustering.\nLoading packages\n\n\nlibrary(pacman)\np_load(corrr, GGally, tidymodels, tidytext, tidyverse, psych,\n       skimr, gridExtra, kohonen, janitor, learntidymodels, kohonen,\n       cluster, factoextra)\n\n\n\nImport\nThis dataset is from the kohonen package. It contains 177 rows and 13 columns.\nThese data are the results of chemical analyses of wines grown in the same region in Italy (Piedmont) but derived from three different cultivars: Nebbiolo, Barberas and Grignolino grapes. The wine from the Nebbiolo grape is called Barolo. The data contain the quantities of several constituents found in each of the three types of wines, as well as some spectroscopic variables.\nPCA analysis was performed earlier, and k-means clustering and hierarchical clustering analysis (HCA) will be built upon the PCA loadings.\n\n\ndata(wines)\n\nwines <- as.data.frame(wines) %>% \n  janitor::clean_names() %>%  # require data.frame\n  as_tibble() \n \nglimpse(wines) # does not contain the types of wine (Y variable)\n\n\nRows: 177\nColumns: 13\n$ alcohol          <dbl> 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, …\n$ malic_acid       <dbl> 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, …\n$ ash              <dbl> 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, …\n$ ash_alkalinity   <dbl> 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, …\n$ magnesium        <dbl> 100, 101, 113, 118, 112, 96, 121, 97, 98, …\n$ tot_phenols      <dbl> 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, …\n$ flavonoids       <dbl> 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, …\n$ non_flav_phenols <dbl> 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, …\n$ proanth          <dbl> 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, …\n$ col_int          <dbl> 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, …\n$ col_hue          <dbl> 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, …\n$ od_ratio         <dbl> 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, …\n$ proline          <dbl> 1050, 1185, 1480, 735, 1450, 1290, 1295, 1…\n\nEDA\nRefer to the post for PCA of wine analysis\nTidymodels (PCA)\nRecipe\nThe dataset did not include the y variable (type of wine), so the update_role() function will be omitted.\nstep_normalize() combines step_center() and step_scale()\nNote that step_pca is the second step –> will need to retrieve the PCA results from the second list later.\n\n\nwines_recipe <- recipe(~ ., data = wines) %>% \n  # update_role(vintages, new_role = \"id\") %>%  # skipped\n  # step_naomit(all_predictors()) %>% \n  step_normalize(all_predictors()) %>% \n  step_pca(all_predictors(), id = \"pca\")\n\nwines_recipe # 13 predictors\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n predictor         13\n\nOperations:\n\nCentering and scaling for all_predictors()\nNo PCA components were extracted.\n\nPreparation\n\n\nwines_prep <- prep(wines_recipe)\n\nwines_prep # trained\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n predictor         13\n\nTraining data contained 177 data points and no missing data.\n\nOperations:\n\nCentering and scaling for alcohol, malic_acid, ... [trained]\nPCA extraction with alcohol, malic_acid, ... [trained]\n\ntidy_pca_loadings <- wines_prep %>% \n  tidy(id = \"pca\")\n\ntidy_pca_loadings # values here are the loading\n\n\n# A tibble: 169 x 4\n   terms               value component id   \n   <chr>               <dbl> <chr>     <chr>\n 1 alcohol          -0.138   PC1       pca  \n 2 malic_acid        0.246   PC1       pca  \n 3 ash               0.00432 PC1       pca  \n 4 ash_alkalinity    0.237   PC1       pca  \n 5 magnesium        -0.135   PC1       pca  \n 6 tot_phenols      -0.396   PC1       pca  \n 7 flavonoids       -0.424   PC1       pca  \n 8 non_flav_phenols  0.299   PC1       pca  \n 9 proanth          -0.313   PC1       pca  \n10 col_int           0.0933  PC1       pca  \n# … with 159 more rows\n\nBake\n\n\nwines_bake <- bake(wines_prep, wines)\nwines_bake  # has the PCA SCORES to run HCA and k-means clustering\n\n\n# A tibble: 177 x 5\n     PC1    PC2    PC3      PC4     PC5\n   <dbl>  <dbl>  <dbl>    <dbl>   <dbl>\n 1 -2.22 -0.301 -2.03   0.281   -0.259 \n 2 -2.52  1.06   0.974 -0.734   -0.198 \n 3 -3.74  2.80  -0.180 -0.575   -0.257 \n 4 -1.02  0.886  2.02   0.432    0.274 \n 5 -3.04  2.16  -0.637  0.486   -0.630 \n 6 -2.45  1.20  -0.985  0.00466 -1.03  \n 7 -2.06  1.64   0.143  1.20     0.0105\n 8 -2.51  0.958 -1.78  -0.104   -0.871 \n 9 -2.76  0.822 -0.986 -0.374   -0.437 \n10 -3.48  1.35  -0.428 -0.0399  -0.316 \n# … with 167 more rows\n\nCheck number of PC\nOnly the scree plot is showed below. Refer to PCA analysis of wine for other options in determining number of PCs.\n\n\n# b. Scree plot/Variance plot\n\nwines_prep %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms ==  \"percent variance\") %>% \n  ggplot(aes(x = component, y = value)) +\n  geom_point(size = 2) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = 1:13) +\n  labs(title = \"% Variance explained\",\n       y = \"% total variance\",\n       x = \"PC\",\n       caption = \"Source: Wines dataset from kohonen package\") +\n  theme_classic() +\n  theme(axis.title = element_text(face = \"bold\", size = 12),\n        axis.text = element_text(size = 10),\n        plot.title = element_text(size = 14, face = \"bold\"))  # 2 or 3\n\n\n\n\nVisualize\nLoadings plot\n\n\nplot_loadings <- tidy_pca_loadings %>% \n  filter(component %in% c(\"PC1\", \"PC2\", \"PC3\")) %>% \n  mutate(terms = tidytext::reorder_within(terms, \n                                          abs(value), \n                                          component)) %>% \n  ggplot(aes(abs(value), terms, fill = value>0)) +\n  geom_col() +\n  facet_wrap( ~component, scales = \"free_y\") +\n  scale_y_reordered() + # appends ___ and then the facet at the end of each string\n  scale_fill_manual(values = c(\"deepskyblue4\", \"darkorange\")) +\n  labs( x = \"absolute value of contribution\",\n        y = NULL,\n        fill = \"Positive?\",\n        title = \"PCA Loadings Plot\",\n        subtitle = \"Number of PC should be 3, compare the pos and the neg\",\n        caption = \"Source: ChemometricswithR\") +\n  theme_minimal() +\n  theme(title = element_text(size = 24, face = \"bold\"),\n        axis.text = element_text(size = 16),\n        axis.title = element_text(size = 20))\n\n\nplot_loadings\n\n\n\n# PC1: flavonoids, tot_phenols, od_ratio, proanthocyanidins, col_hue, 36%\n# PC2: col_int, alcohol, proline, ash, magnesium; 19.2%\n# PC3: ash, ash_alkalinity, non_flav phenols; 11.2%\n\n\n\nLoadings only\n\n\n# define arrow style\narrow_style <- arrow(angle = 30,\n                     length = unit(0.2, \"inches\"),\n                     type = \"closed\")\n\n# get pca loadings into wider format\npca_loadings_wider <- tidy_pca_loadings%>% \n  pivot_wider(names_from = component, id_cols = terms)\n\n\npca_loadings_only <- pca_loadings_wider %>% \n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_segment(aes(xend = PC1, yend = PC2),\n               x = 0, \n               y = 0,\n               arrow = arrow_style) +\n  ggrepel::geom_text_repel(aes(x = PC1, y = PC2, label = terms),\n            hjust = 0, \n            vjust = 1,\n            size = 5,\n            color = \"red\") +\n  labs(title = \"Loadings on PCs 1 and 2 for normalized data\") +\n  theme_classic()\n\n\n\nScores plot\n\n\n# Scores plot #####\n# PCA SCORES are in bake\npc1pc2_scores_plot <- wines_bake %>% \n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(color = vintages, shape = vintages), \n             alpha = 0.8, size = 2) +\n  scale_color_manual(values = c(\"deepskyblue4\", \"darkorange\", \"purple\")) +\n  labs(title = \"Scores on PCs 1 and 2 for normalized data\",\n       x = \"PC1 (36%)\",\n       y = \"PC2 (19.2%)\") +\n  theme_classic() +\n  theme(legend.position = \"none\") \n\n\n\nFinalised plots\n\n\ngrid.arrange(pc1pc2_scores_plot, pca_loadings_only, ncol = 2)\n\n\n\n\nk-means clustering\nThe PCA scores will be used for clustering analysis\n\n\nwines_bake\n\n\n# A tibble: 177 x 5\n     PC1    PC2    PC3      PC4     PC5\n   <dbl>  <dbl>  <dbl>    <dbl>   <dbl>\n 1 -2.22 -0.301 -2.03   0.281   -0.259 \n 2 -2.52  1.06   0.974 -0.734   -0.198 \n 3 -3.74  2.80  -0.180 -0.575   -0.257 \n 4 -1.02  0.886  2.02   0.432    0.274 \n 5 -3.04  2.16  -0.637  0.486   -0.630 \n 6 -2.45  1.20  -0.985  0.00466 -1.03  \n 7 -2.06  1.64   0.143  1.20     0.0105\n 8 -2.51  0.958 -1.78  -0.104   -0.871 \n 9 -2.76  0.822 -0.986 -0.374   -0.437 \n10 -3.48  1.35  -0.428 -0.0399  -0.316 \n# … with 167 more rows\n\nNumber of clusters?\nThere are 3 common ways for determining the number of clusters:\ngap statistic method\nwithin sum of square method\nsilhouette method\nLet us look at all three of them.\nGap Statistic Method\n\n\ngap_statistic <- cluster::clusGap(wines_bake,\n                                  FUN = kmeans,\n                                  nstart = 50, \n                                  K.max = 10, # max number of clusters\n                                  B = 1000) # bootstrap\n\nfactoextra::fviz_gap_stat(gap_statistic) # theoretically should have only 3 clusters\n\n\n\n\nWithin Sum of Square Method\n\n\nfviz_nbclust(wines_bake,\n             kmeans,\n             method = \"wss\") # this suggests 3 clusters, in line with theory\n\n\n\n\nSilhouette Method\n\n\nfviz_nbclust(wines_bake,\n             FUN = hcut,\n             method = \"silhouette\") # this suggests 3 clusters\n\n\n\n\nAll three methods agree that there should be 3 clusters. This may not always be the case. In any case, we know that there are 3 different types of wine in the dataset.\nTidymodels workflow for k-means clustering\n\n\n# exploring different k numbers #####\nkclusts_explore <- tibble(k = 1:10) %>% \n  mutate(kclust = purrr::map(k, ~kmeans(wines_bake, .x)),\n         tidied = purrr::map(kclust, tidy),\n         glanced = purrr::map(kclust, glance),\n         augmented = purrr::map(kclust, augment, wines_bake))\n\nkclusts_explore\n\n\n# A tibble: 10 x 5\n       k kclust   tidied            glanced          augmented        \n   <int> <list>   <list>            <list>           <list>           \n 1     1 <kmeans> <tibble [1 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 2     2 <kmeans> <tibble [2 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 3     3 <kmeans> <tibble [3 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 4     4 <kmeans> <tibble [4 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 5     5 <kmeans> <tibble [5 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 6     6 <kmeans> <tibble [6 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 7     7 <kmeans> <tibble [7 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 8     8 <kmeans> <tibble [8 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 9     9 <kmeans> <tibble [9 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n10    10 <kmeans> <tibble [10 × 8]> <tibble [1 × 4]> <tibble [177 × 6…\n\n# turn this into 3 separate datasets, each representing a\n# different type of data\n\n#\nclusters <- kclusts_explore %>% \n  unnest(cols = c(tidied))\n\nclusters\n\n\n# A tibble: 55 x 12\n       k kclust       PC1       PC2       PC3       PC4       PC5\n   <int> <list>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n 1     1 <kmea… -1.03e-15 -2.78e-16 -3.22e-16 -5.39e-17  1.76e-16\n 2     2 <kmea…  1.91e+ 0 -8.65e- 2 -1.73e- 3  7.28e- 2 -1.41e- 2\n 3     2 <kmea… -1.89e+ 0  8.56e- 2  1.71e- 3 -7.20e- 2  1.39e- 2\n 4     3 <kmea…  2.71e+ 0  1.10e+ 0 -2.35e- 1 -6.17e- 2  7.64e- 2\n 5     3 <kmea…  4.43e- 4 -1.76e+ 0  1.85e- 1 -7.36e- 2  7.54e- 2\n 6     3 <kmea… -2.26e+ 0  9.55e- 1 -5.83e- 4  1.30e- 1 -1.44e- 1\n 7     4 <kmea…  2.90e- 1 -1.77e+ 0 -8.54e- 1  4.55e- 1  1.35e- 1\n 8     4 <kmea…  2.76e+ 0  1.21e+ 0 -1.52e- 1 -1.11e- 1  5.10e- 2\n 9     4 <kmea… -2.39e+ 0  1.04e+ 0 -2.57e- 1  1.29e- 1 -2.19e- 1\n10     4 <kmea… -3.41e- 1 -1.30e+ 0  1.28e+ 0 -4.39e- 1  1.17e- 1\n# … with 45 more rows, and 5 more variables: size <int>,\n#   withinss <dbl>, cluster <fct>, glanced <list>, augmented <list>\n\n#\nassignments <- kclusts_explore %>% \n  unnest(cols = c(augmented))\n\nassignments  # can be used to plot, with each point colored according to predicted cluster\n\n\n# A tibble: 1,770 x 10\n       k kclust tidied glanced   PC1    PC2    PC3      PC4     PC5\n   <int> <list> <list> <list>  <dbl>  <dbl>  <dbl>    <dbl>   <dbl>\n 1     1 <kmea… <tibb… <tibbl… -2.22 -0.301 -2.03   0.281   -0.259 \n 2     1 <kmea… <tibb… <tibbl… -2.52  1.06   0.974 -0.734   -0.198 \n 3     1 <kmea… <tibb… <tibbl… -3.74  2.80  -0.180 -0.575   -0.257 \n 4     1 <kmea… <tibb… <tibbl… -1.02  0.886  2.02   0.432    0.274 \n 5     1 <kmea… <tibb… <tibbl… -3.04  2.16  -0.637  0.486   -0.630 \n 6     1 <kmea… <tibb… <tibbl… -2.45  1.20  -0.985  0.00466 -1.03  \n 7     1 <kmea… <tibb… <tibbl… -2.06  1.64   0.143  1.20     0.0105\n 8     1 <kmea… <tibb… <tibbl… -2.51  0.958 -1.78  -0.104   -0.871 \n 9     1 <kmea… <tibb… <tibbl… -2.76  0.822 -0.986 -0.374   -0.437 \n10     1 <kmea… <tibb… <tibbl… -3.48  1.35  -0.428 -0.0399  -0.316 \n# … with 1,760 more rows, and 1 more variable: .cluster <fct>\n\n#\nclusterings <- kclusts_explore %>% \n  unnest(cols = c(glanced))\n\nclusterings\n\n\n# A tibble: 10 x 8\n       k kclust  tidied  totss tot.withinss betweenss  iter augmented \n   <int> <list>  <list>  <dbl>        <dbl>     <dbl> <int> <list>    \n 1     1 <kmean… <tibbl… 1834.        1834. -2.50e-12     1 <tibble […\n 2     2 <kmean… <tibbl… 1834.        1192.  6.42e+ 2     1 <tibble […\n 3     3 <kmean… <tibbl… 1834.         820.  1.01e+ 3     2 <tibble […\n 4     4 <kmean… <tibbl… 1834.         730.  1.10e+ 3     3 <tibble […\n 5     5 <kmean… <tibbl… 1834.         659.  1.18e+ 3     3 <tibble […\n 6     6 <kmean… <tibbl… 1834.         603.  1.23e+ 3     4 <tibble […\n 7     7 <kmean… <tibbl… 1834.         562.  1.27e+ 3     3 <tibble […\n 8     8 <kmean… <tibbl… 1834.         545.  1.29e+ 3     5 <tibble […\n 9     9 <kmean… <tibbl… 1834.         471.  1.36e+ 3     3 <tibble […\n10    10 <kmean… <tibbl… 1834.         465.  1.37e+ 3     4 <tibble […\n\n#  visualize\n\n# number of clusters\nclusterings %>% # from glance\n  ggplot(aes(k, tot.withinss)) + # total within cluster sum of squares, keep low\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 1:10) +\n  labs(title = \"Plot of Total Within Sum of Squares for different number of clusters\",\n       subtitle = \"Additional clusters beyond k = 3 have little value\") +\n  theme_classic()\n\n\n\n# how datapoints are separated\nglimpse(assignments)\n\n\nRows: 1,770\nColumns: 10\n$ k        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ kclust   <list> [<1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ tidied   <list> [<tbl_df[1 x 8]>, <tbl_df[1 x 8]>, <tbl_df[1 x 8]…\n$ glanced  <list> [<tbl_df[1 x 4]>, <tbl_df[1 x 4]>, <tbl_df[1 x 4]…\n$ PC1      <dbl> -2.223934, -2.524760, -3.744056, -1.017245, -3.040…\n$ PC2      <dbl> -0.30145757, 1.05925179, 2.79737289, 0.88586726, 2…\n$ PC3      <dbl> -2.0271695, 0.9739613, -0.1798599, 2.0181445, -0.6…\n$ PC4      <dbl> 0.281108579, -0.733645703, -0.575492236, 0.4315681…\n$ PC5      <dbl> -0.25880549, -0.19804048, -0.25714173, 0.27445613,…\n$ .cluster <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\nassignments %>% # from augment\n  ggplot(aes(x = PC1, y = PC2)) + # use PCA data\n  geom_point(aes(color = .cluster), size = 2, alpha = 0.8) +\n  facet_wrap( ~ k) +\n  # to see the center of the clusters\n  geom_point(data = clusters, size = 9, shape  = \"x\") +\n  labs(x = \"PC1 (36% variance)\",\n       y = \"PC2 (19.2% variance\",\n       title = \"Visualization of k-means clustering\",\n       subtitle = \"Optimal k = 3\",\n       caption = \"Source: Wines dataset from kohonen package\") +\n  theme_minimal()\n\n\n\n\nHierarchical Clustering Analysis\n\n\nwines_HC <- wines_bake %>% \n    dist(.,method = \"euclidean\") %>% \n    hclust(., method = \"ward.D2\")\n\n# 3 clusters:\nfviz_dend(wines_HC,\n          k = 3,\n          rect = T,\n          rect_border = \"jco\",\n          rect_fill = T)\n\n\n\n\nLearning pointers:\nInitially, I was stuck at the visualization part for k-means clustering as I didn’t know how to bring in my x and y-axis data. I had been using the original dataset all along, and was wondering why plots created using the factoextra::fviz_cluster() could report Dim 1 for x axis and Dim 2 for y axis. I finally had the eureka moment when I realised I should use the PCA scores from the bake step earlier.\nI really like the tidymodels way of allowing for visualizing how the clusters are separated when different values of k are used. The functions augment, tidy and glance were very efficient in reporting the results for k-means clustering. Previously I only used tidy and glance for regression, and I didn’t know they could be extended to cluster analysis as well.\nLastly, I find dendrograms very aesthetically intuitive and I like how the colors and types of dendrograms could be customised. However, the assumption is that there must be some structure in the data in the first place, otherwise HCA would give very misleading results.\nReferences\nhttps://rdrr.io/github/tidymodels/learntidymodels/f/inst/tutorials/pca_recipes/pca_recipes.Rmd\nhttps://allisonhorst.github.io/palmerpenguins/articles/articles/pca.html\nhttps://www.ibm.com/support/knowledgecenter/en/SSLVMB_subs/statistics_casestudies_project_ddita/spss/tutorials/fac_telco_kmo_01.html\nhttps://www.tidymodels.org/learn/statistics/k-means/\nhttps://www.r-bloggers.com/2019/07/use-the-k-means-clustering-luke/\nhttps://statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/\nhttps://agroninfotech.blogspot.com/2020/06/visualizing-clusters-in-r-hierarchical.html\n\n\n\n",
    "preview": "posts/20210202_Clustering wine/Clustering-wine_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-02-03T00:24:11+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210130_Volatiles_tomato/",
    "title": "Volatile Compounds in Tomato and Tomato Products",
    "description": "Scraping information from journal article",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-02-01",
    "categories": [],
    "contents": "\nOverview of tomato volatiles\nTomato flavor is the result of interaction of aroma and taste, arising from the interplay of mixture of acids, sugars, amino acids, minerals and volatile compounds. Presence of sugar or organic acids alters taste panel perception of aroma descriptions of samples with the same concentration of volatile compounds.\nVolatile compounds may originate from different biosynthesis pathways (Buttery, Teranishi, and Ling 1987), (Baldwin et al. 1998), (Yilmaz 2001):\nDerived from amino acids: Amino acids are acted upon by transaminase enzymes and converted into alpha-keto acids; which then undergo decarboxylation to form aldehydes, which may be reduced to form ketones.\nDerived from carotenoids: eg 6-methyl-5-hepten-2-one and geranial\nDerived from lipid degradation by lipoxygenase: eg C6 volatiles\nDerived from peroxide lyase and alcohol dehydrogenase enzymes (ADH catalysed alcohol formation from aldehydes)\nMaillard reaction products: furans, pyrroles (Strecker degradation products), pyrazines. These are usually seen in thermally processed tomato products/flavors\nDerived from action of endogenous glycosidases (eg guaiacol, eugenol, methyl salicylate)\nThe amount and types of volatiles are also influenced by:\nTissue disruption\nRipening of fruit\nCultivar\nProcessing/Heating\nAddition of other ingredients (eg herbs and spices for pasta sauces)\nAim of this exercise:\nThe aim of this exercise was to scrape the table of approximately 400 compounds from the pdf, and to visualize them by chemical categories.\nThere is a very handy package, tabulizer, which allows for scraping of information from pdf articles. I tried out text scraping, and text cleaning, from the article (Petro‐Turza 1986)\nWorkflow\nImport data using tabulizer\nText cleaning using stringr package\nVisualize using ggplot2\nLoading packages\n\n\nlibrary(rJava)\nlibrary(tabulizer)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(stringi)\n\n\n\nImport & Tidy\nI am interested in scraping the table of approximately 400 volatiles from page 18 to page 28. A copy of the pdf is saved in my working directory. I used the locate_area() function to determine the coordinates for scraping, and then extracted the text using extract_text() function.\n\n\nfile <- \"1986-tomato volatiles.pdf\"\n# locate_areas(file)\n\np18 <- extract_text(file, pages = 18, area = list(c(115.5, 33.05, 641.923, 443.94)))\n\np19 <- extract_text(file, pages = 19, \n                    area = list(c(117.15636,  53.20364, 651.60000, 465.22545 )))\n\np20 <- extract_text(file, pages = 20, \n                    area = list(c(114.52364,  38.72364, 650.28364, 449.42909 )))\n\np21 <- extract_text(file, pages = 21, \n                    area = list(c(119.29273 , 48.89727, 656.76545, 457.90091 )))\n\np22 <- extract_text(file, pages = 22, \n                    area = list(c(113.20727,  30.32545, 643.70182, 466.04182)))\n\np23 <- extract_text(file, pages = 23, \n                    area = list(c(122.25273,  49.84636, 652.01455, 473.13000 )))\n\np24 <- extract_text(file, pages = 24, \n                    area = list(c(113.,  34, 645.01818, 430.50000 )))\n\np25 <- extract_text(file, pages = 25, \n                    area = list(c(119,  58 ,642.81273, 478.38818)))\n\np26 <- extract_text(file, pages = 26, \n                    area = list(c(114,  35, 668.17818, 443.44000)))\n\np27 <- extract_text(file, pages = 27, \n                    area = list(c(114.20727,  41.24545, 656.36364, 442.94000)))\n\np28 <- extract_text(file, pages = 28, \n                    area = list(c(115.52000,  33.86909, 404.32000, 469.69455 )))\n\ncombined <- tribble(~page, ~text,\n                    \"p18\", p18,\n                    \"p19\", p19,\n                    \"p20\", p20,\n                    \"p20\", p20,\n                    \"p21\", p21,\n                    \"p22\", p22,\n                    \"p23\", p23,\n                    \"p24\", p24,\n                    \"p25\", p25,\n                    \"p26\", p26,\n                    \"p27\", p27,\n                    \"p28\", p28) %>% \n  dplyr::mutate(text_2 = gsub(\"\\\\n\", \"; \", text),\n         text_3 = str_split(text_2, \"; \")) %>% # split by ; into new columns\n  unnest() \n\n\n\nI combined all the text that was extracted into a tibble. Then I replaced all the “” with “;” and then used str_split() to split the compounds into individual rows.\nFollowing which, I used a series to str_replace_all to clean up the text. The list of things to remove include:\ndigits that followed after chemical names\nseries of commars\ntext that mentioned unknown structure\nalternative synonoyms of chemical compounds that were located within square brackets\nodd chemical names\nAs I replaced the commars, some of the chemical names were also changed. For example, 2,6-dimethylpyrazine became 26-dimethylpyrazine. I had to change the names by looking for numbers 26, and replacing them as 2,6.\nI visually scanned through the list again and made changes where necessary, for eg, 2-formylpyiTole is actually 2-formylpyrrole.\n\n\ncleaned_text <- combined %>% \n  dplyr::select(text_3) # 521\n\n\ncleaned_text_b <- cleaned_text %>% \n  filter(!text_3 %in% c(\"(Continued)\", \"\")) %>%  # 503\n  filter(!is.na(text_3)) %>% \n  mutate(text_4 = str_replace_all(text_3, \"[0-9]{2,3}\", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\ , \", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\,,+\", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\(unknown structure\\\\)\", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\[[^\\\\]\\\\[]*]\", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\[[^\\\\[]*\", \"\"),\n         text_4 = str_replace_all(text_4, \" \\\\,\", \"\"),\n         text_4 = str_replace_all(text_4, \" \\\\/\", \"\"),\n         # replace last commar, dash\n         text_4 = stri_replace_last(text_4, fixed = \",\", \"\"),\n         text_4 = str_replace_all(text_4, \"PJ-dimethyl-ö-octen-l-ol]\", \"\"),\n         text_4 = str_replace_all(text_4, \"/\", \"\"),\n         \n         # correct for commar replacement\n         text_4 = str_replace_all(text_4, \"45\", \"4,5\"),\n         text_4 = str_replace_all(text_4, \"26\", \"2,6\"),\n         text_4 = str_replace_all(text_4, \"25\", \"2,5\"),\n         text_4 = str_replace_all(text_4, \"33\", \"3,3\"),\n         text_4 = str_replace_all(text_4, \"23\", \"2,3\"),\n         text_4 = str_replace_all(text_4, \"14\", \"1,4\"),\n         text_4 = str_replace_all(text_4, \"12\", \"1,2\"),\n         text_4 = str_replace_all(text_4, \"24\", \"2,4\"),\n         text_4 = str_replace_all(text_4, \"34\", \"3,4\"),\n         text_4 = str_replace_all(text_4, \"11\", \"1,1\"),\n         text_4 = str_replace_all(text_4, \"2E 4Z\", \"2E,4Z\"),\n         text_4 = str_replace_all(text_4, \"2E4E\", \"2E,4E\"),\n         text_4 = str_replace_all(text_4, \"2E4Z\", \"2E,4Z\"),\n         text_4 = str_replace_all(text_4, \"hy droxy\", \"hydroxy\"),\n         text_4 = str_replace_all(text_4, \"66\", \"6,6\"),\n         text_4 = str_replace_all(text_4, \"3E5E\", \"3E,5E\"),\n         \n         # further clean up\n         text_4 = str_replace_all(text_4, \"\\\\(unknownstructure\\\\)\", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\(unknown\", \"\"),\n         text_4 = str_replace_all(text_4, \"2-formylpyiTole \", \"2-formylpyrrole\"),\n         text_4 = str_replace_all(text_4, \"neraUcis-SJ-dimethyl\\\\^.o-octadienal]\", \n                                  \"neral\"),\n         text_4 = str_replace_all(text_4, \"2-methyl-l-propanol -\", \n                                  \"2-methyl-l-propanol\"),\n         text_4 = str_replace_all(text_4, \"propanal 9 09 39 49 5\",\n                                  \"propanal\"),\n         text_4 = str_replace_all(text_4, \"\\\\(methylthioH-propanol\",\n                                  \"\\\\(methylthio)-propanol\"),\n         text_4 = str_replace_all(text_4, \"\\\\(2,2,6-trimethyl-7-oxabicyclo\",\n                                  \"\"),\n         text_4 = str_replace_all(text_4, \"._.\", \"\"),\n         \n         # remove white space\n         text_4 = str_replace_all(text_4, \" \\\\s\", \"\"),\n         \n         # remove quotation marks\n         text_4 = str_remove_all(text_4, \"\\\"\")) \n\n\n\nOne final clean:\n\n\ntomato_cleaned <- cleaned_text_b$text_4 %>% \n  as.data.frame() %>% \n  unique()\n\nnames(tomato_cleaned) <- c(\"compounds\")\n\ntomato_cleaned <- print(tomato_cleaned, quote = FALSE) %>% \n  filter(!compounds == \" \",\n         !compounds == \"\") # remove quotation marks \n\n\n                                                 compounds\n1                                             HYDROCARBONS\n2                                                 heptane \n3                                                  octane \n4                                                  nonane \n5                                                  decane \n6                                                 undecane\n7                                             pentadecane \n8                                                ethylene \n9                                                camphene \n10                                                3-carene\n11                                               limonene \n12                                                myrcene \n13                                          a-phellandrene\n14                                          ß-phellandrene\n15                                               o-pinene \n16                                                3-pinene\n17                                               sabinene \n18                                            terpinolene \n19                                          triisobutylene\n20                                                benzene \n21                                                 toluene\n22                                            ethylbenzene\n23                                                 styrene\n24                                          propylbenzene \n25                                                  cumene\n26                                           butylbenzene \n27                                                o-xylene\n28                                               m-xylene \n29                                                p-xylene\n30                                 l-ethyl-4-methylbenzene\n31                                         diethylbenzene \n32                                                  cymene\n33                                               p-cymene \n34                                       trimethylbenzene \n35                                           hemimellitene\n36                                            pseudocumene\n37                                              mesitylene\n38                                                biphenyl\n39                                             naphtalene \n40                                                ALCOHOLS\n41                                               methanol \n42                                                 ethanol\n43                                                        \n44                                             1-propanol \n46                                              2-propanol\n47                                     2-methyl-l-propanol\n48                                                        \n49                                    2-methyl-2-propanol \n50                                           2-propen-l-ol\n51                                               1-butanol\n53                                              2-butanol \n54                                              buten-1-ol\n55                                     2-methyl-l-butanol \n56                                      3-methyl-l-butanol\n58                                   3-methyl-2-buten-l-ol\n59                                  2-methyl-3-buten-2-ol \n60                                         2,3-butanediol \n61                                             1-pentanol \n63                                             2-pentanol \n64                                             3-pentanol \n65                                      cis-3-penten-l-ol \n66                                          l-penten-3-ol \n67                                    2-methyl-l-pentanol \n68                                    3-methyl-l-pentanol \n69                                     2-methyl-2-pentanol\n70                                               1-hexanol\n72                                              2-hexanol \n73                                                 hexenol\n74                                            2-hexen-l-ol\n75                                        cis-2-hexen-l-ol\n76                                     trans-2-hexen-l-ol \n77                                           3-hexen-l-ol \n78                                       cis-3-hexen-l-ol \n80                                     trans-3-hexen-1 -ol\n81                                       cis-4-hexen-l-ol \n82                                           methylhexanol\n83                                     2-methyl-3-hexanol \n84                                    5-methyl-1 -hexanol \n85                                             1-heptanol \n86                                              2-heptanol\n87                                              4-heptanol\n88                                 6-methyl-5-hepten-2-ol \n89                                               1-octanol\n91                                           l-octen-3-ol \n92                                     7-methyl-l-octanol \n93                                              1-decanol \n94                                            8-p-cymenol \n95                                             citronellol\n96                                                farnesol\n97                                               geraniol \n98                                                linalool\n100                                                 nerol \n101                                              nerolidol\n102                                          terpinen-4-ol\n103                                            o-terpineol\n104                                        benzyl alcohol \n105                                        2-phenylethanol\n107                              4-isopropylbenzyl alcohol\n108                                               menthol \n109                                                PHENOLS\n110                                                phenol \n111                                              o-cresol \n112                                               p-cresol\n113                                          4-ethylphenol\n114                                         4-vinylphenol \n115                                               guaiacol\n116                                4-ethyl-2-methoxyphenol\n117                                2-methoxy-4-vinylphenol\n118                                                eugenol\n119                                           3,4-xylenol \n120                                                 ETHERS\n121                                          diethyl ether\n122                                   1,1-dipropoxyethane \n123                    1-ethoxy-l (3-methylbutoxy)-ethane \n124                              1-ethoxy-l-pentoxyethane \n125                             l-methoxy-4-methylbenzene \n126                              isopropyl-methoxybenzene \n127                                    2-methoxy-biphenyl \n128                                              ALDEHYDES\n129                                           formaldehyde\n130                                           acetaldehyde\n175                                              propanal \n176                                               acrolein\n177                                       2-methylpropanal\n178                                                butanal\n179                                              2-butenal\n180                                       2-methylbutanal \n181                                       3-methylbutanal \n183                                    2-methyl-2-butenal \n184                                          tiglaldehyde \n185                                               pentanal\n186                                            2-pentenal \n187                                       trans-2-pentenal\n188                                             3-pentenal\n189                                         methylpentenal\n190                                                hexanal\n192                                               hexenal \n193                                              2-hexenal\n194                                          cis-2-hexenal\n195                                        trans-2-hexenal\n197                                         cis-3-hexenal \n198                                      2E,4Z-hexadienal \n199                                      2E,4E-hexadienal \n200                                               heptanal\n201                                              heptenal \n202                                             2-heptenal\n203                                       trans-2-heptenal\n204                                      2E,4Z-heptadienal\n205                                     2E,4E-heptadienal \n206                                               octanal \n207                                             2-octenal \n208                                       trans-2-octenal \n209                                               nonanal \n210                                              2-nonenal\n211                                        trans-2-nonenal\n212                                      2E,4E-nonadienal \n213                                               decanal \n214                                             2-decenal \n215                                        2,4-decadienal \n216                                       2E,4Z-decadienal\n217                                      2E,4E-decadienal \n218                                             undecanal \n219                                              dodecanal\n221                                                citral \n222                                                 neral \n223                                              geranial \n224                                           citronellal \n225                                               farnesal\n226                                          benzaldehyde \n228                                  3-methylbenzaldehyde \n229                                  4-methylbenzaldehyde \n230                                        salicylaldehyde\n231                                  4-hydroxybenzaldehyde\n232                                 3-methoxybenzaldehyde \n233                                          anisaldehyde \n234                                  2-phenylacetaldehyde \n235                                      3-phenylpropanal \n236                                         cinnamaldehyde\n237                                                KETONES\n238                                                acetone\n239                                            2-butanone \n240                                  3-hydroxy-2-butanone \n241                                          3-buten-2-one\n242                                            2-pentanone\n243                                           3-pentanone \n244                                         l-penten-3-one\n245                                         3-penten-2-one\n246                                         cyclopentanone\n247                                 2-ethylcyclopentanone \n248                                  2-methyl-3-pentanone \n249                                          mesityl oxide\n250                       4-hydroxy- 4-methyl-2-pentanone \n251                              2,4-dimethylpentan-3-one \n252                                            2-hexanone \n253                2-hydroxy-2,6,6-trimethylcyclohexanone \n254                                           2-heptanone \n255                                  4-methyl-3-heptanone \n256                                        methylheptenone\n257                               6-methyl-5-hepten-2-one \n259                           6-methyl-35-heptadien-2-one \n260                                             2-octanone\n261                                   3E,5E-octadien-2-one\n262                                            2-nonanone \n263                                   trans-2-nonen-4-one \n265                                             undecanone\n266                                          2-dodecanone \n267                                         pseudo-ionone \n268                                        geranylacetone \n270                                       farnesylacetone \n272                                              tt-ionone\n273                                               ß-ionone\n275                                              •y-ionone\n276                                         epoxy-0-ionone\n278                                                carvone\n279                                          acetophenone \n280                                  4-methylacetophenone \n281                                 2-hydroxyacetophenone \n282                                  4-methoxyacetophenone\n283                         4-methyl-4-phenyl-2-pentanone \n284                                   l-phenyl-2-propanone\n285                                    l-phenyl-2-butanone\n286                                   DICARBONYL COMPOUNDS\n287                                               glyoxal \n288                                          methylglyoxal\n289                                              biacetyl \n291                                       2-oxo-3-butenal \n292                                       2,3-pentanedione\n293                                      2,3-heptanedione \n294                                                  ACIDS\n295                                                formic \n296                                                 acetic\n297                                             propanoic \n298                                     2-methylpropanoic \n299                                              butanoic \n300                                      2-methylbutanoic \n301                                      3-methylbutanoic \n302                                              pentanoic\n303                                     4-methylpentanoic \n304                                              hexanoic \n305                                             4-hexenoic\n306                                             heptanoic \n308                                              octanoic \n309                                                geranic\n310                                               nonanoic\n311                                               myristic\n312                                         pentadecanoic \n313                                               palmitic\n314                                               stearic \n315                                                 oleic \n316                                               linoleic\n317                                             linolenic \n318                                               benzoic \n319                                              salicylic\n320                                        2-phenylacetic \n321                                              cinnamic \n322                                     4-hydroxycinnamic \n323                                                 ESTERS\n324                                        methyl fonnate \n325                                         ethyl fonnate \n326                                        pentyl fonnate \n327                                      phenetyl fonnate \n328                                        methyl acetate \n329                                         ethyl acetate \n330                                        propyl acetate \n331                                          butyl acetate\n332                                 2-methylbutyl acetate \n333                                     isopentyl acetate \n334                                         pentyl acetate\n335                                         hexyl acetate \n336                               trans-2-hexenyl acetate \n337                                     3-hexenyl acetate \n338                                  cis-3-hexenyl acetate\n339                              trans-3-hexeny 1 acetate2\n340                                        heptyl acetate \n341                               6 -methylheptyl acetate \n342                                         nonyl acetate \n343                                      phenethyl acetate\n344                                    citronellyl acetate\n345                                       geranyl acetate \n346                                        linalyl acetate\n347                                       ethyl propanoate\n348                                  isopentyl propanoate \n349                                 citronellyl propanoate\n350                                      methyl butanoate \n352                                     2-butyl butanoate \n353                                   isopentyl butanoate \n354                                 citronellyl butanoate \n355                                     geranyl butanoate \n356                            isobutyl 3-methylbutanoate \n357                       2-methylbutyl 3-methylbutanoate \n358                           isopentyl 3-methylbutanoate \n359                                   isobutyl pentanoate \n360                                   isopentyl pentanoate\n361                                      methyl hexanoate \n362                                        ethyl hexanoate\n363                                        butyl hexanoate\n364                                   isopentyl hexanoate \n365                                        hexyl hexanoate\n366                                  isopentyl heptanoate \n367                                       methyl octanoate\n368                                      propyl nonanoate \n369                                   isopentyl nonanoate \n370                                      propyl decanoate \n371                                   isopentyl decanoate \n372                                       methyl myristate\n373                                        ethyl myristate\n374                                  methyl pentadecanoate\n375                                      methyl palmitate \n376                                        ethyl palmitate\n377                                          methyl oléate\n378                                      methyl linoleate \n379                                        ethyl linoleate\n380                                      methyl linolenate\n381                                       ethyl linolenate\n382                                       methylsalicylate\n383                                       ethyl salicylate\n384                                               LACTONES\n385                                        7-butyrolactone\n386                                 2-methyl-4-butanolide \n387                              3-methyl-2-buten-4-olide \n388                                          4-pentanolide\n389                                3-methyl-4-pentanolide \n390                                          4-hexanolide \n391                                6-hydroxy-5-hexanolide \n392                                          4-octanolide \n393                                          5-octanolide \n394                                           4-nonanolide\n396                          2,4-dimethyl-2-nonen-4-olide \n397                                  dihydroactinidiolide \n399                                              phtalide \n400                                       SULFUR COMPOUNDS\n401                                       hydrogen sulfide\n402                                       dimethyl sulfide\n403                                   ethylmethyl sulfide \n404                                    dimethyl disulfide \n405                                methylpropyl disulfide \n406                                          methanethiol \n407                                 2-(methylthio)ethanol \n408                                 3(methylthio)-propanol\n409                              5(methylthio)-l-pentanol \n410                            2(methylthio)-acetaldehyde \n411                                3(methylthio)-propanal \n412                           methyl-methanethiosulfonate \n413                                     NITROGEN COMPOUNDS\n414                                            methylamine\n415                                             ethylamine\n416                                          dimethylamine\n417                                        trimethylamine \n418                                            propylamine\n419                                            butylamine \n420                                          isobutylamine\n421                                    dimethylethylamine \n422                                           diethylamine\n423                                    2-methylbutylamine \n424                                           pentylamine \n425                                         isopentylamine\n426                                         diphenylamine \n427                                 3-methylbutanal-oxime \n428                                         butanenitrile \n429                                  3-methylbutanenitrile\n430                                        pentanenitrile \n431                                         benzyl cyanide\n432                                    3-methylnitrobutane\n433                          3-hydroxy-3-methylnitrobutane\n434                                      HALOGEN COMPOUNDS\n435                                       trichloromethane\n436                                     trichloroethylene \n437                                    1,2-dichlorobenzene\n438               OXYGEN-CONTAINING HETEROCYCLIC COMPOUNDS\n439                                                 furan \n440                                          2-methylfuran\n441                                          2-ethylfuran \n442                                         2-propylfuran \n443                                      2-isobutenylfuran\n444                             2-isopropyl-5-methylfuran \n445                            2-isopropenyl-5-methylfuran\n446                              2-methyl-5-propenylfuran \n447                                         2-pentylfuran \n448                                          2-hexylfuran \n449                                         2-heptylfuran \n450                                            acetylfuran\n451                                         2-acetylfuran \n452                                              furfural \n454                                       5-methylfurfural\n455                                 2-acetyl-5-methylfuran\n456                              2-acetonyl-5-methylfuran \n457                             methyl-2-furancarboxylate \n458                                          dibenzofuran \n459                                      furfuryl alcohol \n460                                2-furancarboxylic acid \n461                          2-methyltetrahydro-3-furanone\n462                               linalool oxide I. or II.\n463                                     linalool oxide I. \n465                                    linalool oxide II. \n467                                   linalool III. or IV.\n468                                            structure) \n469                                           1,4-dioxane \n470                          2,2,4-trimethyl-l3-dioxolane \n471               2,7-dioxa-l,3,3-trimethylbicycloheptane \n472                   ó.S-dioxa-l.S-dimethylbicyclooctane \n473               SULFUR-CONTAINING HETEROCYCLIC COMPOUNDS\n474                                     2-forraylthiophene\n475                                     3-formylthiophene \n476                            2-formyl-5-methylthiophene \n477                                     2-acetylthiophene \n478                            2-thiophenecarboxyIic acid \n479             NITROGEN-CONTAINING HETEROCYCLIC COMPOUNDS\n480                                               pyrrole \n481                                   2,5-dimethylpyrrole \n483                                        2-formylpyrrole\n484                                       2-acetylpyrrole \n485                                              pyridine \n486                                      2-formylpyridine \n487                                         methylpyrazine\n488                                      2-methylpyrazine \n489                                  2,6-dimethylpyrazine \n490                               2-ethyl-6-vinylpyrazine \n491                          2-isopropyl-3-methoxypyrazine\n492                                                indene \n493 SULFUR- AND NITROGEN-CONTAINING HETEROCYCLIC COMPOUNDS\n494                                      2-propylthiazole \n495                                    2-isobutylthiazole \n497                                   2-sec-butylthiazole \n498                          2-isopropyl-4-methylthiazole \n499                                          benzothiazole\n500               NITROGEN-AND OXYGEN-CONTAINING COMPOUNDS\n501                                        4-butyloxazole \n502                               5-pentyl-4-ethyloxazole \n503                                 4,5-dimethylisoxazole \n\ntomato_cleaned$compounds <- str_remove_all(tomato_cleaned$compounds, \n                                           \"\\\\s\")\n\n\n\nThe file could be exported as a .csv file as a back up, in case it is needed again in the future.\nTRANSFORM\nNext, I had to split the compounds into various chemical categories.\n\n\nhydrocarbons <- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(2:39) %>% \n  mutate(category = \"hydrocarbons\")\n\n\nalcohols <- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(41:98) %>% \n  mutate(category = \"alcohols\")\n\nphenols <- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(99:108) %>% \n  mutate(category = \"phenols\")\n\nethers<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(111:116) %>% \n  mutate(category = \"ethers\")\n\naldehydes<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(118:176) %>% \n  mutate(category = \"aldehydes\")\n\nketones<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(178:219) %>% \n  mutate(category = \"ketones\")\n\ndicarbonyl<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(221:226) %>% \n  mutate(category = \"dicarbonyl\")\n\nacids<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(228:254) %>% \n  mutate(category = \"acids\")\n\nesters<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(256:314) %>% \n  mutate(category = \"esters\")\n\nlactones<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(316:328) %>% \n  mutate(category = \"lactones\")\n\nsulfur<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(330:341) %>% \n  mutate(category = \"sulfur\")\n\nnitrogen<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(343:362) %>% \n  mutate(category = \"nitrogen\")\n\nhalogen<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(364:366) %>% \n  mutate(category = \"halogen\")\n\noxygen<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(368:398) %>% \n  mutate(category = \"oxygen\")\n\nsulfur_heterocyclic<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(400:404) %>% \n  mutate(category = \"sulfur_heterocyclic\")\n\nnitrogen_heterocyclic<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(406:417) %>% \n  mutate(category = \"nitrogen_heterocyclic\")\n\nnitrogen_sulfur_heterocyclic<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(419:423) %>% \n  mutate(category = \"nitrogen_sulfur_heterocyclic\")\n\nnitrogen_oxygen_heterocyclic<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(425:427) %>% \n  mutate(category = \"nitrogen_oxygen_heterocyclic\")\n\n\n\nVisualize\n\n\ntomatoes_compounds <- bind_rows(hydrocarbons,\n                                alcohols,\n                                phenols,\n                                ethers,\n                                aldehydes,\n                                ketones,\n                                dicarbonyl,\n                                acids,\n                                esters,\n                                lactones,\n                                sulfur,\n                                halogen,\n                                oxygen,\n                                sulfur_heterocyclic,\n                                nitrogen_heterocyclic,\n                                nitrogen_sulfur_heterocyclic,\n                                nitrogen_oxygen_heterocyclic\n                                )\n\nplot <- tomatoes_compounds %>% \n  group_by(category) %>% \n  summarise(count = n()) %>% \n  ggplot(aes(x = reorder(category, count), y = count, label = count)) +\n  geom_col(fill = \"tomato2\") +\n  geom_text(aes(label = count), hjust = -0.5, size = 5) +\n  scale_y_continuous(expand = c(0,0), limits = c(0, 80)) +\n  labs(y = \"No. of compounds\",\n       x = \"Category\",\n       title = \"Number of volatile compounds identified in tomatoes, sorted by chemical category\",\n       subtitle = \"Esters, aldehydes and alcohols dominate the types of compounds identified\",\n       caption = \"Petro-Turza(1989): Flavor of tomato and tomato products \") +\n  coord_flip() +\n  theme_classic() +\n   theme(title = element_text(size = 28),\n        axis.title = element_text(size = 24, face = \"bold\"),\n        axis.text = element_text(size = 20))\n\nplot\n\n\n\n\nReflections\nIt may have been easier to type out the list of 400 compounds, which would only take an hour or less, with the formatting done properly on the onset. However, if the table was much longer, text cleaning would be more effectively carried out by stringr. Some improvements could be made to the script so that I do not have to carry out multiple str_replace_all, and to automatically filter out by categories instead of manually defining them. However, it was a good beginner’s practice on text cleaning using the stringr package as I do not often have the chance to use regular expressions, and I found the str_detect, str_which and str_view_all functions extremely useful in locating regex matches.\nThe plot above only lists the number of compounds identified so far by chemical classes, but does not show which are the character impact compounds that contribute significantly to tomatoes.\nHistorically, researchers focused on identifying volatiles, quantifying them and classifying them based on their odor thresholds to determine which compounds played a contributory role to tomato flavor. However, the new trend is in assessing the importance of compounds based on how much they contribute to the liking of tomato flavor, and this could be by means of targeted metabolomics, or by generating prediction models for different descriptors of tomato flavor using regression analysis of both volatile and non-volatile compounds, or by carrying out multivariate modelling on physicochemical, volatile and sensory parameters(Rambla et al. 2013).\nIt would be interesting to try to apply prediction models and multivariate analysis in R.\nReferences\nhttps://www.r-bloggers.com/2019/09/pdf-scraping-in-r-with-tabulizer/\n\n\n\nBaldwin, E. A., J. W. Scott, M. A. Einstein, T. M. M. Malundo, B. T. Carr, R. L. Shewfelt, and K. S. Tandon. 1998. “Relationship Between Sensory and Instrumental Analysis for Tomato Flavor.” Journal of the American Society for Horticultural Science 123 (5): 906–15.\n\n\nButtery, Ron G., Roy Teranishi, and Louisa C. Ling. 1987. “Fresh Tomato Aroma Volatiles: A Quantitative Study.” Journal of Agricultural and Food Chemistry 35 (4): 540–44.\n\n\nPetro‐Turza, Martha. 1986. “Flavor of Tomato and Tomato Products.” Food Reviews International 2 (3): 309–51. https://doi.org/10.1080/87559128609540802.\n\n\nRambla, José L., Yury M. Tikunov, Antonio J. Monforte, Arnaud G. Bovy, and Antonio Granell. 2013. “The Expanded Tomato Fruit Volatile Landscape.” Journal of Experimental Botany 65 (16): 4613–23.\n\n\nYilmaz, Emin. 2001. “The Chemistry of Fresh Tomato Flavor.” Turkish Journal of Agriculture and Forestry 25 (3): 149–55.\n\n\n\n\n",
    "preview": "posts/20210130_Volatiles_tomato/Volatiles_tomatoes_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-02-01T23:36:52+08:00",
    "input_file": {},
    "preview_width": 3840,
    "preview_height": 2304
  },
  {
    "path": "posts/20210123_PCA wine/",
    "title": "PCA Wine",
    "description": "PCA (using tidymodels) with wine dataset",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-23",
    "categories": [],
    "contents": "\nSummary\nPCA is a data reduction technique, to uncover latent variables that are uncorrelated. It is an unsupervised way of classification. Not all of the variables in high-dimensional data are required. Some are highly correlated with others and these variables may be omitted, while retaining a similar level of information in the dataset in terms of explaining the variance.\nIt is used as an exploratory data analysis tool, and may be used for feature engineering and/or clustering.\nWorkflow\nImport data\nExploratory data analysis\nskim\nggcorr\nggpairs\nCheck assumptions on whether PCA can be carried out\nKMO\nBartlett\nCarry out PCA using tidymodels workflow\nAlways use only continuous variables, ensure that there are no missing data. Determine the number of components using eigenvalues, scree plots and parallel analysis.\nrecipe : preprocess the data (missing values, center and scale, ensuring that variables are continuous)\nprep : evaluate the data\nbake : get the PCA Scores results\nvisualize\ncommunicate results: show the scree plot, PCA loadings, variance explained by each component, loadings and score plot.\nThe scores plot show the positions of the individual wine samples in the coordinate system of the PCs.\nThe loadings plot shows the contribution of the X variables to the PCs.\nLoading packages\n\n\nlibrary(pacman)\np_load(corrr, palmerpenguins, GGally, tidymodels, tidytext, tidyverse, psych,\n       skimr, gridExtra, kohonen, janitor, learntidymodels, kohonen)\n\n\n\nImport\nThis dataset is from the kohonen package. It contains 177 rows and 13 columns.\nThese data are the results of chemical analyses of wines grown in the same region in Italy (Piedmont) but derived from three different cultivars: Nebbiolo, Barberas and Grignolino grapes. The wine from the Nebbiolo grape is called Barolo. The data contain the quantities of several constituents found in each of the three types of wines, as well as some spectroscopic variables.\nThe dataset requires some cleaning, and the type of wine was added to the datset.\n\n\ndata(wines)\n\nwines <- as.data.frame(wines) %>% \n  janitor::clean_names() %>%  # require data.frame\n  as_tibble() %>% \n  cbind(vintages)  # vintages = Y outcome = category\n \nglimpse(wines)\n\n\nRows: 177\nColumns: 14\n$ alcohol          <dbl> 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, 1…\n$ malic_acid       <dbl> 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, 1…\n$ ash              <dbl> 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, 2…\n$ ash_alkalinity   <dbl> 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, 1…\n$ magnesium        <dbl> 100, 101, 113, 118, 112, 96, 121, 97, 98, 1…\n$ tot_phenols      <dbl> 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, 2…\n$ flavonoids       <dbl> 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, 2…\n$ non_flav_phenols <dbl> 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, 0…\n$ proanth          <dbl> 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, 1…\n$ col_int          <dbl> 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, 5…\n$ col_hue          <dbl> 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, 1…\n$ od_ratio         <dbl> 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, 2…\n$ proline          <dbl> 1050, 1185, 1480, 735, 1450, 1290, 1295, 10…\n$ vintages         <fct> Barolo, Barolo, Barolo, Barolo, Barolo, Bar…\n\nEDA\nSome exploratory data analysis was carried out:\nWhat are the types of variables? Categorical or numerical?\nWhat is the distribution like? Skewed?\nAre there any missing values?\nAre there any outliers?\nCheck the types of wine\nAre the variables quite correlated with each other?\nskimr\n\n\nskim(wines) # 177 x 13, all numeric + Y outcome\n\n\nTable 1: Data summary\nName\nwines\nNumber of rows\n177\nNumber of columns\n14\n_______________________\n\nColumn type frequency:\n\nfactor\n1\nnumeric\n13\n________________________\n\nGroup variables\nNone\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\nvintages\n0\n1\nFALSE\n3\nGri: 71, Bar: 58, Bar: 48\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nalcohol\n0\n1\n12.99\n0.81\n11.03\n12.36\n13.05\n13.67\n14.83\n▂▇▇▇▃\nmalic_acid\n0\n1\n2.34\n1.12\n0.74\n1.60\n1.87\n3.10\n5.80\n▇▅▂▂▁\nash\n0\n1\n2.37\n0.28\n1.36\n2.21\n2.36\n2.56\n3.23\n▁▂▇▅▁\nash_alkalinity\n0\n1\n19.52\n3.34\n10.60\n17.20\n19.50\n21.50\n30.00\n▁▆▇▃▁\nmagnesium\n0\n1\n99.59\n14.17\n70.00\n88.00\n98.00\n107.00\n162.00\n▅▇▃▁▁\ntot_phenols\n0\n1\n2.29\n0.63\n0.98\n1.74\n2.35\n2.80\n3.88\n▅▇▇▇▁\nflavonoids\n0\n1\n2.02\n1.00\n0.34\n1.20\n2.13\n2.86\n5.08\n▆▆▇▂▁\nnon_flav_phenols\n0\n1\n0.36\n0.12\n0.13\n0.27\n0.34\n0.44\n0.66\n▃▇▅▃▂\nproanth\n0\n1\n1.59\n0.57\n0.41\n1.25\n1.55\n1.95\n3.58\n▃▇▆▂▁\ncol_int\n0\n1\n5.05\n2.32\n1.28\n3.21\n4.68\n6.20\n13.00\n▇▇▃▂▁\ncol_hue\n0\n1\n0.96\n0.23\n0.48\n0.78\n0.96\n1.12\n1.71\n▅▇▇▃▁\nod_ratio\n0\n1\n2.60\n0.71\n1.27\n1.93\n2.78\n3.17\n4.00\n▆▃▆▇▂\nproline\n0\n1\n745.10\n314.88\n278.00\n500.00\n672.00\n985.00\n1680.00\n▇▇▅▃▁\n\nGGally\n\n\nwines %>% \n  select(-vintages) %>% \n  ggcorr(label = T, label_alpha = T, label_round = 2)\n\n\n\nwines %>% \n  ggpairs(aes(col = vintages))\n\n\n\n\nChecking assumptions\nIs the dataset suitable for PCA analysis?\n\n\n# Continuous Y\n# No missing data\n# Check assumptions for PCA #####\nwines_no_y <- wines %>% \n  select(-vintages)\n\nglimpse(wines_no_y)\n\n\nRows: 177\nColumns: 13\n$ alcohol          <dbl> 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, 1…\n$ malic_acid       <dbl> 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, 1…\n$ ash              <dbl> 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, 2…\n$ ash_alkalinity   <dbl> 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, 1…\n$ magnesium        <dbl> 100, 101, 113, 118, 112, 96, 121, 97, 98, 1…\n$ tot_phenols      <dbl> 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, 2…\n$ flavonoids       <dbl> 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, 2…\n$ non_flav_phenols <dbl> 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, 0…\n$ proanth          <dbl> 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, 1…\n$ col_int          <dbl> 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, 5…\n$ col_hue          <dbl> 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, 1…\n$ od_ratio         <dbl> 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, 2…\n$ proline          <dbl> 1050, 1185, 1480, 735, 1450, 1290, 1295, 10…\n\n# KMO: Indicates the proportion of variance in the variables that may be caused by underlying factors. High values (close to 1) indicate that factor analysis may be useful.\nwines_no_y %>% \n  cor() %>% \n  KMO() # .70 above : YES\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = .)\nOverall MSA =  0.78\nMSA for each item = \n         alcohol       malic_acid              ash   ash_alkalinity \n            0.73             0.80             0.44             0.68 \n       magnesium      tot_phenols       flavonoids non_flav_phenols \n            0.67             0.87             0.81             0.82 \n         proanth          col_int          col_hue         od_ratio \n            0.85             0.62             0.79             0.86 \n         proline \n            0.81 \n\n# Bartlett's test of sphericity: tests the hypothesis that the correlation matrix is an identity matrix (ie variables are unrelated and not suitable for structure detection.) For factor analysis, the p. value should be <0.05.\n\nwines_no_y %>% \n  cor() %>% \n  cortest.bartlett(., n = 177) # p<0.05\n\n\n$chisq\n[1] 1306.787\n\n$p.value\n[1] 3.302319e-222\n\n$df\n[1] 78\n\nTidymodels (PCA)\nRecipe\nWith the use of update_role(), the types of wine information is retained in the dataset.\nstep_normalize() combines step_center() and step_scale()\nNote that step_pca is the second step –> will need to retrieve the PCA results from the second list later.\n\n\nwines_recipe <- recipe(~ ., data = wines) %>% \n  update_role(vintages, new_role = \"id\") %>%  \n  # step_naomit(all_predictors()) %>% \n  step_normalize(all_predictors()) %>% \n  step_pca(all_predictors(), id = \"pca\")\n\nwines_recipe # 13 predictors\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n        id          1\n predictor         13\n\nOperations:\n\nCentering and scaling for all_predictors()\nNo PCA components were extracted.\n\nPreparation\n\n\nwines_prep <- prep(wines_recipe)\n\nwines_prep # trained\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n        id          1\n predictor         13\n\nTraining data contained 177 data points and no missing data.\n\nOperations:\n\nCentering and scaling for alcohol, malic_acid, ... [trained]\nPCA extraction with alcohol, malic_acid, ... [trained]\n\ntidy_pca_loadings <- wines_prep %>% \n  tidy(id = \"pca\")\n\ntidy_pca_loadings # values here are the loading\n\n\n# A tibble: 169 x 4\n   terms               value component id   \n   <chr>               <dbl> <chr>     <chr>\n 1 alcohol          -0.138   PC1       pca  \n 2 malic_acid        0.246   PC1       pca  \n 3 ash               0.00432 PC1       pca  \n 4 ash_alkalinity    0.237   PC1       pca  \n 5 magnesium        -0.135   PC1       pca  \n 6 tot_phenols      -0.396   PC1       pca  \n 7 flavonoids       -0.424   PC1       pca  \n 8 non_flav_phenols  0.299   PC1       pca  \n 9 proanth          -0.313   PC1       pca  \n10 col_int           0.0933  PC1       pca  \n# … with 159 more rows\n\nBake\n\n\nwines_bake <- bake(wines_prep, wines)\nwines_bake  # has the PCA LOADING VECTORS that we are familiar with\n\n\n# A tibble: 177 x 6\n   vintages   PC1    PC2    PC3      PC4     PC5\n   <fct>    <dbl>  <dbl>  <dbl>    <dbl>   <dbl>\n 1 Barolo   -2.22 -0.301 -2.03   0.281   -0.259 \n 2 Barolo   -2.52  1.06   0.974 -0.734   -0.198 \n 3 Barolo   -3.74  2.80  -0.180 -0.575   -0.257 \n 4 Barolo   -1.02  0.886  2.02   0.432    0.274 \n 5 Barolo   -3.04  2.16  -0.637  0.486   -0.630 \n 6 Barolo   -2.45  1.20  -0.985  0.00466 -1.03  \n 7 Barolo   -2.06  1.64   0.143  1.20     0.0105\n 8 Barolo   -2.51  0.958 -1.78  -0.104   -0.871 \n 9 Barolo   -2.76  0.822 -0.986 -0.374   -0.437 \n10 Barolo   -3.48  1.35  -0.428 -0.0399  -0.316 \n# … with 167 more rows\n\nCheck number of PC\n\n\n# a. Eigenvalues: Keep components greater than 1\n# data is stored in penguins_prep, step 3\n\nwines_prep$steps[[2]]$res$sdev # 3\n\n\n [1] 2.1628220 1.5815708 1.2055413 0.9614802 0.9282978 0.8030241\n [7] 0.7429548 0.5922321 0.5377546 0.4967984 0.4748054 0.4103374\n[13] 0.3224124\n\n# b. Scree plot/Variance plot\n\nwines_prep %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms ==  \"percent variance\") %>% \n  ggplot(aes(x = component, y = value)) +\n  geom_point(size = 2) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = 1:4) +\n  labs(title = \"% Variance explained\",\n       y = \"% total variance\",\n       x = \"PC\",\n       caption = \"Source: ChemometricswithR book\") +\n  geom_text(aes(label = round(value, 2)), vjust = -0.3, size = 4) +\n  theme_minimal() +\n  theme(axis.title = element_text(face = \"bold\", size = 12),\n        axis.text = element_text(size = 10),\n        plot.title = element_text(size = 14, face = \"bold\"))  # 2 or 3\n\n\n\n# bii: Cumulative variance plot\n\nwines_prep %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms == \"cumulative percent variance\") %>%\n  ggplot(aes(component, value)) +\n  geom_col(fill= \"forestgreen\") +\n  labs(x = \"Principal Components\", \n       y = \"Cumulative variance explained (%)\",\n       title = \"Cumulative Variance explained\") +\n  geom_text(aes(label = round(value, 2)), vjust = -0.2, size = 4) +\n  theme_minimal() +\n  theme(axis.title = element_text(face = \"bold\", size = 12),\n        axis.text = element_text(size = 10),\n        plot.title = element_text(size = 14, face = \"bold\")) \n\n\n\n# c. Parallel analysis\n\nfa.parallel(cor(wines_no_y),\n            n.obs = 333,\n            cor = \"cor\",\n            plot = T)  # 3\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  3 \n\nVisualize\nLoadings plot\n\n\nplot_loadings <- tidy_pca_loadings %>% \n  filter(component %in% c(\"PC1\", \"PC2\", \"PC3\", \"PC4\")) %>% \n  mutate(terms = tidytext::reorder_within(terms, \n                                          abs(value), \n                                          component)) %>% \n  ggplot(aes(abs(value), terms, fill = value>0)) +\n  geom_col() +\n  facet_wrap( ~component, scales = \"free_y\") +\n  scale_y_reordered() + # appends ___ and then the facet at the end of each string\n  scale_fill_manual(values = c(\"deepskyblue4\", \"darkorange\")) +\n  labs( x = \"absolute value of contribution\",\n        y = NULL,\n        fill = \"Positive?\",\n        title = \"PCA Loadings Plot\",\n        subtitle = \"Number of PC should be 3, compare the pos and the neg\",\n        caption = \"Source: ChemometricswithR\") +\n  theme_minimal()\n\n\nplot_loadings\n\n\n\n# PC1: flavonoids, tot_phenols, od_ratio, proanthocyanidins, col_hue, 36%\n# PC2: col_int, alcohol, proline, ash, magnesium; 19.2%\n# PC3: ash, ash_alkalinity, non_flav phenols; 11.2%\n# PC4: malic acid?\n\n\n\nAn alternative way to plot:\n\n\n# alternate plot loadings\n\nlearntidymodels::plot_top_loadings(wines_prep,\n                  component_number <= 4, n = 5) +\n  scale_fill_manual(values = c(\"deepskyblue4\", \"darkorange\")) +\n  theme_minimal()\n\n\n\n\nLoadings only\n\n\n# define arrow style\narrow_style <- arrow(angle = 30,\n                     length = unit(0.2, \"inches\"),\n                     type = \"closed\")\n\n# get pca loadings into wider format\npca_loadings_wider <- tidy_pca_loadings%>% \n  pivot_wider(names_from = component, id_cols = terms)\n\n\npca_loadings_only <- pca_loadings_wider %>% \n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_segment(aes(xend = PC1, yend = PC2),\n               x = 0, \n               y = 0,\n               arrow = arrow_style) +\n  ggrepel::geom_text_repel(aes(x = PC1, y = PC2, label = terms),\n            hjust = 0, \n            vjust = 1,\n            size = 5,\n            color = \"red\") +\n  labs(title = \"Loadings on PCs 1 and 2 for normalized data\") +\n  theme_classic()\n\n\n\nScores plot\n\n\n# Scores plot #####\n# PCA SCORES are in bake\npc1pc2_scores_plot <- wines_bake %>% \n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(color = vintages, shape = vintages), \n             alpha = 0.8, size = 2) +\n  scale_color_manual(values = c(\"deepskyblue4\", \"darkorange\", \"purple\")) +\n  labs(title = \"Scores on PCs 1 and 2 for normalized data\",\n       x = \"PC1 (36%)\",\n       y = \"PC2 (19.2%)\") +\n  theme_classic() +\n  theme(legend.position = \"none\") \n\n\n\nFinalised plots\n\n\ngrid.arrange(pc1pc2_scores_plot, pca_loadings_only, ncol = 2)\n\n\n\n\nCheck against Data\n\n\nwines %>% \n  group_by(vintages) %>% \n  summarise(across(c(flavonoids, col_int, ash, malic_acid),\n                   mean,\n                   na.rm = T))\n\n\n# A tibble: 3 x 5\n  vintages   flavonoids col_int   ash malic_acid\n  <fct>           <dbl>   <dbl> <dbl>      <dbl>\n1 Barbera         0.781    7.40  2.44       3.33\n2 Barolo          2.98     5.53  2.46       2.02\n3 Grignolino      2.08     3.09  2.24       1.93\n\nInterpretation of results\nPCA allows for exploratory characterizing of x variables that are associated with each other.\nPC1: flavanoids, total phenols, OD_ratio. PC2: color intensity, alcohol, proline PC3: ash, ash_alkalinity PC4: malic acid (by right 3 components are sufficient)\nBarbera, indicated in blue, has the largest score on PC 1 and PC2. Barolo, indicated in orange, has the smallest score on PC 1. Grignolo, indicated in purple, has the lowest score on PC 2.\nBarbera has low flavonoids, high col_int and high malic acid Barolo has high flavonoids, medium col_int and intermediate malic acid Grignolino has intermediate flavonoids, high col_int and low malic acid.\nReferences\nhttps://rdrr.io/github/tidymodels/learntidymodels/f/inst/tutorials/pca_recipes/pca_recipes.Rmd\nhttps://allisonhorst.github.io/palmerpenguins/articles/articles/pca.html\nhttps://www.ibm.com/support/knowledgecenter/en/SSLVMB_subs/statistics_casestudies_project_ddita/spss/tutorials/fac_telco_kmo_01.html\n\n\n\n",
    "preview": "posts/20210123_PCA wine/PCA-wine_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-03-16T20:20:38+08:00",
    "input_file": {},
    "preview_width": 3072,
    "preview_height": 2304
  },
  {
    "path": "posts/20210120_statistical concepts/",
    "title": "Statistical Concepts",
    "description": "Definition of terms",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [],
    "contents": "\nThis is a glossary of terms, in alphabetical order.\nCorrelation, r : whether there is any relationship between two variables. If so, whether the relationship is weak or strong, and what the direction of relationship is.\nPrincipal Component Analysis (PCA): a multivariate technique used to reduce the number of dimensions to explain the total variation in the data with a few linear combinations of original variables, which are uncorrelated.\nVariance, Sˆ2: the average of squared deviations of the values from mean. Square root of variance = standard deviation.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-21T00:34:33+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210115_kovats/",
    "title": "Kovats Index",
    "description": "R script for calculating Kovats Index",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\nContents\nBackground\nR workflow\nExampleLoad packages\nImport\nTransform\n\nReferences:\n\nBackground\nAbout 70% of my time at work is spent on interpreting GCMS and GC data. It is more of a qualitative type of identifying what each peak is, and this requires a seach based on mass spectra found in the GCMS library, as well as using the retention index. When working on GC data, I am even more reliant on the retention index for cross checking of peaks on GCMS, since there is no spectra information available.\nRetention time is influenced by GC conditions and column types. Using retention time alone is not useful when you are trying to compare with retention times stated in the literature, since the elution conditions are different.\nThe Kovats index (KI) may be used to convert retention times into standardised retention indices (RI), based on retention times of alkane standards. The equation for Non-Isothermal Kovats RI is shown below.\n\\[\nI_x = 100n + 100(t_x-t_n) / (t_(n+1) − t_n)\n\\]\nPrior to learning R, I used to do the calculation on an excel spreadsheet. This was cumbersome, first I had to key in the retention times of each alkane standard, and then update my formula for the range of retention times between each alkane standard, and then copy and paste all the compiled retention times into 2 columns. That involved a lot of clicking with the mouse.\nR workflow\nRun alkane standards on instrument (for example, GCMS) and compile the retention times in either .csv or .xlsx.\nCreate a function to calculate KI.\nCalculate the KI for retention times between each pair of alkane standard\nMerge the compiled retention times and corresponding KI together\nExport the data to excel and use the vlookup function to find out the KI when retention time is keyed in; alternatively, use inner_join function to tabulate calculated KI before identifying the peaks. I am using the former as there may be some small peaks that were not integrated, or coeluted with other peaks, so there is still a degree of manual input that is required.\nExample\nSample retention time data was retrieved from: https://massfinder.com/wiki/Retention_index_guide\nLoad packages\n\n\nlibrary(tidyverse)\n\n\n\nImport\n\n\n# Key in values\ncarbon_number <- c(\"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\")\nMS_RT <- c(1.85, 2.71, 3.69, 4.59, 5.37, 6.19, 7.17, 8.40, 9.99)\n\n# Create a tibble\nms_rt <- cbind(carbon_number, MS_RT) %>% as_tibble()\nms_rt$carbon_number <- as.numeric(ms_rt$carbon_number)\nms_rt$MS_RT <- as.numeric(ms_rt$MS_RT)\n\n# The data may also be imported from excel\n\n\n\nTransform\n\n\n# create function to calculate KI ####\nto_Calc_KI = function(n,Tn,m,Tm,Ti){\n  RI = 100*n + (100*(m-n)*((Ti-Tn)/(Tm-Tn)))\n  round(RI, 0)\n  \n}\n\n\n\n\n\n# create function to filter by carbon number ####\n# dat refers to data\n# col refers to column\n# val refers to values\n\nfilter_by_carbon_number <- function(dat, col, val){\n  filter(dat, col %in%  val)\n}\n\n\n\nThe following step could be improved on by creating another function to repeat the codes rather than manually changing the values.\n\n\nfil_c8c9 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(8,9)) \n\nfil_c9c10 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(9,10)) \n\nfil_c10c11 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(10,11)) \nfil_c11c12 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(11,12)) \nfil_c12c13 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(12,13)) \nfil_c13c14 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(13,14)) \nfil_c14c15 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(14,15)) \nfil_c15c16 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(15,16)) \n\n\n\n\n\n# create function to generate tibble for KI calculation\ncreate_KI_tibble <- function(msrt_col, n , m){\n  seq(from = min(msrt_col), to = max(msrt_col), by = 0.01) %>% \n    as_tibble() %>% \n    rename(\"Ti\" = value) %>% \n    mutate(n = n,\n           m = m,\n           Tn = min(msrt_col), \n           Tm = max(msrt_col)) %>% \n    dplyr::select(n, Tn, m, Tm, Ti) %>% \n    mutate(KI = pmap_dbl(., to_Calc_KI))\n}\n\n\n\n\n\nc8c9 <- create_KI_tibble(fil_c8c9$MS_RT, 8, 9)\nc9c10 <- create_KI_tibble(fil_c9c10$MS_RT, 9, 10)\nc10c11 <- create_KI_tibble(fil_c10c11$MS_RT, 10, 11)\nc11c12 <- create_KI_tibble(fil_c11c12$MS_RT, 11, 12)\nc12c13 <- create_KI_tibble(fil_c12c13$MS_RT, 12, 13)\nc13c14 <- create_KI_tibble(fil_c13c14$MS_RT, 13, 14)\nc14c15 <- create_KI_tibble(fil_c14c15$MS_RT, 14, 15)\nc15c16 <- create_KI_tibble(fil_c15c16$MS_RT, 15, 16)\n\ncalculated_MS_KI <- rbind(c8c9, c9c10, c10c11, c11c12, c12c13, \n                          c13c14, c14c15, c15c16) %>% \n  select(Ti, KI)\n\n# Export created file if needed\n# write_xlsx(calculated_MS_KI, \"Kovats_Indices.xlsx\")\n\n\n\nLooking at the first 6 lines of tabulated KI:\n\n\nhead(calculated_MS_KI)\n\n\n# A tibble: 6 x 2\n     Ti    KI\n  <dbl> <dbl>\n1  1.85   800\n2  1.86   801\n3  1.87   802\n4  1.88   803\n5  1.89   805\n6  1.9    806\n\nReferences:\nhttps://webbook.nist.gov/chemistry/gc-ri/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-15T11:19:30+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210118_calibration curves/",
    "title": "Calibration Curves Data",
    "description": "R script for calculating Limit of Detection and Limit of Quantification",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\nContents\nLoading required packages\nBackground\nImport Example Dataset\nVisualize\nModel\nErrors in slope and intercept of regression line\nPredict the value of x from y:\n\nNote: the data and theory on calibration curve were with reference from: Statistics and Chemometrics for Analytical Chemistry, James N. Miller and Jane Charlotte Miller, 6th edition, Chapter 5\nLoading required packages\n\n\nlibrary(pacman)\np_load(tidyverse, broom, chemCal)\n\n\n\nBackground\nChemists often work with calibration data using standards of known concentrations and putting them through instrumental analysis. When plotting a calibration curve, it is of interest to calculate the limit of detection (LOD) and limit of quantification (LOQ) of the method.\nImport Example Dataset\nThe fluorescence intensities of standard aqueous fluorescein solutions were analysed with a spectrophotometer, and the fluorescence results are shown below:\n\n# A tibble: 7 x 2\n  conc_pgml  fluo\n      <dbl> <dbl>\n1         0   2.1\n2         2   5  \n3         4   9  \n4         6  12.6\n5         8  17.3\n6        10  21  \n7        12  24.7\n\nVisualize\n\n\n\nModel\nLet’s fit a linear model to get the slope (b) and intercept(a).\n\\[\ny = a + bx\n\\]\n\n\nCall:\nlm(formula = fluo ~ conc_pgml, data = data)\n\nResiduals:\n       1        2        3        4        5        6        7 \n 0.58214 -0.37857 -0.23929 -0.50000  0.33929  0.17857  0.01786 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.5179     0.2949   5.146  0.00363 ** \nconc_pgml     1.9304     0.0409  47.197 8.07e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4328 on 5 degrees of freedom\nMultiple R-squared:  0.9978,    Adjusted R-squared:  0.9973 \nF-statistic:  2228 on 1 and 5 DF,  p-value: 8.066e-08\n\nFrom above, we can see that slope = 1.9304, and intercept = 1.5179.\nErrors in slope and intercept of regression line\nThe limit of detection is defined as:\n\\[\nLOD = \\gamma_B + 3_{SB}\n\\] where LOD is the analyte concentration wich gives a signal equal to the blank signal plus three standard deviations of the blank.\nA function was created to calculate LOD and LOQ:\n\n\ncalcLOD_y <- function(model) {\n  SSE <- sum(model$residuals**2)\n  n <- length(model$residuals) -2\n  Syx <- sqrt(SSE/n)\n  intercept <- as.numeric(model$coefficients[1])\n  calculated_y <- intercept + 3*Syx\n  names(calculated_y) <- \"calculated_y\"\n  print(calculated_y)\n  \n  chemCal::inverse.predict(model,\n                  newdata = calculated_y,\n                  alpha = 0.05) \n}\n\n\n\n\n\ncalcLOQ_y <- function(model) {\n  SSE <- sum(model$residuals**2)\n  n <- length(model$residuals) -2\n  Syx <- sqrt(SSE/n)\n  intercept <- as.numeric(model$coefficients[1])\n  calculated_y <- intercept + 10*Syx\n  names(calculated_y) <- \"calculated_y\"\n  print(calculated_y)\n  \n  chemCal::inverse.predict(model,\n                  newdata = calculated_y,\n                  alpha = 0.05) \n}\n\n\n\nInserting the linear model from the fluorescence data:\n\n\nLOD_x <- calcLOD_y(fl_mod)\n\n\ncalculated_y \n      2.8164 \n\nLOD_x$Prediction \n\n\n[1] 0.6726958\n\n\n\nLOQ_x <- calcLOQ_y(fl_mod)\n\n\ncalculated_y \n    5.846334 \n\nLOQ_x$Prediction \n\n\n[1] 2.242319\n\nPredict the value of x from y:\nTo predict the concentration of fluorescein that has fluorescence units of 2.9, we use the function inverse.predict():\n\n\nchemCal::inverse.predict(fl_mod, \n                newdata = 2.9,\n                alpha = 0.05)\n\n\n$Prediction\n[1] 0.7160037\n\n$`Standard Error`\n[1] 0.2645698\n\n$Confidence\n[1] 0.6800982\n\n$`Confidence Limits`\n[1] 0.03590545 1.39610195\n\n\n\n\n",
    "preview": "posts/20210118_calibration curves/calibration-curves_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-01-18T23:23:27+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210114_motivations/",
    "title": "Motivations",
    "description": "why R?",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\nWhy R?\nI attended a short modular course on R, and was introduced to more effective and efficient ways of structuring data for customised plots that look way better than on Excel and SPSS. At the end of the course, I really wanted to retain what I have learnt, and build on what I have learnt, so that I can be better at R.\nR, to me, is a new form of literacy (like how Microsoft Office was taught in school last time). It is also an effective approach to learn problem solving, as well as a job skill.\nAristotle — ‘The more you know, the more you know you don’t know.’\nand that makes me want to learn even more.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-14T20:47:28+08:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "My first post: Learning goals for 2021",
    "description": "pRactice corner for coding in R",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\nHi, this is my practice corner for coding in R. I would want to:\nlearn tidyverse\npractice on data visualization, exploration.\nlearn tidymodels/machine learning\nwork on chemistry related datasets using R\nlearn Design of Experiment\nlearn Chemometrics\nlearn how to analyse sensory data\nbe able to communicate insights from data analysis using the Rmarkdown/distill packages\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-18T21:54:53+08:00",
    "input_file": {}
  }
]
