[
  {
    "path": "posts/20210810 High Dimension Data /",
    "title": "High Dimension Data",
    "description": "EdX Course",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-08-11",
    "categories": [],
    "contents": "\n\nContents\nResources\nWeek 1: Distance\nExamples\n\nWeek 2: Dimension Reduction\nDimension Reduction Motivation\nProjections\nRotations\nSingular Value Decomposition\nMulti-Dimensional Scaling Plots\nPrincipal Component Analysis\n\nWeek 3: Basic Machine Learning\nClustering\nClassification\nConditional Probabilities and Expectations\nClass Prediction\nSmoothing\nBin Smoothing\nLoess\nCross-validation\n\nWeek 4: Batch Effects\nConfounding\n\n\nI’m taking a course from EdX on High Dimension Data, and this is a post for me to note down the learning points and codes. I am interested in this course because I am often dealing with GC-MS data on flavor compounds found in different types of food, and I would like to carry out more high level analysis rather than just looking at percentage area for comparison. I think there is a lot that can be learnt from the life science sector in this sense, on how high dimensional data is carried out, how to apply clustering, principal component analysis.\nPreviously, I tried to use tidymodels for PCA, and would like to learn other approaches as well.\nAll the material below are from EdX. Most of the codes are in base R language. Personally, I find the tidymodels method more intuitive, but I can pick up the learning points on the theory behind analysing High Dimension Data, and see how the language evolved to the tidymodels method.\nThe main learning points from the course are:\nWhat is distance\nHow to calculate distance (althought the calculation part is largely taken care of by R)\nWhat is projection\nHow PC uses rotation to maximise variability for dimension reduction\nImportance of cross-validation, splitting data into training and test sets for machine learning\nBrief glimpse into hierarchical clustering, k-means clustering, principal component analysis.\nHow confounding can give you different conclusions from the truth.\nAfter this course, I would like to revisit the tidymodels approach and revise on what I learnt previously.\nResources\nPH525x series - Biomedical Data Science http://genomicsclass.github.io/book/\nR & Bioconductor Manual http://manuals.bioinformatics.ucr.edu/home/R_BioCondManual\nAdvanced R http://adv-r.had.co.nz/Data-structures.html\nGithub for course materials http://github.com/genomicsclass/labs https://github.com/genomicsclass/labs/tree/master/highdim\nWeek 1: Distance\nClustering is a methodology that requires us to define distance. When we cluster animals into groups, in a way, we are saying which animals are closer to each other.\nIn genomics literature, heatmaps are often used. In a heatmap, on the top and on the left, dendrograms are used to show clustering.\nIt is important to define distance between both rows and columns of data.\nFor simple two dimensional data, the distance between two points can be calculated by the Pythygoras theorem.\n\n\nlibrary(rafalib)\nmypar()\nplot(c(0,1,1),c(0,0,1),pch=16,cex=2,xaxt=\"n\",yaxt=\"n\",xlab=\"\",ylab=\"\",bty=\"n\",xlim=c(-0.25,1.25),ylim=c(-0.25,1.25))\nlines(c(0,1,1,0),c(0,0,1,0))\ntext(0,.2,expression(paste('(A'[x]*',A'[y]*')')),cex=1.5)\ntext(1,1.2,expression(paste('(B'[x]*',B'[y]*')')),cex=1.5)\ntext(-0.1,0,\"A\",cex=2)\ntext(1.1,1,\"B\",cex=2)\n\n\n\n\nThe euclidean distance between \\(A\\) and \\(B\\) is simply:\n\\[\\sqrt{ (A_x-B_x)^2 + (A_y-B_y)^2}\\]\nFor high dimensional data, we can’t really do that because data is not two dimensional. High dimensional data has many rows and many columns (eg thousands of gene expression measurements for dozens of samples). (For my field, it may be hundreds of flavor compounds for dozens of food samples).\nDistance is defined for two columns (or two samples), and distance between two rows (or two features, or in this case, two genes.)\nThis is the transposed version, probably unique to genomics field. From what I learnt from ISLR previously, rows are observations (samples), and columns are features (genes, or compounds).\n\n\nlibrary(devtools)\n# install_github(\"genomicsclass/tissuesGeneExpression\")\n\n\nlibrary(tissuesGeneExpression)\ndata(tissuesGeneExpression)\ndim(e) ##e contains the expression data\n\n\n[1] 22215   189\n\ntable(tissue) ##tissue[i] tells us what tissue is represented by e[,i]\n\n\ntissue\n cerebellum       colon endometrium hippocampus      kidney \n         38          34          15          31          39 \n      liver    placenta \n         26           6 \n\nIf we are interested in describing distance between samples (columns), or in finding genes (rows) that behave similarly across samples.\nComparing 2 columns/samples: Distance between two samples i and j would be taking the sum of the differences squared, then take the square root.\n\\[\n\\mbox{dist}(i,j) = \\sqrt{ \\sum_{g=1}^{22215} (Y_{g,i}-Y_{g,j })^2 }\n\\]\nComparing 2 rows/features: take the same approach above.\n\\[\n\\mbox{dist}(h,g) = \\sqrt{ \\sum_{i=1}^{189} (Y_{h,i}-Y_{g,i})^2 }\n\\] Note: In practice, distances between features are typically applied after standardizing the data for each feature.\nExamples\nWe can now use the formulas above to compute distance. Let’s compute distance between samples 1 and 2, both kidneys, and then to sample 87, a colon.\n\n\nx <- e[,1] # kidney\ny <- e[,2] # kidney\nz <- e[,87] # colon\nsqrt(sum((x-y)^2))\n\n\n[1] 85.8546\n\nsqrt(sum((x-z)^2))\n\n\n[1] 122.8919\n\nAs expected, the kidneys are closer to each other. A faster way to compute this is using matrix algebra:\n\n\nsqrt( crossprod(x-y) ) # cross product\n\n\n        [,1]\n[1,] 85.8546\n\nsqrt( crossprod(x-z) )\n\n\n         [,1]\n[1,] 122.8919\n\nNow to compute all the distances at once, we have the function dist.\nBecause it computes the distance between each row (genes), and here we are interested in the distance between columns (samples), we transpose the matrix\n\n\nd <- dist(t(e)) # t: transpose\nclass(d) # distance: not easy to work with this type of data, better to turn it into matrix\n\n\n[1] \"dist\"\n\n# do not run d, may crash r, as it computes all pairwise distances between genes\n\n\n\nNote that this produces an object of class dist and, to access the entries using row and column indices, we need to coerce it into a matrix:\n\n\nas.matrix(d)[1,2] # first and second sample\n\n\n[1] 85.8546\n\nas.matrix(d)[1,87] # first and 87th sample\n\n\n[1] 122.8919\n\n# to get color image of all the distances: 189 by 189 matrix\nimage(as.matrix(d))\n\n\n\n# red is smaller and yellow is larger\n\n\n\nIt is important to remember that if we run dist on e, it will compute all pairwise distances between genes. This will try to create a \\(22215 \\times 22215\\) matrix that may crash your R sessions.\nWeek 2: Dimension Reduction\nto reduce the dimensions of the data, but at the same time, preserving important properties, such as the distance between samples.\nfor PCA, the units in the standard statistical analysis are in the rows.\nfor genomics, the units are in the columns.\n\n\nlibrary(knitr)\n\n\n\nDimension Reduction Motivation\nVisualizing data is one of the most, if not the most, important step in the analysis of high-throughput data. The right visualization method may reveal problems with the experimental data that can render the results from a standard analysis, although typically appropriate, completely useless.\nWe have shown methods for visualizing global properties of the columns or rows, but plots that reveal relationships between columns or between rows are more complicated due to the high dimensionality of data. Creating one single scatterplot of the data is impossible since points are very high dimensional.\nWe will describe powerful techniques for exploratory data analysis based on dimension reduction. The general idea is to reduce the dataset to have fewer dimensions, yet approximately preserve important properties, such as the distance between samples. If we are able to reduce down to, say, two dimensions, we can then easily make plots. The technique behind it all, the singular value decomposition (SVD), is also useful in other contexts. Before introducing the rather complicated mathematics behind the SVD, we will motivate the ideas behind it with a simple example.\nExample: Reducing two dimensions to one\nWe consider an example with twin heights. Here we simulate 100 two dimensional points that represent the number of standard deviations each individual is from the mean height. Each point is a pair of twins:\n\n\n\n(#fig:simulate_twin_heights)Simulated twin pair heights.\n\n\n\nTo help with the illustration, think of this as high-throughput gene expression data with the twin pairs representing the \\(N\\) samples and the two heights representing gene expression from two genes.\nWe are interested in the distance between any two samples. We can compute this using dist. For example, here is the distance between the two orange points in the figure above:\n\n\nd=dist(t(y)) # must transpose in this case\nas.matrix(d)[1,2] # easier to work with matrix than distance objects\n\n\n[1] 1.140897\n\nWhat if making two dimensional plots was too complex and we were only able to make 1 dimensional plots. Can we, for example, reduce the data to a one dimensional matrix that preserves distances between points?\nIf we look back at the plot, and visualize a line between any pair of points, the length of this line is the distance between the two points. These lines tend to go along the direction of the diagonal. We have seen before that we can “rotate” the plot so that the diagonal is in the x-axis by making a MA-plot instead:\n\n\nz1 = (y[1,]+y[2,])/2 #the sum \nz2 = (y[1,]-y[2,])   #the difference\nz = rbind( z1, z2) #matrix now same dimensions as y\nthelim <- c(-3,3)\nmypar(1,2)\nplot(y[1,],y[2,],xlab=\"Twin 1 (standardized height)\",\n     ylab=\"Twin 2 (standardized height)\",\n     xlim=thelim,ylim=thelim)\npoints(y[1,1:2],y[2,1:2],col=2,pch=16)\nplot(z[1,],z[2,],xlim=thelim,ylim=thelim,xlab=\"Average height\",ylab=\"Difference in height\")\npoints(z[1,1:2],z[2,1:2],col=2,pch=16)\n\n\n\n\nFigure 1: Twin height scatterplot (left) and MA-plot (right).\n\n\n\nRotations\nIn the plot above, the distance between the two orange points remains roughly the same, relative to the distance between other points. This is true for all pairs of points. A simple re-scaling of the transformation we performed above will actually make the distances exactly the same. What we will do is multiply by a scalar so that the standard deviations of each point is preserved. The calculation is such that the matrices are orthogonal and it guarantees the SD-preserving properties described above. The distances are now exactly preserved:\n\n\nA <- 1/sqrt(2)*matrix(c(1,1,1,-1),2,2)\nz <- A%*%y\nd <- dist(t(y))\nd2 <- dist(t(z))\nmypar(1,1)\nplot(as.numeric(d),as.numeric(d2)) #as.numeric turns distances into long vector\nabline(0,1,col=2)\n\n\n\n\n(#fig:rotation_preserves_dist)Distance computed from original data and after rotation is the same.\n\n\n\nWe call this particular transformation a rotation of y.\n\n\nmypar(1,2)\nthelim <- c(-3,3)\nplot(y[1,],y[2,],xlab=\"Twin 1 (standardized height)\",\n     ylab=\"Twin 2 (standardized height)\",\n     xlim=thelim,ylim=thelim)\npoints(y[1,1:2],y[2,1:2],col=2,pch=16)\nplot(z[1,],z[2,],xlim=thelim,ylim=thelim,xlab=\"Average height\",ylab=\"Difference in height\")\npoints(z[1,1:2],z[2,1:2],col=2,pch=16)\n\n\n\n\nFigure 2: Twin height scatterplot (left) and after rotation (right).\n\n\n\nThe reason we applied this transformation in the first place was because we noticed that to compute the distances between points, we followed a direction along the diagonal in the original plot, which after the rotation falls on the horizontal, or the first dimension of z. So this rotation actually achieves what we originally wanted: we can preserve the distances between points with just one dimension. Let’s remove the second dimension of z and recompute distances:\n\n\nd3 = dist(z[1,]) ##distance computed using just first dimension\nmypar(1,1)\nplot(as.numeric(d),as.numeric(d3)) \nabline(0,1)\n\n\n\n\n(#fig:approx_dist)Distance computed with just one dimension after rotation versus actual distance.\n\n\n\nThe distance computed with just the one dimension provides a very good approximation to the actual distance and a very useful dimension reduction: from 2 dimensions to 1. This first dimension of the transformed data is actually the first principal component. This idea motivates the use of principal component analysis (PCA) and the singular value decomposition (SVD) to achieve dimension reduction more generally.\nImportant note on a difference to other explanations\nIf you search the web for descriptions of PCA, you will notice a difference in notation to how we describe it here. This mainly stems from the fact that it is more common to have rows represent units. In statistics this is also the most common way to represent the data: individuals in the rows.\nHowever, for practical reasons, in genomics it is more common to represent units in the columns. For example, genes are rows and samples are columns. For this reason, in this book we explain PCA and all the math that goes with it in a slightly different way than it is usually done.\nBasically, if you want our explanations to match others you have to transpose the matrices we show here.\nProjections\nNow that we have described the concept of dimension reduction and some of the applications of SVD and principal component analysis, we focus on more details related to the mathematics behind these. We start with projections. A projection is a linear algebra concept that helps us understand many of the mathematical operations we perform on high-dimensional data. For more details, you can review projects in a linear algebra book. Here we provide a quick review and then provide some data analysis related examples.\nAs a review, remember that projections minimize the distance between points and subspace.\nWe illustrate projections using a figure, in which the arrow on top is pointing to a point in space. In this particular cartoon, the space is two dimensional, but we should be thinking abstractly. The space is represented by the Cartesian plan and the line on which the little person stands is a subspace of points. The projection to this subspace is the place that is closest to the original point. Geometry tells us that we can find this closest point by dropping a perpendicular line (dotted line) from the point to the space. The little person is standing on the projection. The amount this person had to walk from the origin to the new projected point is referred to as the coordinate.\nWe can plot it like this:\n\n\nmypar (1,1)\nplot(c(0,4),c(0,4),xlab=\"Dimension 1\",ylab=\"Dimension 2\",type=\"n\")\narrows(0,0,2,3,lwd=3)\ntext(2,3,\" Y\",pos=4,cex=3)\n\n\n\n\nFigure 3: Geometric representation of Y.\n\n\n\nThe following R code confirms this equation works:\n\n\nmypar(1,1)\nplot(c(0,4),c(0,4),xlab=\"Dimension 1\",ylab=\"Dimension 2\",type=\"n\")\narrows(0,0,2,3,lwd=3)\nabline(0,0.5,col=\"red\",lwd=3) #if x=2c and y=c then slope is 0.5 (y=0.5x)\ntext(2,3,\" Y\",pos=4,cex=3)\ny=c(2,3)\nx=c(2,1)\ncc = crossprod(x,y)/crossprod(x)\nsegments(x[1]*cc,x[2]*cc,y[1],y[2],lty=2)\ntext(x[1]*cc,x[2]*cc,expression(hat(Y)),pos=4,cex=3)\n\n\n\n\nFigure 4: Projection of Y onto new subspace.\n\n\n\nRotations\nOne of the most useful applications of projections relates to coordinate rotations. In data analysis, simple rotations can result in easier to visualize and interpret data.\n\n\nlibrary(rafalib)\nmypar()\nplot(c(-2,4),c(-2,4),xlab=\"Dimension 1\",ylab=\"Dimension 2\",\n     type=\"n\",xaxt=\"n\",yaxt=\"n\",bty=\"n\")\ntext(rep(0,6),c(c(-2,-1),c(1:4)),as.character(c(c(-2,-1),c(1:4))),pos=2)\ntext(c(c(-2,-1),c(1:4)),rep(0,6),as.character(c(c(-2,-1),c(1:4))),pos=1)\nabline(v=0,h=0)\narrows(0,0,2,3,lwd=3)\nsegments(2,0,2,3,lty=2)\nsegments(0,3,2,3,lty=2)\ntext(2,3,\" Y\",pos=4,cex=3)\n\n\n\n\nFigure 5: Plot of (2,3) as coordinates along Dimension 1 (1,0) and Dimension 2 (0,1).\n\n\n\nGraphically, we can see that the coordinates are the projections to the spaces defined by the new basis:\n\n\nlibrary(rafalib)\nmypar()\nplot(c(-2,4),c(-2,4),xlab=\"Dimension 1\",ylab=\"Dimension 2\",\n     type=\"n\",xaxt=\"n\",yaxt=\"n\",bty=\"n\")\ntext(rep(0,6),c(c(-2,-1),c(1:4)),as.character(c(c(-2,-1),c(1:4))),pos=2)\ntext(c(c(-2,-1),c(1:4)),rep(0,6),as.character(c(c(-2,-1),c(1:4))),pos=1)\nabline(v=0,h=0)\nabline(0,1,col=\"red\")\nabline(0,-1,col=\"red\")\narrows(0,0,2,3,lwd=3)\ny=c(2,3)\nx1=c(1,1)##new basis\nx2=c(0.5,-0.5)##new basis\nc1 = crossprod(x1,y)/crossprod(x1)\nc2 = crossprod(x2,y)/crossprod(x2)\nsegments(x1[1]*c1,x1[2]*c1,y[1],y[2],lty=2)\nsegments(x2[1]*c2,x2[2]*c2,y[1],y[2],lty=2)\ntext(2,3,\" Y\",pos=4,cex=3)\n\n\n\n\nFigure 6: Plot of (2,3) as a vector in a rotatated space, relative to the original dimensions.\n\n\n\nExample: Twin heights\n\n\n\nFigure 7: Twin 2 heights versus twin 1 heights.\n\n\n\nHere are the rotations:\n\n\n\nFigure 8: Rotation of twin 2 heights versus twin 1 heights.\n\n\n\nWhat we have done here is rotate the data so that the first coordinate is the average height, while the second is the difference between twin heights.\nWe have used the singular value decomposition to find principal components. It is sometimes useful to think of the SVD as a rotation, in which the dimensions are ordered by how much variance they explain.\nSingular Value Decomposition\nIn the previous section, we motivated dimension reduction and showed a transformation that permitted us to approximate the distance between two dimensional points with just one dimension.\nThe singular value decomposition (SVD) is a generalization of the algorithm we used in the motivational section.\nAs in the example, the SVD provides a transformation of the original data. This transformation has some very useful properties.\nApplying the SVD to the motivating example we have:\n\n\nlibrary(rafalib)\nlibrary(MASS)\nn <- 100\ny <- t(mvrnorm(n,c(0,0), matrix(c(1,0.95,0.95,1),2,2)))\ns <- svd(y)\n\n\n\nWe can immediately see that applying the SVD results in a transformation very similar to the one we used in the motivating example:\n\n\nround(sqrt(2) * s$u , 3)\n\n\n       [,1]   [,2]\n[1,] -0.982 -1.017\n[2,] -1.017  0.982\n\nThe plot we showed after the rotation was showing what we call the principal components: the second plotted against the first.\n\n\nPC1 = s$d[1]*s$v[,1]\nPC2 = s$d[2]*s$v[,2]\nplot(PC1,PC2,xlim=c(-3,3),ylim=c(-3,3))\n\n\n\n\nFigure 9: Second PC plotted against first PC for the twins height data.\n\n\n\nSVD is related to principal component that explicitly tries to find rotations that maximise the variability explained.\nApplying SVD to real dataset\nLet’s compute the SVD on the gene expression table we have been working with. We will take a subset of 500 genes so that computations are faster.\n\n\nlibrary(tissuesGeneExpression)\ndata(tissuesGeneExpression) # very big dataset: 22215 rows (genes) by 189 columns (samples)\n\nset.seed(1)\n\nind <- sample(nrow(e),500) \n\nY <- t(apply(e[ind,],1,scale)) #standardize data for illustration ie scaling --> tidymodels is more intuitive\n\n\n\nThe svd command returns the three matrices (only the diagonal entries are returned for \\(D\\))\n\n\ns <- svd(Y)\nU <- s$u\nV <- s$v\nD <- diag(s$d) ##turn it into a matrix\n\n\n\nFirst note that we can in fact reconstruct y:\n\n\nYhat <- U %*% D %*% t(V)\nresid <- Y - Yhat\nmax(abs(resid))\n\n\n[1] 3.275158e-14\n\nIf we look at the sum of squares, we see that the last few are quite close to 0 (perhaps we have some replicated columns).\n\n\nplot(s$d)\n\n\n\n\n(#fig:D_entries)Entries of the diagonal of D for gene expression data.\n\n\n\nYou commonly see this described as “explaining less variance”. This implies that for a large matrix, by the time you get to the last columns, it is possible that there is not much left to “explain” As an example, we will look at what happens if we remove the four last columns:\n\n\nk <- ncol(U)-4\nYhat <- U[,1:k] %*% D[1:k,1:k] %*% t(V[,1:k])\nresid <- Y - Yhat \nmax(abs(resid))\n\n\n[1] 3.275158e-14\n\nThe largest residual is practically 0, and we need 4 fewer dimensions to transmit the information.\nIn this particular dataset, we can obtain a good approximation keeping only 94 columns. The following plots are useful for seeing how much of the variability is explained by each column:\n\n\nplot(s$d^2/sum(s$d^2)*100,ylab=\"Percent variability explained\")\n\n\n\n\n(#fig:percent_var_explained)Percent variance explained by each principal component of gene expression data.\n\n\n\nWe can also make a cumulative plot:\n\n\nplot(cumsum(s$d^2)/sum(s$d^2)*100,ylab=\"Percent variability explained\",ylim=c(0,100),type=\"l\")\n\n\n\n\n(#fig:cum_variance_explained)Cumulative variance explained by principal components of gene expression data.\n\n\n\nTherefore, by using only half as many dimensions, we retain most of the variability in our data:\n\n\nvar(as.vector(resid))/var(as.vector(Y))\n\n\n[1] 9.78718e-30\n\nWe say that we explain 96% of the variability.\nNote that we can compute this proportion from \\(D\\):\n\n\n1-sum(s$d[1:k]^2)/sum(s$d^2)\n\n\n[1] 0\n\nThe entries of \\(D\\) therefore tell us how much each PC contributes in term of variability explained.\nHighly correlated data\nTo help understand how the SVD works, we construct a dataset with two highly correlated columns.\nFor example:\n\n\nm <- 100\nn <- 2\nx <- rnorm(m)\ne <- rnorm(n*m,0,0.01)\nY <- cbind(x,x)+e\ncor(Y)\n\n\n          x         x\nx 1.0000000 0.9999263\nx 0.9999263 1.0000000\n\nThe SVD helps us notice that we explain almost all the variability with just this first column:\n\n\nd <- svd(Y)$d\nd[1]^2/sum(d^2)\n\n\n[1] 0.9999627\n\nIn cases with many correlated columns, we can achieve great dimension reduction:\n\n\nm <- 100\nn <- 25\nx <- rnorm(m)\ne <- rnorm(n*m,0,0.01)\nY <- replicate(n,x)+e\nd <- svd(Y)$d\nd[1]^2/sum(d^2)\n\n\n[1] 0.9999123\n\nMulti-Dimensional Scaling Plots\nWe will motivate multi-dimensional scaling (MDS) plots with a gene expression example. To simplify the illustration we will only consider three tissues:\n\n\nlibrary(rafalib)\nlibrary(tissuesGeneExpression)\ndata(tissuesGeneExpression)\ncolind <- tissue%in%c(\"kidney\",\"colon\",\"liver\") # only these three tissues\nmat <- e[,colind]\ngroup <- factor(tissue[colind])\ndim(mat) # 22215 features\n\n\n[1] 22215    99\n\nAs an exploratory step, we wish to know if gene expression profiles stored in the columns of mat show more similarity between tissues than across tissues. Unfortunately, as mentioned above, we can’t plot multi-dimensional points. In general, we prefer two-dimensional plots, but making plots for every pair of genes or every pair of samples is not practical. Multi-Dimensionsal Scaling (MDS) plots become a powerful tool in this situation.\n\n\ns <- svd(mat-rowMeans(mat))\nPC1 <- s$d[1]*s$v[,1]\nPC2 <- s$d[2]*s$v[,2]\nmypar(1,1)\nplot(PC1,PC2,pch=21,bg=as.numeric(group))\nlegend(\"bottomright\",levels(group),col=seq(along=levels(group)),pch=15,cex=1.5)\n\n\n\n\nFigure 10: Multi-dimensional scaling (MDS) plot for tissue gene expression data.\n\n\n\nNote that the points separate by tissue type as expected. Now the accuracy of the approximation above depends on the proportion of variance explained by the first two principal components. As we showed above, we can quickly see this by plotting the variance explained plot:\n\n\nplot(s$d^2/sum(s$d^2))\n\n\n\n\n(#fig:variance_explained)Variance explained for each principal component.\n\n\n\nAlthough the first two PCs explain over 50% of the variability, there is plenty of information that this plot does not show. However, it is an incredibly useful plot for obtaining, via visualization, a general idea of the distance between points. Also, notice that we can plot other dimensions as well to search for patterns. Here are the 3rd and 4th PCs:\n\n\nPC3 <- s$d[3]*s$v[,3]\nPC4 <- s$d[4]*s$v[,4]\nmypar(1,1)\nplot(PC3,PC4,pch=21,bg=as.numeric(group))\nlegend(\"bottomright\",levels(group),col=seq(along=levels(group)),pch=15,cex=1.5)\n\n\n\n\n(#fig:PC_3_and_4)Third and fourth principal components.\n\n\n\nNote that the 4th PC shows a strong separation within the kidney samples. Later we will learn about batch effects, which might explain this finding.\ncmdscale\nAlthough we used the svd functions above, there is a special function that is specifically made for multi-dimensional scaling plots.\nIt takes a distance object as an argument and then uses principal component analysis to provide the best approximation to this distance that can be obtained with \\(k\\) dimensions.\nThis function is more efficient because one does not have to perform the full SVD, which can be time consuming.\nBy default it returns two dimensions, but we can change that through the parameter k which defaults to 2.\n\n\nd <- dist(t(mat))\nmds <- cmdscale(d)\nmypar()\nplot(mds[,1],mds[,2],bg=as.numeric(group),pch=21,\n     xlab=\"First dimension\",ylab=\"Second dimension\")\nlegend(\"bottomleft\",levels(group),col=seq(along=levels(group)),pch=15)\n\n\n\n\nFigure 11: MDS computed with cmdscale function.\n\n\n\nThese two approaches are equivalent up to an arbitrary sign change.\n\n\nmypar(1,2)\nfor(i in 1:2){\n  plot(mds[,i],s$d[i]*s$v[,i],main=paste(\"PC\",i))\n  b = ifelse( cor(mds[,i],s$v[,i]) > 0, 1, -1)\n  abline(0,b) ##b is 1 or -1 depending on the arbitrary sign \"flip\"\n}\n\n\n\n\n(#fig:mds_same_as_svd)Comparison of MDS first two PCs to SVD first two PCs.\n\n\n\nWhy we substract the mean\nIn all calculations above we subtract the row means before we compute the singular value decomposition.\nWe are trying to approximate the distance between columns (samples), and removing row averages reduces the total variation to make the SVD approximation better.\nPrincipal Component Analysis\nWe have already mentioned principal component analysis (PCA) above and noted its relation to the SVD. Here we provide further mathematical details.\nExample: Twin heights\nWe started the motivation for dimension reduction with a simulated example and showed a rotation that is very much related to PCA.\n\n\n\n(#fig:simulate_twin_heights_again)Twin heights scatter plot.\n\n\n\nWhat are principal components (PCs)?\nWe are looking for a transformation in which the coordinates show high variability.\nThe principal components\nThe orthogonal vector that maximizes the sum of squares is referred to as the first PC. The weights used to obtain this PC are referred to as the loadings. Using the language of rotations, it is also referred to as the direction of the first PC, which are the new coordinates.\nTo obtain the second PC, we repeat the exercise above, but for the residuals.\nprcomp\nWe have shown how to obtain PCs using the SVD. However, R has a function specifically designed to find the principal components. In this case, the data is centered by default. The following function:\n\n\npc <- prcomp( t(Y) ) # transposed for genomic data.\n\n\n\nproduces the same results as the SVD up to arbitrary sign flips:\n\n\ns <- svd( Y - rowMeans(Y) )\nmypar(1,2)\nfor(i in 1:nrow(Y) ){\n  plot(pc$x[,i], s$d[i]*s$v[,i])\n}\n\n\n\n\n(#fig:pca_svd)Plot showing SVD and prcomp give same results.\n\n\n\nThe loadings can be found this way:\n\n\npc$rotation # loadings\n\n\n           PC1        PC2\n[1,] 0.7072304  0.7069831\n[2,] 0.7069831 -0.7072304\n\nwhich are equivalent (up to a sign flip) to:\n\n\ns$u\n\n\n           [,1]       [,2]\n[1,] -0.7072304 -0.7069831\n[2,] -0.7069831  0.7072304\n\nThe equivalent of the variance explained is included in the:\n\n\npc$sdev\n\n\n[1] 1.2542672 0.2141882\n\nWe take the transpose of Y because prcomp assumes the previously discussed ordering: units/samples in row and features in columns.\nWeek 3: Basic Machine Learning\nClustering\nMachine learning is a very broad topic and a highly active research area. In the life sciences, much of what is described as “precision medicine” is an application of machine learning to biomedical data. The general idea is to predict or discover outcomes from measured predictors. Can we discover new types of cancer from gene expression profiles? Can we predict drug response from a series of genotypes? Here we give a very brief introductions to two major machine learning components: clustering and class prediction.\n\n\nlibrary(tissuesGeneExpression)\ndata(tissuesGeneExpression)\n\n\n\nTo illustrate the main application of clustering in the life sciences, let’s pretend that we don’t know these are different tissues and are interested in clustering. The first step is to compute the distance between each sample:\n\n\nd <- dist( t(e) )\n\n\n\nHierarchical clustering\nWith the distance between each pair of samples computed, we need clustering algorithms to join them into groups. Hierarchical clustering is one of the many clustering algorithms available to do this. Each sample is assigned to its own group and then the algorithm continues iteratively, joining the two most similar clusters at each step, and continuing until there is just one group.\nWhile we have defined distances between samples, we have not yet defined distances between groups. There are various ways this can be done and they all rely on the individual pairwise distances. The helpfile for hclust includes detailed information.\nWe can perform hierarchical clustering based on the distances defined above using the hclust function. This function returns an hclust object that describes the groupings that were created using the algorithm described above. The plot method represents these relationships with a tree or dendrogram:\n\n\nlibrary(rafalib)\nmypar()\nhc <- hclust(d)\nhc\n\n\n\nCall:\nhclust(d = d)\n\nCluster method   : complete \nDistance         : euclidean \nNumber of objects: 189 \n\nplot(hc,cex=0.5)\n\n\n\nmyplclust(hc, labels=tissue, lab.col=as.fumeric(tissue), cex=0.5)\n\n\n\n\nVisually, it does seem as if the clustering technique has discovered the tissues. However, hierarchical clustering does not define specific clusters, but rather defines the dendrogram above. From the dendrogram we can decipher the distance between any two groups by looking at the height at which the two groups split into two. To define clusters, we need to “cut the tree” at some distance and group all samples that are within that distance into groups below. To visualize this, we draw a horizontal line at the height we wish to cut and this defines that line. We use 120 as an example:\n\n\nmyplclust(hc, labels=tissue, lab.col=as.fumeric(tissue),cex=0.5)\nabline(h=120)\n\n\n\n\nIf we use the line above to cut the tree into clusters, we can examine how the clusters overlap with the actual tissues:\n\n\nhclusters <- cutree(hc, h=120)\ntable(true=tissue, cluster=hclusters)\n\n\n             cluster\ntrue           1  2  3  4  5  6  7  8  9 10 11 12 13 14\n  cerebellum   0  0  0  0 31  0  0  0  2  0  0  5  0  0\n  colon        0  0  0  0  0  0 34  0  0  0  0  0  0  0\n  endometrium  0  0  0  0  0  0  0  0  0  0 15  0  0  0\n  hippocampus  0  0 12 19  0  0  0  0  0  0  0  0  0  0\n  kidney       9 18  0  0  0 10  0  0  2  0  0  0  0  0\n  liver        0  0  0  0  0  0  0 24  0  2  0  0  0  0\n  placenta     0  0  0  0  0  0  0  0  0  0  0  0  2  4\n\nWe can also ask cutree to give us back a given number of clusters. The function then automatically finds the height that results in the requested number of clusters:\n\n\nhclusters <- cutree(hc, k=8)\ntable(true=tissue, cluster=hclusters)\n\n\n             cluster\ntrue           1  2  3  4  5  6  7  8\n  cerebellum   0  0 31  0  0  2  5  0\n  colon        0  0  0 34  0  0  0  0\n  endometrium 15  0  0  0  0  0  0  0\n  hippocampus  0 12 19  0  0  0  0  0\n  kidney      37  0  0  0  0  2  0  0\n  liver        0  0  0  0 24  2  0  0\n  placenta     0  0  0  0  0  0  0  6\n\nIn both cases we do see that, with some exceptions, each tissue is uniquely represented by one of the clusters. In some instances, the one tissue is spread across two tissues, which is due to selecting too many clusters. Selecting the number of clusters is generally a challenging step in practice and an active area of research.\nk-means\nWe can also cluster with the kmeans function to perform k-means clustering. As an example, let’s run k-means on the samples in the space of the first two genes:\n\n\nset.seed(1)\nkm <- kmeans(t(e[1:2,]), centers=7)\nnames(km)\n\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"    \n[5] \"tot.withinss\" \"betweenss\"    \"size\"         \"iter\"        \n[9] \"ifault\"      \n\nmypar(1,2)\nplot(e[1,], e[2,], col=as.fumeric(tissue), pch=16)\nplot(e[1,], e[2,], col=km$cluster, pch=16)\n\n\n\n\nIn the first plot, color represents the actual tissues, while in the second, color represents the clusters that were defined by kmeans. We can see from tabulating the results that this particular clustering exercise did not perform well:\n\n\ntable(true=tissue,cluster=km$cluster)\n\n\n             cluster\ntrue           1  2  3  4  5  6  7\n  cerebellum   1  0  0 13  6  4 14\n  colon        3  0 22  0  6  3  0\n  endometrium  3  0  0  6  0  2  4\n  hippocampus  0  0  0  0 16 15  0\n  kidney      10  0  2  1  0  9 17\n  liver        0 18  0  7  0  0  1\n  placenta     4  0  0  1  0  0  1\n\nThis is very likely due to the fact the the first two genes are not informative regarding tissue type. We can see this in the first plot above. If we instead perform k-means clustering using all of the genes, we obtain a much improved result. To visualize this, we can use an MDS plot:\n\n\nkm <- kmeans(t(e), centers=7)\nmds <- cmdscale(d)\n\nmypar(1,2)\nplot(mds[,1], mds[,2]) \nplot(mds[,1], mds[,2], col=km$cluster, pch=16)\n\n\n\n\nBy tabulating the results, we see that we obtain a similar answer to that obtained with hierarchical clustering.\n\n\ntable(true=tissue,cluster=km$cluster)\n\n\n             cluster\ntrue           1  2  3  4  5  6  7\n  cerebellum   0  2  0  5  0  0 31\n  colon        0  0 34  0  0  0  0\n  endometrium  0  0  0  0  0 15  0\n  hippocampus  0  0  0 31  0  0  0\n  kidney       0  2  0  0 19 18  0\n  liver       24  2  0  0  0  0  0\n  placenta     0  0  6  0  0  0  0\n\nHeatmaps\nHeatmaps are ubiquitous in the genomics literature. They are very useful plots for visualizing the measurements for a subset of rows over all the samples. A dendrogram is added on top and on the side that is created with hierarchical clustering. We will demonstrate how to create heatmaps from within R. Let’s begin by defining a color palette:\n\n\nlibrary(RColorBrewer) \nhmcol <- colorRampPalette(brewer.pal(9, \"GnBu\"))(100)\n\n\n\nNow, pick the genes with the top variance over all samples:\n\n\n#if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n #   install.packages(\"BiocManager\")\n\n# BiocManager::install(\"genefilter\")\n\nlibrary(genefilter)\n\nrv <- rowVars(e)\nidx <- order(-rv)[1:40]\n\n\n\nWhile a heatmap function is included in R, we recommend the heatmap.2 function from the gplots package on CRAN because it is a bit more customized. For example, it stretches to fill the window. Here we add colors to indicate the tissue on the top:\n\n\nlibrary(gplots) ##Available from CRAN\ncols <- palette(brewer.pal(8, \"Dark2\"))[as.fumeric(tissue)]\nhead(cbind(colnames(e),cols))\n\n\n                       cols     \n[1,] \"GSM11805.CEL.gz\" \"#1B9E77\"\n[2,] \"GSM11814.CEL.gz\" \"#1B9E77\"\n[3,] \"GSM11823.CEL.gz\" \"#1B9E77\"\n[4,] \"GSM11830.CEL.gz\" \"#1B9E77\"\n[5,] \"GSM12067.CEL.gz\" \"#1B9E77\"\n[6,] \"GSM12075.CEL.gz\" \"#1B9E77\"\n\nheatmap.2(e[idx,], labCol=tissue,\n          trace=\"none\", \n          ColSideColors=cols, \n          col=hmcol)\n\n\n\n\nWe did not use tissue information to create this heatmap, and we can quickly see, with just 40 genes, good separation across tissues.\nClassification\nConditional Probabilities and Expectations\nPrediction problems can be divided into categorical and continuous outcomes. However, many of the algorithms can be applied to both due to the connection between conditional probabilities and conditional expectations.\nRegression in the context of prediction\nWe use the son and father height example to illustrate how regression can be interpreted as a machine learning technique. In our example, we are trying to predict the son’s height (Y) based on the father’s (X). Here we have only one predictor. Now if we were asked to predict the height of a randomly selected son, we would go with the average height:\n\n\nlibrary(rafalib)\nmypar(1,1)\ndata(father.son,package=\"UsingR\")\nx=round(father.son$fheight) ##round to nearest inch\ny=round(father.son$sheight)\nhist(y,breaks=seq(min(y),max(y)))\nabline(v=mean(y),col=\"red\",lwd=2)\n\n\n\n\n(#fig:height_hist)Histogram of son heights.\n\n\n\nIn this case, we can also approximate the distribution of Y as normal, which implies the mean maximizes the probability density.\nLet’s imagine that we are given more information. We are told that the father of this randomly selected son has a height of 71 inches (1.25 SDs taller than the average). What is our prediction now?\n\n\nmypar(1,2)\nplot(x,y,xlab=\"Father's height in inches\",ylab=\"Son's height in inches\",\n     main=paste(\"correlation =\",signif(cor(x,y),2)))\nabline(v=c(-0.35,0.35)+71,col=\"red\")\nhist(y[x==71],xlab=\"Heights\",nc=8,main=\"\",xlim=range(y))\n\n\n\n\n(#fig:conditional_distribution)Son versus father height (left) with the red lines denoting the stratum defined by conditioning on fathers being 71 inches tall. Conditional distribution: son height distribution of stratum defined by 71 inch fathers.\n\n\n\nThe best guess is still the expectation, but our strata has changed from all the data, to only the Y with X=71. So we can stratify and take the average, which is the conditional expectation.\n\n\nmypar(1,2)\nplot(x,y,xlab=\"Father's height in inches\",ylab=\"Son's height in inches\",\n     main=paste(\"correlation =\",signif(cor(x,y),2)))\nabline(v=c(-0.35,0.35)+71,col=\"red\")\nfit <- lm(y~x)\nabline(fit,col=1)\nhist(y[x==71],xlab=\"Heights\",nc=8,main=\"\",xlim=range(y))\nabline(v = fit$coef[1] + fit$coef[2]*71, col=1)\n\n\n\n\nFigure 12: Son versus father height showing predicted heights based on regression line (left). Conditional distribution with vertical line representing regression prediction.\n\n\n\nClass Prediction\nHere we give a brief introduction to the main task of machine learning: class prediction. In fact, many refer to class prediction as machine learning and we sometimes use the two terms interchangeably. We give a very brief introduction to this vast topic, focusing on some specific examples.\nSome of the examples we give here are motivated by those in the excellent textbook The Elements of Statistical Learning: Data Mining, Inference, and Prediction, by Trevor Hastie, Robert Tibshirani and Jerome Friedman, which can be found here.\nSimilar to inference in the context of regression, Machine Learning (ML) studies the relationships between outcomes \\(Y\\) and covariates \\(X\\). In ML, we call \\(X\\) the predictors or features. The main difference between ML and inference is that, in ML, we are interested mainly in predicting \\(Y\\) using \\(X\\). Statistical models are used, but while in inference we estimate and interpret model parameters, in ML they are mainly a means to an end: predicting \\(Y\\).\nHere we introduce the main concepts needed to understand ML, along with two specific algorithms: regression and k nearest neighbors (kNN). Keep in mind that there are dozens of popular algorithms that we do not cover here.\nIn a previous section, we covered the very simple one-predictor case. However, most of ML is concerned with cases with more than one predictor.\nFor illustration purposes, we move to a case in which \\(X\\) is two dimensional and \\(Y\\) is binary. We simulate a situation with a non-linear relationship using an example from the Hastie, Tibshirani and Friedman book.\nIn the plot below, we show the actual values of \\(f(x_1,x_2)=E(Y \\mid X_1=x_1,X_2=x_2)\\) using colors. The following code is used to create a relatively complex conditional probability function. We create the test and train data we use later (code not shown). Here is the plot of \\(f(x_1,x_2)\\) with red representing values close to 1, blue representing values close to 0, and yellow values in between.\n\n\n\n(#fig:conditional_prob)Probability of Y=1 as a function of X1 and X2. Red is close to 1, yellow close to 0.5, and blue close to 0.\n\n\n\nIf we show points for which \\(E(Y \\mid X=x)>0.5\\) in red and the rest in blue, we see the boundary region that denotes the boundary in which we switch from predicting 0 to 1.\n\n\n\n(#fig:bayes_rule)Bayes rule. The line divides part of the space for which probability is larger than 0.5 (red) and lower than 0.5 (blue).\n\n\n\nThe above plots relate to the “truth” that we do not get to see. Most ML methodology is concerned with estimating \\(f(x)\\). A typical first step is usually to consider a sample, referred to as the training set, to estimate \\(f(x)\\). We will review two specific ML techniques. First, we need to review the main concept we use to evaluate the performance of these methods.\nTraining and test sets\nIn the code (not shown) for the first plot in this chapter, we created a test and a training set. We plot them here:\n\n\n#x, test, cols, and coltest were created in code that was not shown\n#x is training x1 and x2, test is test x1 and x2\n#cols (0=blue, 1=red) are training observations\n#coltests are test observations\nmypar(1,2)\nplot(x,pch=21,bg=cols,xlab=\"X1\",ylab=\"X2\",xlim=XLIM,ylim=YLIM)\nplot(test,pch=21,bg=colstest,xlab=\"X1\",ylab=\"X2\",xlim=XLIM,ylim=YLIM)\n\n\n\n\n(#fig:test_train)Training data (left) and test data (right).\n\n\n\nYou will notice that the test and train set have similar global properties since they were generated by the same random variables (more blue towards the bottom right), but are, by construction, different. The reason we create test and training sets is to detect over-training by testing on a different data than the one used to fit models or train algorithms. We will see how important this is below.\nPredicting with regression\nA first naive approach to this ML problem is to fit a two variable linear regression model:\n\n\n##x and y were created in the code (not shown) for the first plot\n#y is outcome for the training set\nX1 <- x[,1] ##these are the covariates\nX2 <- x[,2] \nfit1 <- lm(y~X1+X2)\n\n\n\nOnce we the have fitted values, we can estimate \\(f(x_1,x_2)\\) with \\(\\hat{f}(x_1,x_2)=\\hat{\\beta}_0 + \\hat{\\beta}_1x_1 +\\hat{\\beta}_2 x_2\\). To provide an actual prediction, we simply predict 1 when \\(\\hat{f}(x_1,x_2)>0.5\\). We now examine the error rates in the test and training sets and also plot the boundary region:\n\n\n##prediction on train\nyhat <- predict(fit1)\nyhat <- as.numeric(yhat>0.5)\ncat(\"Linear regression prediction error in train:\",1-mean(yhat==y),\"\\n\")\n\n\nLinear regression prediction error in train: 0.2975 \n\nWe can quickly obtain predicted values for any set of values using the predict function:\n\n\nyhat <- predict(fit1,newdata=data.frame(X1=newx[,1],X2=newx[,2]))\n\n\n\nNow we can create a plot showing where we predict 1s and where we predict 0s, as well as the boundary. We can also use the predict function to obtain predicted values for our test set. Note that nowhere do we fit the model on the test set:\n\n\ncolshat <- yhat\ncolshat[yhat>=0.5] <- mycols[2]\ncolshat[yhat<0.5] <- mycols[1]\nm <- -fit1$coef[2]/fit1$coef[3] #boundary slope\nb <- (0.5 - fit1$coef[1])/fit1$coef[3] #boundary intercept\n##prediction on test\nyhat <- predict(fit1,newdata=data.frame(X1=test[,1],X2=test[,2]))\nyhat <- as.numeric(yhat>0.5)\ncat(\"Linear regression prediction error in test:\",1-mean(yhat==ytest),\"\\n\")\n\n\nLinear regression prediction error in test: 0.32 \n\nplot(test,type=\"n\",xlab=\"X1\",ylab=\"X2\",xlim=XLIM,ylim=YLIM)\nabline(b,m)\npoints(newx,col=colshat,pch=16,cex=0.35)\n##test was created in the code (not shown) for the first plot\npoints(test,bg=cols,pch=21)\n\n\n\n\n(#fig:regression_prediction)We estimate the probability of 1 with a linear regression model with X1 and X2 as predictors. The resulting prediction map is divided into parts that are larger than 0.5 (red) and lower than 0.5 (blue).\n\n\n\nThe error rates in the test and train sets are quite similar. Thus, we do not seem to be over-training. This is not surprising as we are fitting a 2 parameter model to 400 data points. However, note that the boundary is a line. Because we are fitting a plane to the data, there is no other option here. The linear regression method is too rigid. The rigidity makes it stable and avoids over-training, but it also keeps the model from adapting to the non-linear relationship between \\(Y\\) and \\(X\\). We saw this before in the smoothing section. The next ML technique we consider is similar to the smoothing techniques described before.\nK-nearest neighbor\nK-nearest neighbors (kNN) is similar to bin smoothing, but it is easier to adapt to multiple dimensions. Basically, for any point \\(x\\) for which we want an estimate, we look for the k nearest points and then take an average of these points. This gives us an estimate of \\(f(x_1,x_2)\\), just like the bin smoother gave us an estimate of a curve. We can now control flexibility through \\(k\\). Here we compare \\(k=1\\) and \\(k=100\\).\n\n\nlibrary(class)\nmypar(2,2)\nfor(k in c(1,100)){\n  ##predict on train\n  yhat <- knn(x,x,y,k=k)\n  cat(\"KNN prediction error in train:\",1-mean((as.numeric(yhat)-1)==y),\"\\n\")\n  ##make plot\n  yhat <- knn(x,test,y,k=k)\n  cat(\"KNN prediction error in test:\",1-mean((as.numeric(yhat)-1)==ytest),\"\\n\")\n}\n\n\nKNN prediction error in train: 0 \nKNN prediction error in test: 0.3375 \nKNN prediction error in train: 0.2725 \nKNN prediction error in test: 0.3125 \n\nTo visualize why we make no errors in the train set and many errors in the test set when \\(k=1\\) and obtain more stable results from \\(k=100\\), we show the prediction regions (code not shown):\n\n\n\nFigure 13: Prediction regions obtained with kNN for k=1 (top) and k=200 (bottom). We show both train (left) and test data (right).\n\n\n\nWhen \\(k=1\\), we make no mistakes in the training test since every point is its closest neighbor and it is equal to itself. However, we see some islands of blue in the red area that, once we move to the test set, are more error prone. In the case \\(k=100\\), we do not have this problem and we also see that we improve the error rate over linear regression. We can also see that our estimate of \\(f(x_1,x_2)\\) is closer to the truth.\nBayes rule\nHere we include a comparison of the test and train set errors for various values of \\(k\\). We also include the error rate that we would make if we actually knew \\(\\mbox{E}(Y \\mid X_1=x1,X_2=x_2)\\) referred to as Bayes Rule.\nWe start by computing the error rates…\n\n\n###Bayes Rule\nyhat <- apply(test,1,p)\ncat(\"Bayes rule prediction error in train\",1-mean(round(yhat)==y),\"\\n\")\n\n\nBayes rule prediction error in train 0.2825 \n\nbayes.error=1-mean(round(yhat)==y)\ntrain.error <- rep(0,16)\ntest.error <- rep(0,16)\nfor(k in seq(along=train.error)){\n  ##predict on train\n  yhat <- knn(x,x,y,k=2^(k/2))\n  train.error[k] <- 1-mean((as.numeric(yhat)-1)==y)\n  ##prediction on test    \n  yhat <- knn(x,test,y,k=2^(k/2))\n  test.error[k] <- 1-mean((as.numeric(yhat)-1)==y)\n}\n\n\n\n… and then plot the error rates against values of \\(k\\). We also show the Bayes rules error rate as a horizontal line.\n\n\nks <- 2^(seq(along=train.error)/2)\nmypar()\nplot(ks,train.error,type=\"n\",xlab=\"K\",ylab=\"Prediction Error\",log=\"x\",\n     ylim=range(c(test.error,train.error)))\nlines(ks,train.error,type=\"b\",col=4,lty=2,lwd=2)\nlines(ks,test.error,type=\"b\",col=5,lty=3,lwd=2)\nabline(h=bayes.error,col=6)\nlegend(\"bottomright\",c(\"Train\",\"Test\",\"Bayes\"),col=c(4,5,6),lty=c(2,3,1),box.lwd=0)\n\n\n\n\n(#fig:bayes_rule2)Prediction error in train (pink) and test (green) versus number of neighbors. The yellow line represents what one obtains with Bayes Rule.\n\n\n\nNote that these error rates are random variables and have standard errors. In the next section we describe cross-validation which helps reduce some of this variability. However, even with this variability, the plot clearly shows the problem of over-fitting when using values lower than 20 and under-fitting with values above 100.\nSmoothing\nSmoothing is a very powerful technique used all across data analysis. It is designed to estimate \\(f(x)\\) when the shape is unknown, but assumed to be smooth. The general idea is to group data points that are expected to have similar expectations and compute the average, or fit a simple parametric model. We illustrate two smoothing techniques using a gene expression example.\nThe following data are gene expression measurements from replicated RNA samples.\n\n\n##Following three packages are available from Bioconductor\n\n# if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n  #  install.packages(\"BiocManager\")\n\n# BiocManager::install(\"Biobase\")\n# BiocManager::install(\"SpikeIn\")\n# BiocManager::install(\"hgu95acdf\")\n\nlibrary(Biobase)\nlibrary(SpikeIn)\nlibrary(hgu95acdf)\ndata(SpikeIn95)\n\n\n\nWe consider the data used in an MA-plot comparing two replicated samples (\\(Y\\) = log ratios and \\(X\\) = averages) and take down-sample in a way that balances the number of points for different strata of \\(X\\) (code not shown):\n\n\n\n\n\nlibrary(rafalib)\nmypar()\nplot(X,Y)\n\n\n\n\nFigure 14: MA-plot comparing gene expression from two arrays.\n\n\n\nIn the MA plot we see that \\(Y\\) depends on \\(X\\). This dependence must be a bias because these are based on replicates, which means \\(Y\\) should be 0 on average regardless of \\(X\\). We want to predict \\(f(x)=\\mbox{E}(Y \\mid X=x)\\) so that we can remove this bias. Linear regression does not capture the apparent curvature in \\(f(x)\\):\n\n\nmypar()\nplot(X,Y)\nfit <- lm(Y~X)\npoints(X,Y,pch=21,bg=ifelse(Y>fit$fitted,1,3))\nabline(fit,col=2,lwd=4,lty=2)\n\n\n\n\n(#fig:MAplot_with_regression_line)MA-plot comparing gene expression from two arrays with fitted regression line. The two colors represent positive and negative residuals.\n\n\n\nThe points above the fitted line (green) and those below (purple) are not evenly distributed. We therefore need an alternative more flexible approach.\nBin Smoothing\nInstead of fitting a line, let’s go back to the idea of stratifying and computing the mean. This is referred to as bin smoothing. The general idea is that the underlying curve is “smooth” enough so that, in small bins, the curve is approximately constant. If we assume the curve is constant, then all the \\(Y\\) in that bin have the same expected value. For example, in the plot below, we highlight points in a bin centered at 8.6, as well as the points of a bin centered at 12.1, if we use bins of size 1. We also show the fitted mean values for the \\(Y\\) in those bins with dashed lines (code not shown):\n\n\n\nFigure 15: MAplot comparing gene expression from two arrays with bin smoother fit shown for two points.\n\n\n\nBy computing this mean for bins around every point, we form an estimate of the underlying curve \\(f(x)\\). Below we show the procedure happening as we move from the smallest value of \\(x\\) to the largest. We show 10 intermediate cases as well (code not shown):\n\n\n\n(#fig:bin_smoothing_demo)Illustration of how bin smoothing estimates a curve. Showing 12 steps of process.\n\n\n\nThe final result looks like this (code not shown):\n\n\n\n(#fig:bin_smooth_final)MA-plot with curve obtained with bin-smoothed curve shown.\n\n\n\nThere are several functions in R that implement bin smoothers. One example is ksmooth. However, in practice, we typically prefer methods that use slightly more complicated models than fitting a constant. The final result above, for example, is somewhat wiggly. Methods such as loess, which we explain next, improve on this.\nLoess\nLocal weighted regression (loess) is similar to bin smoothing in principle. The main difference is that we approximate the local behavior with a line or a parabola. This permits us to expand the bin sizes, which stabilizes the estimates. Below we see lines fitted to two bins that are slightly larger than those we used for the bin smoother (code not shown). We can use larger bins because fitting lines provide slightly more flexibility.\n\n\n\nFigure 16: MA-plot comparing gene expression from two arrays with bin local regression fit shown for two points.\n\n\n\nAs we did for the bin smoother, we show 12 steps of the process that leads to a loess fit (code not shown):\n\n\n\n(#fig:loess_demo)Illustration of how loess estimates a curve. Showing 12 steps of the process.\n\n\n\nThe final result is a smoother fit than the bin smoother since we use larger sample sizes to estimate our local parameters (code not shown):\n\n\n\n(#fig:loess_final)MA-plot with curve obtained with loess.\n\n\n\nThe function loess performs this analysis for us:\n\n\nfit <- loess(Y~X, degree=1, span=1/3)\nnewx <- seq(min(X),max(X),len=100) \nsmooth <- predict(fit,newdata=data.frame(X=newx))\nmypar ()\nplot(X,Y,col=\"darkgrey\",pch=16)\nlines(newx,smooth,col=\"black\",lwd=3)\n\n\n\n\nFigure 17: Loess fitted with the loess function.\n\n\n\nThere are three other important differences between loess and the typical bin smoother. The first is that rather than keeping the bin size the same, loess keeps the number of points used in the local fit the same. This number is controlled via the span argument which expects a proportion. For example, if N is the number of data points and span=0.5, then for a given \\(x\\) , loess will use the 0.5*N closest points to \\(x\\) for the fit. The second difference is that, when fitting the parametric model to obtain \\(f(x)\\), loess uses weighted least squares, with higher weights for points that are closer to \\(x\\). The third difference is that loess has the option of fitting the local model robustly. An iterative algorithm is implemented in which, after fitting a model in one iteration, outliers are detected and downweighted for the next iteration. To use this option, we use the argument family=\"symmetric\".\nCross-validation\nHere we describe cross-validation: one of the fundamental methods in machine learning for method assessment and picking parameters in a prediction or machine learning task. Suppose we have a set of observations with many features and each observation is associated with a label. We will call this set our training data. Our task is to predict the label of any new samples by learning patterns from the training data. For a concrete example, let’s consider gene expression values, where each gene acts as a feature. We will be given a new set of unlabeled data (the test data) with the task of predicting the tissue type of the new samples.\nIf we choose a machine learning algorithm with a tunable parameter, we have to come up with a strategy for picking an optimal value for this parameter. We could try some values, and then just choose the one which performs the best on our training data, in terms of the number of errors the algorithm would make if we apply it to the samples we have been given for training. However, we have seen how this leads to over-fitting.\nLet’s start by loading the tissue gene expression dataset:\n\n\nlibrary(tissuesGeneExpression)\ndata(tissuesGeneExpression)\n\n\n\nFor illustration purposes, we will drop one of the tissues which doesn’t have many samples:\n\n\ntable(tissue)\n\n\ntissue\n cerebellum       colon endometrium hippocampus      kidney \n         38          34          15          31          39 \n      liver    placenta \n         26           6 \n\nind <- which(tissue != \"placenta\")\ny <- tissue[ind]\nX <- t( e[,ind] )\n\n\n\nThis tissue will not form part of our example.\nNow let’s try out k-nearest neighbors for classification, using \\(k=5\\). What is our average error in predicting the tissue in the training set, when we’ve used the same data for training and for testing?\n\n\nlibrary(class)\npred <- knn(train =  X, test = X, cl=y, k=5)\nmean(y != pred)\n\n\n[1] 0\n\nWe have no errors in prediction in the training set with \\(k=5\\). What if we use \\(k=1\\)?\n\n\npred <- knn(train=X, test=X, cl=y, k=1)\nmean(y != pred)\n\n\n[1] 0\n\nTrying to classify the same observations as we use to train the model can be very misleading. In fact, for k-nearest neighbors, using k=1 will always give 0 classification error in the training set, because we use the single observation to classify itself. The reliable way to get a sense of the performance of an algorithm is to make it give a prediction for a sample it has never seen. Similarly, if we want to know what the best value for a tunable parameter is, we need to see how different values of the parameter perform on samples, which are not in the training data.\nCross-validation is a widely-used method in machine learning, which solves this training and test data problem, while still using all the data for testing the predictive accuracy. It accomplishes this by splitting the data into a number of folds. If we have \\(N\\) folds, then the first step of the algorithm is to train the algorithm using \\((N-1)\\) of the folds, and test the algorithm’s accuracy on the single left-out fold. This is then repeated N times until each fold has been used as in the test set. If we have \\(M\\) parameter settings to try out, then this is accomplished in an outer loop, so we have to fit the algorithm a total of \\(N \\times M\\) times.\nWe will use the createFolds function from the caret package to make 5 folds of our gene expression data, which are balanced over the tissues. Don’t be confused by the fact that the createFolds function uses the same letter ‘k’ as the ‘k’ in k-nearest neighbors. These ‘k’ are totally unrelated. The caret function createFolds is asking for how many folds to create, the \\(N\\) from above. The ‘k’ in the knn function is for how many closest observations to use in classifying a new sample. Here we will create 10 folds:\n\n\nlibrary(caret)\nset.seed(1)\nidx <- createFolds(y, k=10)\nsapply(idx, length)\n\n\nFold01 Fold02 Fold03 Fold04 Fold05 Fold06 Fold07 Fold08 Fold09 Fold10 \n    19     19     18     18     19     18     20     16     16     20 \n\nThe folds are returned as a list of numeric indices. The first fold of data is therefore:\n\n\ny[idx[[1]]] ##the labels\n\n\n [1] \"kidney\"      \"hippocampus\" \"hippocampus\" \"hippocampus\"\n [5] \"cerebellum\"  \"cerebellum\"  \"cerebellum\"  \"kidney\"     \n [9] \"kidney\"      \"kidney\"      \"colon\"       \"colon\"      \n[13] \"colon\"       \"liver\"       \"endometrium\" \"endometrium\"\n[17] \"liver\"       \"liver\"       \"cerebellum\" \n\nhead( X[idx[[1]], 1:3] ) ##the genes (only showing the first 3 genes...)\n\n\n                1007_s_at  1053_at   117_at\nGSM12105.CEL.gz  9.913031 6.337478 7.983850\nGSM21209.cel.gz 11.667431 5.785190 7.666343\nGSM21215.cel.gz 10.361340 5.663634 7.328729\nGSM21218.cel.gz 10.757734 5.984170 8.525524\nGSM87066.cel.gz  9.746007 5.886079 7.459517\nGSM87085.cel.gz  9.864295 5.753874 7.712646\n\nWe can see that, in fact, the tissues are fairly equally represented across the 10 folds:\n\n\nsapply(idx, function(i) table(y[i]))\n\n\n            Fold01 Fold02 Fold03 Fold04 Fold05 Fold06 Fold07 Fold08\ncerebellum       4      4      4      4      4      3      4      3\ncolon            3      4      4      3      3      3      4      3\nendometrium      2      1      1      1      2      2      2      1\nhippocampus      3      3      3      3      3      3      3      3\nkidney           4      4      4      4      4      4      4      4\nliver            3      3      2      3      3      3      3      2\n            Fold09 Fold10\ncerebellum       4      4\ncolon            3      4\nendometrium      1      2\nhippocampus      3      4\nkidney           3      4\nliver            2      2\n\nBecause tissues have very different gene expression profiles, predicting tissue with all genes will be very easy. For illustration purposes we will try to predict tissue type with just two dimensional data. We will reduce the dimension of our data using cmdscale:\n\n\nlibrary(rafalib)\nmypar()\nXsmall <- cmdscale(dist(X))\nplot(Xsmall,col=as.fumeric(y))\nlegend(\"topleft\",levels(factor(y)),fill=seq_along(levels(factor(y))))\n\n\n\n\nFigure 18: First two PCs of the tissue gene expression data with color representing tissue. We use these two PCs as our two predictors throughout.\n\n\n\nNow we can try out the k-nearest neighbors method on a single fold. We provide the knn function with all the samples in Xsmall except those which are in the first fold. We remove these samples using the code -idx[[1]] inside the square brackets. We then use those samples in the test set. The cl argument is for the true classifications or labels (here, tissue) of the training data. We use 5 observations to classify in our k-nearest neighbor algorithm:\n\n\npred <- knn(train=Xsmall[ -idx[[1]] , ], test=Xsmall[ idx[[1]], ], cl=y[ -idx[[1]] ], k=5)\ntable(true=y[ idx[[1]] ], pred)\n\n\n             pred\ntrue          cerebellum colon endometrium hippocampus kidney liver\n  cerebellum           4     0           0           0      0     0\n  colon                0     2           0           0      1     0\n  endometrium          0     0           2           0      0     0\n  hippocampus          2     0           0           1      0     0\n  kidney               0     0           0           0      4     0\n  liver                0     0           0           0      0     3\n\nmean(y[ idx[[1]] ] != pred)\n\n\n[1] 0.1578947\n\nNow we have some misclassifications. How well do we do for the rest of the folds?\n\n\nfor (i in 1:10) {\n  pred <- knn(train=Xsmall[ -idx[[i]] , ], test=Xsmall[ idx[[i]], ], cl=y[ -idx[[i]] ], k=5)\n  print(paste0(i,\") error rate: \", round(mean(y[ idx[[i]] ] != pred),3)))\n}\n\n\n[1] \"1) error rate: 0.158\"\n[1] \"2) error rate: 0.158\"\n[1] \"3) error rate: 0.278\"\n[1] \"4) error rate: 0.167\"\n[1] \"5) error rate: 0.158\"\n[1] \"6) error rate: 0.167\"\n[1] \"7) error rate: 0.25\"\n[1] \"8) error rate: 0.188\"\n[1] \"9) error rate: 0.062\"\n[1] \"10) error rate: 0.1\"\n\nSo we can see there is some variation for each fold, with error rates hovering around 0.1-0.3. But is k=5 the best setting for the k parameter? In order to explore the best setting for k, we need to create an outer loop, where we try different values for k, and then calculate the average test set error across all the folds.\nWe will try out each value of k from 1 to 12. Instead of using two for loops, we will use sapply:\n\n\nset.seed(1)\nks <- 1:12\nres <- sapply(ks, function(k) {\n  ##try out each version of k from 1 to 12\n  res.k <- sapply(seq_along(idx), function(i) {\n    ##loop over each of the 10 cross-validation folds\n    ##predict the held-out samples using k nearest neighbors\n    pred <- knn(train=Xsmall[ -idx[[i]], ],\n                test=Xsmall[ idx[[i]], ],\n                cl=y[ -idx[[i]] ], k = k)\n    ##the ratio of misclassified samples\n    mean(y[ idx[[i]] ] != pred)\n  })\n  ##average over the 10 folds\n  mean(res.k)\n})\n\n\n\nNow for each value of k, we have an associated test set error rate from the cross-validation procedure.\n\n\nres\n\n\n [1] 0.1882164 0.1692032 0.1694664 0.1639108 0.1511184 0.1586184\n [7] 0.1589108 0.1865058 0.1880482 0.1714108 0.1759795 0.1744664\n\nWe can then plot the error rate for each value of k, which helps us to see in what region there might be a minimal error rate:\n\n\nplot(ks, res, type=\"o\",ylab=\"misclassification error\")\n\n\n\n\n(#fig:misclassification_error)Misclassification error versus number of neighbors.\n\n\n\nRemember, because the training set is a random sample and because our fold-generation procedure involves random number generation, the “best” value of k we pick through this procedure is also a random variable. If we had new training data and if we recreated our folds, we might get a different value for the optimal k.\nFinally, to show that gene expression can perfectly predict tissue, we use 5 dimensions instead of 2, which results in perfect prediction:\n\n\nXsmall <- cmdscale(dist(X),k=5)\nset.seed(1)\nks <- 1:12\nres <- sapply(ks, function(k) {\n  res.k <- sapply(seq_along(idx), function(i) {\n    pred <- knn(train=Xsmall[ -idx[[i]], ],\n                test=Xsmall[ idx[[i]], ],\n                cl=y[ -idx[[i]] ], k = k)\n    mean(y[ idx[[i]] ] != pred)\n  })\n  mean(res.k)\n})\nplot(ks, res, type=\"o\",ylim=c(0,0.20),ylab=\"misclassification error\")\n\n\n\n\n(#fig:misclassification_error2)Misclassification error versus number of neighbors when we use 5 dimensions instead of 2.\n\n\n\nImportant note: we applied cmdscale to the entire dataset to create a smaller one for illustration purposes. However, in a real machine learning application, this may result in an underestimation of test set error for small sample sizes, where dimension reduction using the unlabeled full dataset gives a boost in performance. A safer choice would have been to transform the data separately for each fold, by calculating a rotation and dimension reduction using the training set only and applying this to the test set.\nWeek 4: Batch Effects\nConfounding\nBatch effects have the most devastating effects when they are confounded with outcomes of interest. Here we describe confounding and how it relates to data interpretation.\n“Correlation is not causation” is one of the most important lessons you should take from this or any other data analysis course. A common example for why this statement is so often true is confounding. Simply stated confounding occurs when we observe a correlation or association between \\(X\\) and \\(Y\\), but this is strictly the result of both \\(X\\) and \\(Y\\) depending on an extraneous variable \\(Z\\). Here we describe Simpson’s paradox, an example based on a famous legal case, and an example of confounding in high-throughput biology.\nExample of Simpson’s Paradox\nAdmission data from U.C. Berkeley 1973 showed that more men were being admitted than women: 44% men were admitted compared to 30% women.See: PJ Bickel, EA Hammel, and JW O’Connell. Science (1975). Here is the data:\n\n\n#install_github(\"genomicsclass/dagdata\")\nlibrary(dagdata)\ndata(admissions)\nadmissions$total=admissions$Percent*admissions$Number/100\n##percent men get in\nsum(admissions$total[admissions$Gender==1]/sum(admissions$Number[admissions$Gender==1]))\n\n\n[1] 0.4451951\n\n##percent women get in\nsum(admissions$total[admissions$Gender==0]/sum(admissions$Number[admissions$Gender==0]))\n\n\n[1] 0.3033351\n\nA chi-square test clearly rejects the hypothesis that gender and admission are independent:\n\n\n##make a 2 x 2 table\nindex = admissions$Gender==1\nmen = admissions[index,]\nwomen = admissions[!index,]\nmenYes = sum(men$Number*men$Percent/100)\nmenNo = sum(men$Number*(1-men$Percent/100))\nwomenYes = sum(women$Number*women$Percent/100)\nwomenNo = sum(women$Number*(1-women$Percent/100))\ntab = matrix(c(menYes,womenYes,menNo,womenNo),2,2)\nprint(chisq.test(tab)$p.val)\n\n\n[1] 9.139492e-22\n\nBut closer inspection shows a paradoxical result. Here are the percent admissions by major:\n\n\ny=cbind(admissions[1:6,c(1,3)],admissions[7:12,3])\ncolnames(y)[2:3]=c(\"Male\",\"Female\")\ny\n\n\n  Major Male Female\n1     A   62     82\n2     B   63     68\n3     C   37     34\n4     D   33     35\n5     E   28     24\n6     F    6      7\n\nNotice that we no longer see a clear gender bias. The chi-square test we performed above suggests a dependence between admission and gender. Yet when the data is grouped by major, this dependence seems to disappear. What’s going on?\nThis is an example of Simpson’s paradox. A plot showing the percentages that applied to a major against the percent that get into that major, for males and females starts to point to an explanation.\n\n\ny=cbind(admissions[1:6,5],admissions[7:12,5])\ny=sweep(y,2,colSums(y),\"/\")*100\nx=rowMeans(cbind(admissions[1:6,3],admissions[7:12,3]))\nlibrary(rafalib)\nmypar()\nmatplot(x,y,xlab=\"percent that gets in the major\",\n        ylab=\"percent that applies to major\",\n        col=c(\"blue\",\"red\"),cex=1.5)\nlegend(\"topleft\",c(\"Male\",\"Female\"),col=c(\"blue\",\"red\"),pch=c(\"1\",\"2\"),box.lty=0)\n\n\n\n\n(#fig:hard_major_confounding)Percent of students that applied versus percent that were admitted by gender.\n\n\n\nWhat the plot suggests is that males were much more likely to apply to “easy” majors. The plot shows that males and “easy” majors are confounded.\nConfounding explained graphically\nHere we visualize the confounding. In the plots below, each letter represents a person. Accepted individuals are denoted in green and not admitted in orange. The letter indicates the major. In this first plot we group all the students together and notice that the proportion of green is larger for men.\n\n\n\n(#fig:simpsons_paradox_illustration)Admitted are in green and majors are denoted with letters. Here we clearly see that more males were admitted.\n\n\n\nNow we stratify the data by major. The key point here is that most of the accepted men (green) come from the easy majors: A and B.\n\n\n\n(#fig:simpsons_paradox_illustration2)Simpon’s Paradox illustrated. Admitted students are in green. Students are now stratified by the major to which they applied.\n\n\n\nAverage after stratifying\nIn this plot, we can see that if we condition or stratify by major, and then look at differences, we control for the confounder and this effect goes away.\n\n\ny=cbind(admissions[1:6,3],admissions[7:12,3])\nmatplot(1:6,y,xaxt=\"n\",xlab=\"major\",ylab=\"percent\",col=c(\"blue\",\"red\"),cex=1.5)\naxis(1,1:6,LETTERS[1:6])\nlegend(\"topright\",c(\"Male\",\"Female\"),col=c(\"blue\",\"red\"),pch=c(\"1\",\"2\"),\n       box.lty=0)\n\n\n\n\n(#fig:admission_by_major)Admission percentage by major for each gender.\n\n\n\nThe average difference by major is actually 3.5% higher for women.\n\n\nmean(y[,1]-y[,2])\n\n\n[1] -3.5\n\nSimpson’s Paradox in baseball\nSimpson’s Paradox is commonly seen in baseball statistics. Here is a well known example in which David Justice had a higher batting average than Derek Jeter in both 1995 and 1996, but Jeter had a higher overall average:\n\n1995\n1996\nCombined\nDerek Jeter\n12/48 (.250)\n183/582 (.314)\n195/630 (.310)\nDavid Justice\n104/411 (.253)\n45/140 (.321)\n149/551 (.270)\nThe confounder here is games played. Jeter played more games during the year he batted better, while the opposite is true for Justice.\n\n\n\n",
    "preview": "posts/20210810 High Dimension Data /High-Dimension-Data_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-08-15T10:10:34+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210729 Multiple ANOVA in R for different variables/",
    "title": "Multiple ANOVA in R",
    "description": "What if I have many variables to compare?",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-07-29",
    "categories": [],
    "contents": "\n\nContents\nLoad Packages\nLoad dataset\nVisualization\nAnova\nReferences\n\nI was looking at a dateset at work, and was wondering how I can carry out t-test to check if there were any significant difference in flavor compounds between two different species of the same fruit.\nPreviously, I looked at how to carry out t-tests efficiently, using rstatix package.\nFor ANOVA, I can use purrr to carry out iterative statistical testing. If I want to look at differences in chemical groups for 3 different types of stinky tofu, the strategy would be to calculate the % area for each group (analysed as triplicates), and then nest by the chemical group, carry out anova test, and then create a new column to show p-values.\nI used fictitious data, but is based on what I saw from: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6321173/\nLoad Packages\n\n\nlibrary(tidyverse)\nlibrary(datapasta)\n\n\n\nLoad dataset\n\n\ntofu <- \n  tibble::tribble(\n          ~group_no,             ~groups,   ~a1,   ~a2,   ~a3,   ~b1,   ~b2,  ~b3,  ~c1,   ~c2,  ~c3,\n                 1L,           \"phenols\", 23.32, 23.56, 22.97, 47.87,  46.6, 46.9, 9.93, 10.21, 9.79,\n                 2L,            \"ethers\",  0.02,  0.03,  0.01,  0.99,  0.94, 0.97, 14.3,  14.4, 14.3,\n                 3L, \"aldehydes_ketones\",  0.21,  0.24,  0.25,  0.02,  0.02, 0.01, 0.11,  0.12, 0.11,\n                 4L,           \"indoles\", 22.99,  22.9, 22.93,  35.2, 35.22, 35.9,  5.5,   5.3,  5.4\n          )\n\n\ntofu\n\n\n# A tibble: 4 × 11\n  group_no groups         a1    a2    a3    b1    b2    b3    c1    c2\n     <int> <chr>       <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1        1 phenols     23.3  23.6  23.0  47.9  46.6  46.9   9.93 10.2 \n2        2 ethers       0.02  0.03  0.01  0.99  0.94  0.97 14.3  14.4 \n3        3 aldehydes_…  0.21  0.24  0.25  0.02  0.02  0.01  0.11  0.12\n4        4 indoles     23.0  22.9  22.9  35.2  35.2  35.9   5.5   5.3 \n# … with 1 more variable: c3 <dbl>\n\nVisualization\n\n\ntofu_long <- tofu %>% \n  pivot_longer(cols = a1:c3,\n               names_to = \"sample\",\n               values_to = \"percent_area\") %>% \n  mutate(sample_cleaned = fct_recode(sample,\n                                     a = \"a1\",\n                                     a = \"a2\",\n                                     a = \"a3\",\n                                     b = \"b1\",\n                                     b = \"b2\",\n                                     b = \"b3\",\n                                     c = \"c1\",\n                                     c = \"c2\",\n                                     c = \"c3\"))\n\ntofu_long%>% \n  group_by(sample_cleaned, groups) %>% \n  summarise(av_percent_area = mean(percent_area)) %>% \n  ggplot(aes(sample_cleaned, av_percent_area)) +\n  geom_boxplot(aes(fill = sample_cleaned)) +\n  facet_wrap( ~ groups, scales = \"free\") +\n  labs(x = \"\") +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\nAnova\n\n\nlibrary(broom)\n\nresults_anova <- tofu_long %>% \n  nest(-group_no) %>% \n  mutate(model = map(data, ~aov(percent_area ~ sample_cleaned, .)),\n         tidy = map(model, tidy)) %>% \n  select(group_no, tidy) %>% \n  unnest(cols = c(tidy)) %>% \n  filter(term != \"Residuals\")\n\nresults_anova\n\n\n# A tibble: 4 × 7\n  group_no term              df     sumsq    meansq statistic  p.value\n     <int> <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>\n1        1 sample_cleaned     2 2125.     1063.         5550. 1.58e-10\n2        2 sample_cleaned     2  384.      192.       141800. 9.47e-15\n3        3 sample_cleaned     2    0.0707    0.0353      212. 2.71e- 6\n4        4 sample_cleaned     2 1366.      683.        11992. 1.56e-11\n\nresults_anova %>% \n  filter(p.value<0.05)\n\n\n# A tibble: 4 × 7\n  group_no term              df     sumsq    meansq statistic  p.value\n     <int> <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>\n1        1 sample_cleaned     2 2125.     1063.         5550. 1.58e-10\n2        2 sample_cleaned     2  384.      192.       141800. 9.47e-15\n3        3 sample_cleaned     2    0.0707    0.0353      212. 2.71e- 6\n4        4 sample_cleaned     2 1366.      683.        11992. 1.56e-11\n\nFrom the analysis, all three compounds differed significantly for all four chemical groups. This is a efficient way of carrying out ANOVA test.\nThe analysis strategy is to nest, mutate, unnest, to efficiently extract the p.values. The Tukey’s post-hoc comparison can be carried out in a similar way.\nReferences\nhttp://ritsokiguess.site/docs/2018/01/30/tidy-simple-effects-in-analysis-of-variance/ https://towardsdatascience.com/anova-in-r-4a4a4edc9448\n\n\n\n",
    "preview": "posts/20210729 Multiple ANOVA in R for different variables/Multiple-ANOVA-in-R-for-different-variables_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-08-10T19:23:25+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210728_Tidytuesday ramen ratings/",
    "title": "Tidy Tuesday Series",
    "description": "20190614 - Ramen Ratings Data",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-07-28",
    "categories": [],
    "contents": "\n\nContents\nBackground\nLoad Packages and Data\nExploratory data\nWhat are the types of variables?\nCheck for missing data:\nWhat is in each variable?\nBrand\nVariety\nCountry\n\nStyle\nStars\nPackaging\n\nFinal dataset:\nVisualize\nTop 10 (by country)\nTop 20 brands, by variety?\nTop Brands\nMean ratings by country\nWordcloud for popular flavors\n\nReferences\n\nBackground\nThis ramen dataset was taken from one of the TidyTuesday datasets released in 2019. https://github.com/rfordatascience/tidytuesday/tree/master/data/2019/2019-06-04. I was halfway through the data exploration step, and went into the original source of data at The Ramen Rater https://www.theramenrater.com/resources-2/the-list/, only to realise that there is a even more updated list. As such, I will use the list compiled in 2021 for exploratory work.\nThe Ramen Rater rates the different ramen (or instant noodles) from different parts of the world. Let’s see what kind of insights we can have from his dataset.\nLoad Packages and Data\n\n\nlibrary(tidyverse)\nlibrary(ggthemes)\nlibrary(skimr)\nlibrary(janitor)\nlibrary(datapasta)\nlibrary(naniar)\nlibrary(tidytext)\nlibrary(wordcloud2)\n\n# Import data \n\n# Old\nramen_ratings <- read_csv(\"https://raw.githubusercontent.com/stephen-haslett/FALL2019TIDYVERSE/master/ramen-ratings.csv\", show_col_types = F)\n\nglimpse(ramen_ratings)\n\n\nRows: 2,580\nColumns: 7\n$ `Review #` <dbl> 2580, 2579, 2578, 2577, 2576, 2575, 2574, 2573, 2…\n$ Brand      <chr> \"New Touch\", \"Just Way\", \"Nissin\", \"Wei Lih\", \"Ch…\n$ Variety    <chr> \"T's Restaurant Tantanmen\", \"Noodles Spicy Hot Se…\n$ Style      <chr> \"Cup\", \"Pack\", \"Cup\", \"Pack\", \"Pack\", \"Pack\", \"Cu…\n$ Country    <chr> \"Japan\", \"Taiwan\", \"USA\", \"Taiwan\", \"India\", \"Sou…\n$ Stars      <chr> \"3.75\", \"1\", \"2.25\", \"2.75\", \"3.75\", \"4.75\", \"4\",…\n$ `Top Ten`  <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\n# Updated on 17 Jan 2021\nupdated_ramen_ratings <- readxl::read_xlsx(\"The-Big-List-20210117.xlsx\")\n\nglimpse(updated_ramen_ratings) # more data, to use this instead\n\n\nRows: 3,702\nColumns: 7\n$ `Review #` <dbl> 3702, 3701, 3700, 3699, 3698, 3697, 3696, 3695, 3…\n$ Brand      <chr> \"Higashimaru\", \"Single Grain\", \"Sau Tao\", \"Sau Ta…\n$ Variety    <chr> \"Seafood Sara Udon\", \"Chongqing Spicy & Sour Rice…\n$ Style      <chr> \"Pack\", \"Cup\", \"Pack\", \"Pack\", \"Cup\", \"Cup\", \"Pac…\n$ Country    <chr> \"Japan\", \"China\", \"Hong Kong\", \"Hong Kong\", \"Japa…\n$ Stars      <chr> \"5\", \"3.5\", \"5\", \"4.5\", \"3.5\", \"4.5\", \"4\", \"5\", \"…\n$ T          <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, N…\n\nExploratory data\nWhat are the types of variables?\n\n\n# Editing raw dataset for use\nramen <- updated_ramen_ratings %>% \n  clean_names() %>% # changing column names\n  mutate(brand = factor(brand),\n         country = factor(country),\n         style = factor(style)) %>% \n  select(-t) # used to be top 10, but will remove this since there are no values\n\nglimpse(ramen)\n\n\nRows: 3,702\nColumns: 6\n$ review_number <dbl> 3702, 3701, 3700, 3699, 3698, 3697, 3696, 3695…\n$ brand         <fct> \"Higashimaru\", \"Single Grain\", \"Sau Tao\", \"Sau…\n$ variety       <chr> \"Seafood Sara Udon\", \"Chongqing Spicy & Sour R…\n$ style         <fct> Pack, Cup, Pack, Pack, Cup, Cup, Pack, Pack, B…\n$ country       <fct> Japan, China, Hong Kong, Hong Kong, Japan, Chi…\n$ stars         <chr> \"5\", \"3.5\", \"5\", \"4.5\", \"3.5\", \"4.5\", \"4\", \"5\"…\n\nsize of dataset: 3702 reviews (observations), 6 variables (review #, brand, variety, style, country, stars)\nCheck for missing data:\n\n\nsapply(ramen, function(x) sum(is.na(x))) # no missing data\n\n\nreview_number         brand       variety         style       country \n            0             0             0             0             0 \n        stars \n            0 \n\nWhat is in each variable?\nBrand\n\n\n# brand, variety, style, country, stars, top 10\n\n# 543 different brands\nramen %>% \n  select(brand) %>% \n  unique()\n\n\n# A tibble: 543 × 1\n   brand          \n   <fct>          \n 1 Higashimaru    \n 2 Single Grain   \n 3 Sau Tao        \n 4 Sapporo Ichiban\n 5 Sichuan Baijia \n 6 Nissin         \n 7 Maruchan       \n 8 Yamamoto Seifun\n 9 Kenko Foods    \n10 Acecook        \n# … with 533 more rows\n\n# brands with most varieties\nramen %>% \n  group_by(brand) %>% \n  summarise(n = n()) %>% \n  arrange(desc(n))\n\n\n# A tibble: 543 × 2\n   brand               n\n   <fct>           <int>\n 1 Nissin            477\n 2 Maruchan          131\n 3 Nongshim          119\n 4 Myojo             111\n 5 Samyang Foods     103\n 6 Paldo              84\n 7 Mama               71\n 8 Sapporo Ichiban    69\n 9 Indomie            56\n10 Ottogi             51\n# … with 533 more rows\n\n# top 20 brands with most varieties\nramen %>% \n  group_by(brand) %>% \n  summarise(n = n()) %>% \n  arrange(desc(n)) %>% \n  slice_head(n = 20) # top 20\n\n\n# A tibble: 20 × 2\n   brand               n\n   <fct>           <int>\n 1 Nissin            477\n 2 Maruchan          131\n 3 Nongshim          119\n 4 Myojo             111\n 5 Samyang Foods     103\n 6 Paldo              84\n 7 Mama               71\n 8 Sapporo Ichiban    69\n 9 Indomie            56\n10 Ottogi             51\n11 Acecook            48\n12 Sau Tao            48\n13 KOKA               39\n14 Maggi              38\n15 Vifon              36\n16 MyKuali            35\n17 Lucky Me!          34\n18 Mamee              34\n19 Vina Acecook       34\n20 MAMA               33\n\n# which country are these top 20 brands located?\nramen %>% \n  group_by(brand, country) %>% \n  summarise(n = n()) %>% \n  arrange(desc(n))  # 1 brand can be located in different countries\n\n\n# A tibble: 623 × 3\n# Groups:   brand [543]\n   brand         country           n\n   <fct>         <fct>         <int>\n 1 Nissin        Japan           144\n 2 Nissin        United States   115\n 3 Samyang Foods South Korea     100\n 4 Nissin        Hong Kong        82\n 5 Paldo         South Korea      82\n 6 Myojo         Japan            77\n 7 Maruchan      Japan            65\n 8 Nongshim      South Korea      65\n 9 Maruchan      United States    64\n10 Mama          Thailand         58\n# … with 613 more rows\n\nVariety\n\n\nglimpse(ramen)\n\n\nRows: 3,702\nColumns: 6\n$ review_number <dbl> 3702, 3701, 3700, 3699, 3698, 3697, 3696, 3695…\n$ brand         <fct> \"Higashimaru\", \"Single Grain\", \"Sau Tao\", \"Sau…\n$ variety       <chr> \"Seafood Sara Udon\", \"Chongqing Spicy & Sour R…\n$ style         <fct> Pack, Cup, Pack, Pack, Cup, Cup, Pack, Pack, B…\n$ country       <fct> Japan, China, Hong Kong, Hong Kong, Japan, Chi…\n$ stars         <chr> \"5\", \"3.5\", \"5\", \"4.5\", \"3.5\", \"4.5\", \"4\", \"5\"…\n\n# now, look at variety\n\nramen %>% \n  group_by(variety) %>% \n  summarise(n = n()) %>% \n  arrange(desc(n))\n\n\n# A tibble: 3,448 × 2\n   variety                               n\n   <chr>                             <int>\n 1 Miso Ramen                            9\n 2 Beef                                  7\n 3 Chicken                               7\n 4 Yakisoba                              7\n 5 Artificial Chicken                    6\n 6 Vegetable                             6\n 7 Artificial Beef Flavor                4\n 8 Artificial Spicy Beef                 4\n 9 Chicken Flavor                        4\n10 Chili Chicken Flavour Noodle Soup     4\n# … with 3,438 more rows\n\n# variety: different names, may need to clean up to standardize flavor\n\n\n\nVariety would mean flavors.\nCountry\n\n\nglimpse(ramen)\n\n\nRows: 3,702\nColumns: 6\n$ review_number <dbl> 3702, 3701, 3700, 3699, 3698, 3697, 3696, 3695…\n$ brand         <fct> \"Higashimaru\", \"Single Grain\", \"Sau Tao\", \"Sau…\n$ variety       <chr> \"Seafood Sara Udon\", \"Chongqing Spicy & Sour R…\n$ style         <fct> Pack, Cup, Pack, Pack, Cup, Cup, Pack, Pack, B…\n$ country       <fct> Japan, China, Hong Kong, Hong Kong, Japan, Chi…\n$ stars         <chr> \"5\", \"3.5\", \"5\", \"4.5\", \"3.5\", \"4.5\", \"4\", \"5\"…\n\n# 51 different countries\n# Japan, USA, South Korea have the most number of products\n\nramen %>% \n  group_by(country) %>% \n  summarise(n = n()) %>% \n  arrange(desc(n))\n\n\n# A tibble: 51 × 2\n   country           n\n   <fct>         <int>\n 1 Japan           684\n 2 United States   462\n 3 South Korea     413\n 4 Taiwan          372\n 5 China           245\n 6 Thailand        212\n 7 Malaysia        208\n 8 Hong Kong       191\n 9 Indonesia       161\n10 Singapore       140\n# … with 41 more rows\n\nProducts from Singapore\n\n\n# singapore has 140 listed products\n ramen %>% \n  filter(country == \"Singapore\") %>% \n  arrange(desc(stars))\n\n\n# A tibble: 140 × 6\n   review_number brand    variety                  style country stars\n           <dbl> <fct>    <chr>                    <fct> <fct>   <chr>\n 1          3446 Prima T… Singapore Black Pepper … Pack  Singap… 5    \n 2          3210 Prima T… Singapore Prawn Soup La… Pack  Singap… 5    \n 3          3196 Prima T… Singapore Chilli Crab F… Pack  Singap… 5    \n 4          3096 Prima T… Singapore Black Pepper … Pack  Singap… 5    \n 5          2921 Prima T… Singapore Prawn Soup Wh… Pack  Singap… 5    \n 6          2882 Miandom  Tasty Asia Green Curry … Cup   Singap… 5    \n 7          2688 KOKA     Silk Laksa Singapura In… Bowl  Singap… 5    \n 8          2625 Nissin   Cup Noodles Potato Chip… Pack  Singap… 5    \n 9          2618 KOKA     Delight Curry Flavor In… Pack  Singap… 5    \n10          2617 Nissin   Cup Noodles Laksa Flavo… Cup   Singap… 5    \n# … with 130 more rows\n\nProducts from Japan\n\n\n# Japan has 684 listed products\n ramen %>% \n  filter(country == \"Japan\") %>% \n  arrange(desc(stars))\n\n\n# A tibble: 684 × 6\n   review_number brand     variety                 style country stars\n           <dbl> <fct>     <chr>                   <fct> <fct>   <chr>\n 1          3150 Hakubaku  Baby Somen              Pack  Japan   NR   \n 2          3149 Hakubaku  Baby Udon               Pack  Japan   NR   \n 3          2641 Nanoblock Ramen Bokki             Pack  Japan   NR   \n 4          3702 Higashim… Seafood Sara Udon       Pack  Japan   5    \n 5          3695 Maruchan  Miyashi Chuka Cold Noo… Pack  Japan   5    \n 6          3683 Sapporo … Sekai no Yamachan Phan… Tray  Japan   5    \n 7          3557 Nakaki F… Salt Yakisoba           Pack  Japan   5    \n 8          3554 Myojo     Vegetable Paitan Tanmen Bowl  Japan   5    \n 9          3525 Maruchan  Oshima x Tanaka Shoten… Bowl  Japan   5    \n10          3521 Maruchan  Sesame Tan Tan Udon     Bowl  Japan   5    \n# … with 674 more rows\n\nProducts from Taiwan\n\n\n# Taiwan has 372 listed products\n ramen %>% \n  filter(country == \"Taiwan\") %>% \n  arrange(desc(stars))\n\n\n# A tibble: 372 × 6\n   review_number brand         variety             style country stars\n           <dbl> <fct>         <chr>               <fct> <fct>   <chr>\n 1          3673 Noodles Acco… Fragrant In Origin… Pack  Taiwan  5    \n 2          3638 Hi-Lai Foods  Lai Noodle          Pack  Taiwan  5    \n 3          3618 PLN Food Co,… Spicy Paste Noodle  Pack  Taiwan  5    \n 4          3617 PLN Food Co,… Classic Dry Noodle  Pack  Taiwan  5    \n 5          3553 Mom's Dry No… Sichuan Spicy Duck… Box   Taiwan  5    \n 6          3523 Little Coupl… Dry Noodle Sesame … Pack  Taiwan  5    \n 7          3468 Little Coupl… Dry Noodle - Onion  Pack  Taiwan  5    \n 8          3413 Shin Horng    Hon's Dry Noodles … Pack  Taiwan  5    \n 9          3392 Wu Mu         Mandashi Mala Spic… Box   Taiwan  5    \n10          3390 Eight Field   Spicy Peanut Paste… Pack  Taiwan  5    \n# … with 362 more rows\n\nProducts from Korea\n\n\n# Korea has 413 listed products\n ramen %>% \n  filter(country %in% c(\"South Korea\")) %>% \n  arrange(desc(stars))\n\n\n# A tibble: 413 × 6\n   review_number brand     variety               style country   stars\n           <dbl> <fct>     <chr>                 <fct> <fct>     <chr>\n 1          2548 Ottogi    Plain Instant Noodle… Pack  South Ko… Unra…\n 2          2458 Samyang … Sari Ramen            Pack  South Ko… Unra…\n 3          3627 Ottogi    Mac & Cheese Spaghet… Bowl  South Ko… 5    \n 4          3451 Nongshim  Shin Light Air Dried… Pack  South Ko… 5    \n 5          3442 Paldo     Mr Kimchi Stirfried … Bowl  South Ko… 5    \n 6          3429 Paldo     Bul Jjamppong         Pack  South Ko… 5    \n 7          3427 Samyang … Buldak Light          Pack  South Ko… 5    \n 8          3404 Nongshim  Big Gomtang Instant … Bowl  South Ko… 5    \n 9          3398 Paldo     King Lid Ramen Noodl… Pack  South Ko… 5    \n10          3388 Paldo     Big 3 Instant Ramen … Bowl  South Ko… 5    \n# … with 403 more rows\n\nStyle\n\n\nglimpse(ramen)\n\n\nRows: 3,702\nColumns: 6\n$ review_number <dbl> 3702, 3701, 3700, 3699, 3698, 3697, 3696, 3695…\n$ brand         <fct> \"Higashimaru\", \"Single Grain\", \"Sau Tao\", \"Sau…\n$ variety       <chr> \"Seafood Sara Udon\", \"Chongqing Spicy & Sour R…\n$ style         <fct> Pack, Cup, Pack, Pack, Cup, Cup, Pack, Pack, B…\n$ country       <fct> Japan, China, Hong Kong, Hong Kong, Japan, Chi…\n$ stars         <chr> \"5\", \"3.5\", \"5\", \"4.5\", \"3.5\", \"4.5\", \"4\", \"5\"…\n\nramen %>% \n  count(style) %>% \n  arrange(desc(n))\n\n\n# A tibble: 8 × 2\n  style          n\n  <fct>      <int>\n1 Pack        2095\n2 Bowl         722\n3 Cup          659\n4 Tray         167\n5 Box           54\n6 Restaurant     3\n7 Bar            1\n8 Can            1\n\n# which are the missing ones?\nramen %>% \n  filter(is.na(style))\n\n\n# A tibble: 0 × 6\n# … with 6 variables: review_number <dbl>, brand <fct>,\n#   variety <chr>, style <fct>, country <fct>, stars <chr>\n\n# bar, can?\n\nramen %>% \n  filter(style %in% c(\"Bar\", \"Can\"))\n\n\n# A tibble: 2 × 6\n  review_number brand       variety              style country   stars\n          <dbl> <fct>       <chr>                <fct> <fct>     <chr>\n1          2513 Pringles    Nissin Top Ramen Ch… Can   United S… 3.5  \n2          1155 Komforte C… Savory Ramen         Bar   United S… 5    \n\n# is pringles potato chips? or ramen?\n\n\n\nStars\nScores are very strange.\n\n\nggplot(ramen, aes(stars)) +\n  geom_histogram(stat = \"count\")\n\n\n\n# scores are as character format at the moment. need to change to numerical.\n\nramen$stars %>% \n  unique()\n\n\n [1] \"5\"                  \"3.5\"                \"4.5\"               \n [4] \"4\"                  \"3.75\"               \"4.25\"              \n [7] \"3\"                  \"3.25\"               \"4.75\"              \n[10] \"2.5\"                \"2\"                  \"0.75\"              \n[13] \"0\"                  \"1.25\"               \"2.75\"              \n[16] \"0.5\"                \"1.5\"                \"2.25\"              \n[19] \"1\"                  \"NS\"                 \"0.25\"              \n[22] \"NR\"                 \"1.75\"               \"3.5/2.5\"           \n[25] \"42829\"              \"42860\"              \"4.5/5\"             \n[28] \"5/2.5\"              \"42859\"              \"4.25/5\"            \n[31] \"Unrated\"            \"1.1000000000000001\" \"2.1\"               \n[34] \"0.9\"                \"3.1\"                \"4.125\"             \n[37] \"3.125\"              \"2.125\"              \"2.9\"               \n[40] \"0.1\"                \"2.8\"                \"3.7\"               \n[43] \"3.4\"                \"3.6\"                \"2.85\"              \n[46] \"2.2999999999999998\" \"3.2\"                \"3.65\"              \n[49] \"1.8\"               \n\nramen %>% \n  filter(stars == \"Unrated\")\n\n\n# A tibble: 3 × 6\n  review_number brand     variety                style country   stars\n          <dbl> <fct>     <chr>                  <fct> <fct>     <chr>\n1          2548 Ottogi    Plain Instant Noodle … Pack  South Ko… Unra…\n2          2458 Samyang … Sari Ramen             Pack  South Ko… Unra…\n3          1587 Mi E-Zee  Plain Noodles          Pack  Malaysia  Unra…\n\ndirty_stars <- ramen %>% \n  filter(stars %in% c(\"0\",\n                      \"NR\", \"Unrated\",\n                      \"NS\",\n                      \"42829\",\n                      \"42859\",\n                      \"42860\",\n                      \"1.1000000000000001\",\n                      \"2.2999999999999998\"))\n\n# to remove unrated, and na values from dataset,\n# replace unrated as \"NA\" in main dataset, using dplyr\n# some noodles had 2 scores: 1 for broth and 1 for noodles, so will take average\n\nramen_cleaned_a <- ramen %>% \n  naniar::replace_with_na(replace = list(stars = c(\"NR\",\n                                                   \"NS\",\n                                                   \"42829\",\n                                                   \"42860\",\n                                                   \"42859\",\n                                                   \"Unrated\"))) %>% \n  mutate(stars_cleaned_a = str_replace_all(stars, \"3.5/2.5\", \"3\")) %>% \n  mutate(stars_cleaned_b = str_replace_all(stars_cleaned_a, \"4.25/5\", \"4.25\")) %>% \n  mutate(stars_cleaned_c = str_replace_all(stars_cleaned_b, \"1.1000000000000001\", \"1.1\")) %>% \n  mutate(stars_cleaned_d = str_replace_all(stars_cleaned_c, \"5/2.5\", \"3.75\")) %>% \n  mutate(stars_cleaned_e = str_replace_all(stars_cleaned_d, \"4.5/5\", \"4.75\")) %>% \n  mutate(stars_cleaned_f = str_replace_all(stars_cleaned_e, \"4.5/5\", \"4.75\")) %>% \n  mutate(stars_cleaned_g = str_replace_all(stars_cleaned_f, \"2.2999999999999998\", \"2.3\")) %>% \n  select(review_number, brand, variety, style, country, stars_cleaned_g) %>% \n  rename(flavor = variety,\n         packaging = style,\n         stars = stars_cleaned_g) %>% \n  filter(!is.na(stars)) %>% \n  mutate(stars = as.numeric(stars))\n\n\nglimpse(ramen_cleaned_a) # 3692, removed na\n\n\nRows: 3,692\nColumns: 6\n$ review_number <dbl> 3702, 3701, 3700, 3699, 3698, 3697, 3696, 3695…\n$ brand         <fct> \"Higashimaru\", \"Single Grain\", \"Sau Tao\", \"Sau…\n$ flavor        <chr> \"Seafood Sara Udon\", \"Chongqing Spicy & Sour R…\n$ packaging     <fct> Pack, Cup, Pack, Pack, Cup, Cup, Pack, Pack, B…\n$ country       <fct> Japan, China, Hong Kong, Hong Kong, Japan, Chi…\n$ stars         <dbl> 5.00, 3.50, 5.00, 4.50, 3.50, 4.50, 4.00, 5.00…\n\nramen_cleaned_a\n\n\n# A tibble: 3,692 × 6\n   review_number brand    flavor               packaging country stars\n           <dbl> <fct>    <chr>                <fct>     <fct>   <dbl>\n 1          3702 Higashi… Seafood Sara Udon    Pack      Japan    5   \n 2          3701 Single … Chongqing Spicy & S… Cup       China    3.5 \n 3          3700 Sau Tao  Seafood Flavour Sic… Pack      Hong K…  5   \n 4          3699 Sau Tao  Jiangnan Style Nood… Pack      Hong K…  4.5 \n 5          3698 Sapporo… CupStar Shio Ramen   Cup       Japan    3.5 \n 6          3697 Sichuan… Big Boss Broad Nood… Cup       China    4.5 \n 7          3696 Nissin   Top Ramen Masala No… Pack      India    4   \n 8          3695 Maruchan Miyashi Chuka Cold … Pack      Japan    5   \n 9          3694 Yamamot… Tanukioyaji Super S… Bowl      Japan    3.5 \n10          3693 Kenko F… Michio Kawamura Nat… Pack      Japan    3.75\n# … with 3,682 more rows\n\nramen_cleaned_a %>% \n  ggplot(aes(stars)) +\n  geom_histogram() # not normally distributed. can ramen receive 0?\n\n\n\nmedian(ramen_cleaned_a$stars) # median = 3.75\n\n\n[1] 3.75\n\nmean(ramen_cleaned_a$stars) # mean = 3.73\n\n\n[1] 3.723226\n\nPackaging\n\n\nramen_cleaned_a %>% \n  count(packaging) %>% \n  arrange(desc(n))\n\n\n# A tibble: 8 × 2\n  packaging      n\n  <fct>      <int>\n1 Pack        2085\n2 Bowl         722\n3 Cup          659\n4 Tray         167\n5 Box           54\n6 Restaurant     3\n7 Bar            1\n8 Can            1\n\n# create a new packaging variable to lump low levels together\n\nramen_cleaned_b <- ramen_cleaned_a %>% \n  mutate(packaging_cleaned = fct_inorder(fct_lump_n(packaging, 5),\n                                         ordered = NA))\n\n# (ramen_cleaned_b$packaging_cleaned)\n\nglimpse(ramen_cleaned_b)\n\n\nRows: 3,692\nColumns: 7\n$ review_number     <dbl> 3702, 3701, 3700, 3699, 3698, 3697, 3696, …\n$ brand             <fct> \"Higashimaru\", \"Single Grain\", \"Sau Tao\", …\n$ flavor            <chr> \"Seafood Sara Udon\", \"Chongqing Spicy & So…\n$ packaging         <fct> Pack, Cup, Pack, Pack, Cup, Cup, Pack, Pac…\n$ country           <fct> Japan, China, Hong Kong, Hong Kong, Japan,…\n$ stars             <dbl> 5.00, 3.50, 5.00, 4.50, 3.50, 4.50, 4.00, …\n$ packaging_cleaned <fct> Pack, Cup, Pack, Pack, Cup, Cup, Pack, Pac…\n\nFinal dataset:\n\n\nramen_final <- ramen_cleaned_b\n\n\n\nVisualize\n\n\ntheme_set(theme_clean())\n\n\n\nTop 10 (by country)\n\n\nramen_final %>% \n  count(packaging_cleaned,country) %>% \n  arrange(desc(n)) %>% \n  group_by(packaging_cleaned) %>% \n  slice_head(n = 10) %>% \n  ggplot(aes(fct_reorder(country, n), n, \n             fill = country,\n             label = n)) +\n  geom_col(show.legend = F) +\n  facet_wrap( . ~ packaging_cleaned, ncol = 3, scales = \"free\") +\n  ylim(0, 400) +\n  geom_text(aes(label = n, hjust = -0.1)) +\n  coord_flip() +\n  labs(title = \"Top 10 Countries with most number of products\",\n       x = \"\",\n       y = \"Count\",\n       subtitle = \"In general, Japan has the highest number of products across main categories, followed by United States. \\nTaiwan has the more products than Japan for Packs.\",\n       caption = \"Source: The Ramen Rater\") +\n  theme(axis.title = element_text(face = \"bold\", size = 20),\n        strip.text = element_text(face = \"bold\", size = 18),\n        axis.text = element_text(face = \"bold\", size = 18),\n        title = element_text(face = \"bold\", size = 24))\n\n\n\n\nTop 20 brands, by variety?\n\n\n# top 20 brands with most varieties\nramen_final %>% \n  group_by(brand) %>% \n  summarise(n = n()) %>% \n  arrange(desc(n)) %>% \n  slice_head(n = 10) %>%  # top 10\n  ggplot(aes(fct_reorder(brand, n), n)) +\n  geom_col(fill = \"deepskyblue4\") +\n  geom_text(aes(label = n), hjust = -0.5) +\n  scale_y_continuous(expand = c(0,0),\n                     limits = c(0, 600)) +\n  labs(title = \"Top 10 brands with the most products\",\n       x = \"\",\n       y = \"No. of products\",\n       subtitle = \"Nissin is the market leader in terms of number of products carried\",\n       caption = \"Source: The Ramen Rater\") +\n  coord_flip() +\n  theme(axis.title = element_text(face = \"bold\", size = 18),\n        axis.text = element_text(size = 16),\n        title = element_text(size = 24))\n\n\n\n\nTop Brands\n\n\ndata_plot <- ramen_final %>% \n  group_by(brand, country) %>% \n  summarise(\n    count = n(),\n    mean_rating = mean(stars),\n    min_rating = min(stars),\n    max_rating = max(stars)\n  ) %>% \n  ungroup() %>% \n  filter(mean_rating > 3.75) %>% \n  select(country, brand, count, mean_rating, min_rating, max_rating) %>% \n  arrange(desc(count, mean_rating)) \n\ndata_plot\n\n\n# A tibble: 277 × 6\n   country     brand           count mean_rating min_rating max_rating\n   <fct>       <fct>           <int>       <dbl>      <dbl>      <dbl>\n 1 Japan       Nissin            144        4.17       1.5           5\n 2 South Korea Samyang Foods      99        4.12       0             5\n 3 Hong Kong   Nissin             82        4.10       1.75          5\n 4 South Korea Paldo              82        4.04       0             5\n 5 Japan       Myojo              77        3.77       0             5\n 6 Japan       Maruchan           65        3.85       0             5\n 7 South Korea Nongshim           65        4.03       0.5           5\n 8 Japan       Sapporo Ichiban    55        3.76       2             5\n 9 Indonesia   Indomie            54        4.15       1.5           5\n10 Hong Kong   Sau Tao            45        4.12       1.25          5\n# … with 267 more rows\n\ndata_plot%>%\n  slice_head(n = 15) %>% \n  ggplot(aes(fct_reorder(brand, mean_rating), mean_rating, col = country)) +\n  geom_point(aes(size = count), show.legend = F) +\n  geom_errorbar(aes(ymin = min_rating,\n                    ymax = max_rating),\n                width = 0.5, size = 0.8) +\n  labs(title = \"Top brands - in terms of number of products launched and mean ratings\",\n       subtitle = \"MyKuali and Indomie may have narrower product range but are highly scored. \\nNissin has many products (produced in JP, SG, US), and has high mean ratings with narrower range in scores - \\nie greater product quality consistency\",\n       caption = \"Source: The Ramen Rater\",\n       x = \"Mean Rating\",\n       y = \"Brand\",\n       fill  = \"Country\") +\n  ylim(0, 5) +\n  coord_flip() +\n  theme(axis.text = element_text(face = \"bold\", size = 16),\n        title = element_text(face = \"bold\", size = 24))\n\n\n\n\nMean ratings by country\n\n\nramen_final %>% \n  group_by(country) %>% \n  summarise(mean_rating = mean(stars),\n            count = n()) %>% \n  arrange(desc(count, mean_rating)) %>% \n  slice_head(n = 10) %>% \n  ggplot(aes(fct_reorder(country, mean_rating), mean_rating, fill = country,\n             text = round(mean_rating, 2))) +\n  geom_col(show.legend = F) +\n\n  geom_text(aes(label = round(mean_rating, 2)), hjust = -0.25) +\n  scale_y_continuous(expand = c(0,0), limits = c(0, 5)) +\n  coord_flip() +\n  labs(title = \"Top Countries with Highest Mean Ratings\",\n       subtitle = \"Southeast Asian Countries are doing very well\",\n       caption = \"Source: The Ramen Rater\",\n       x = \"\", \n       y = \"Mean rating\")\n\n\n\n\nWordcloud for popular flavors\n\n\nramen_cleaned_a %>% \n  select(flavor) %>% \n  distinct()\n\n\n# A tibble: 3,439 × 1\n   flavor                                             \n   <chr>                                              \n 1 Seafood Sara Udon                                  \n 2 Chongqing Spicy & Sour Rice Noodles                \n 3 Seafood Flavour Sichuan Spicy Noodle               \n 4 Jiangnan Style Noodle - Original Flavour           \n 5 CupStar Shio Ramen                                 \n 6 Big Boss Broad Noodle Chili Oil Flavor (Sour & Hot)\n 7 Top Ramen Masala Noodles                           \n 8 Miyashi Chuka Cold Noodle                          \n 9 Tanukioyaji Super Spicy Mazemen                    \n10 Michio Kawamura Nature Ramen Shio                  \n# … with 3,429 more rows\n\nramen_cleaned_a\n\n\n# A tibble: 3,692 × 6\n   review_number brand    flavor               packaging country stars\n           <dbl> <fct>    <chr>                <fct>     <fct>   <dbl>\n 1          3702 Higashi… Seafood Sara Udon    Pack      Japan    5   \n 2          3701 Single … Chongqing Spicy & S… Cup       China    3.5 \n 3          3700 Sau Tao  Seafood Flavour Sic… Pack      Hong K…  5   \n 4          3699 Sau Tao  Jiangnan Style Nood… Pack      Hong K…  4.5 \n 5          3698 Sapporo… CupStar Shio Ramen   Cup       Japan    3.5 \n 6          3697 Sichuan… Big Boss Broad Nood… Cup       China    4.5 \n 7          3696 Nissin   Top Ramen Masala No… Pack      India    4   \n 8          3695 Maruchan Miyashi Chuka Cold … Pack      Japan    5   \n 9          3694 Yamamot… Tanukioyaji Super S… Bowl      Japan    3.5 \n10          3693 Kenko F… Michio Kawamura Nat… Pack      Japan    3.75\n# … with 3,682 more rows\n\n# tidytext::unnest_tokens to split flavor into words ----\n\ntidy_flavor <- ramen_cleaned_a %>% \n  mutate(flavor2 = flavor) %>% \n  unnest_tokens(word, # output column to be created as string \n                flavor) %>%  # input column to be split)\n  group_by(word) %>% \n  count() %>% \n  arrange(desc(n)) %>% \n  anti_join(stop_words) %>% \n  filter(!word %in% c(\"noodle\", \"ramen\", \"instant\", \"flavor\", \"flavour\",\n                      \"noodles\", \"soup\", \"cup\", \"artificial\", \"style\", \"bowl\", \"mi\",\n                      \"sauce\")) %>% \n  ungroup() %>% \n  filter(n>1)\n\n\ntidy_flavor\n\n\n# A tibble: 923 × 2\n   word        n\n   <chr>   <int>\n 1 chicken   419\n 2 spicy     414\n 3 beef      314\n 4 hot       189\n 5 curry     188\n 6 rice      178\n 7 tom       152\n 8 shrimp    151\n 9 pork      140\n10 seafood   140\n# … with 913 more rows\n\n# top flavors are chicken, spicy, beef/\n\n# look at bigrams\n\n\n# bigrams\n\ntidy_bigrams <- ramen_cleaned_a %>% \n  unnest_tokens(bigram, flavor, token = \"ngrams\", n = 2) %>% \n  separate(bigram, c(\"word1\", \"word2\"), sep = \" \") %>% \n  filter(!word1 %in% c(stop_words$word)) %>% \n  filter(!word2 %in% c(stop_words$word)) %>% \n  count(word1, word2, sort = T) %>% \n  drop_na()  %>% \n  filter(!word1 %in% c(\"noodle\", \"ramen\", \"instant\", \"flavor\", \"flavour\",\n                      \"noodles\", \"soup\", \"cup\", \"artificial\", \"style\", \"bowl\", \"mi\",\n                      \"sauce\")) %>% \n  filter(!word2 %in% c(\"noodle\", \"ramen\", \"instant\", \"flavor\", \"flavour\",\n                      \"noodles\", \"soup\", \"cup\", \"artificial\", \"style\", \"bowl\", \"mi\",\n                      \"sauce\")) %>% \n  filter(n>5) %>% \n  unite(\"joined_flv\", word1:word2, sep = \" \")\n\n\ntidy_bigrams\n\n\n# A tibble: 105 × 2\n   joined_flv          n\n   <chr>           <int>\n 1 tom yum           107\n 2 hot spicy          63\n 3 spicy beef         52\n 4 rice vermicelli    47\n 5 chow mein          41\n 6 spicy chicken      34\n 7 white curry        32\n 8 tom yam            31\n 9 hot sour           28\n10 sesame oil         26\n# … with 95 more rows\n\n\n\nwordcloud2(data = tidy_bigrams)\n\n\n\n{\"x\":{\"word\":[\"tom yum\",\"hot spicy\",\"spicy beef\",\"rice vermicelli\",\"chow mein\",\"spicy chicken\",\"white curry\",\"tom yam\",\"hot sour\",\"sesame oil\",\"kung fu\",\"penang white\",\"spicy seafood\",\"demae iccho\",\"buldak bokkeummyun\",\"yum shrimp\",\"black pepper\",\"shrimp tom\",\"2 minute\",\"green curry\",\"rasa ayam\",\"minced pork\",\"south korean\",\"spicy shrimp\",\"braised beef\",\"creamy tom\",\"sopa nissin\",\"spicy hot\",\"curry udon\",\"kuah rasa\",\"pad thai\",\"pepper crab\",\"yum goong\",\"hot chicken\",\"hot pot\",\"kitsune udon\",\"soybean paste\",\"chicken curry\",\"la mian\",\"malaysia penang\",\"pancit canton\",\"shin ramyun\",\"stir fried\",\"stir fry\",\"sweet potato\",\"yum kung\",\"penang red\",\"tempura udon\",\"wei wei\",\"bean vermicelli\",\"ho fan\",\"penang hokkien\",\"rasa soto\",\"red tom\",\"shrimp creamy\",\"spicy miso\",\"spicy pork\",\"asam laksa\",\"braised pork\",\"creamy chicken\",\"hokkien prawn\",\"mee goreng\",\"mie goreng\",\"pork bone\",\"selera pedas\",\"sichuan pepper\",\"spicy sesame\",\"sweet sour\",\"tempura soba\",\"abalone chicken\",\"ayam bawang\",\"curry rice\",\"hao hao\",\"hong kong\",\"nissin sabor\",\"oriental kitchen\",\"perisa kari\",\"pork ribs\",\"potato chips\",\"ppushu ppushu\",\"sesame paste\",\"sour shrimp\",\"spicy tonkotsu\",\"1 minute\",\"1 step\",\"authentic taste\",\"beef rice\",\"goreng perisa\",\"goreng rasa\",\"han feast\",\"kari ayam\",\"laksa singapura\",\"minute asian\",\"pho bo\",\"pickled cabbage\",\"pickled vegetable\",\"roast beef\",\"singapore laksa\",\"spicy kimchi\",\"spicy king\",\"step 1\",\"stewed beef\",\"stewed pork\",\"tan tan\",\"wei premium\"],\"freq\":[107,63,52,47,41,34,32,31,28,26,25,25,24,21,20,19,18,18,16,16,16,15,15,15,14,14,14,14,13,13,13,13,13,12,12,12,12,11,11,11,11,11,11,11,11,11,10,10,10,9,9,9,9,9,9,9,9,8,8,8,8,8,8,8,8,8,8,8,8,7,7,7,7,7,7,7,7,7,7,7,7,7,7,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6,6],\"fontFamily\":\"Segoe UI\",\"fontWeight\":\"bold\",\"color\":\"random-dark\",\"minSize\":0,\"weightFactor\":1.68224299065421,\"backgroundColor\":\"white\",\"gridSize\":0,\"minRotation\":-0.785398163397448,\"maxRotation\":0.785398163397448,\"shuffle\":true,\"rotateRatio\":0.4,\"shape\":\"circle\",\"ellipticity\":0.65,\"figBase64\":null,\"hover\":null},\"evals\":[],\"jsHooks\":{\"render\":[{\"code\":\"function(el,x){\\n                        console.log(123);\\n                        if(!iii){\\n                          window.location.reload();\\n                          iii = False;\\n\\n                        }\\n  }\",\"data\":null}]}}\nBigrams offer more information: hot and spicy, rather that hot, spicy.\nRamen rater seems to like spicy instant noodles!\nReferences\nhttps://casualinference.netlify.app/2019/06/04/tidytuesday-ramen-ratings/ https://beta.rstudioconnect.com/content/5291/tidytuesday-ramen.nb.html https://rstudio-pubs-static.s3.amazonaws.com/502700_3ee879f2e94f4d1da5696be52b9e6107.html\n\n\n\n",
    "preview": "posts/20210728_Tidytuesday ramen ratings/tidy-tuesday-y190614_updated_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2021-08-10T19:14:03+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210726 Multiple t-test in R for different variables/",
    "title": "Multiple t-test in R",
    "description": "What if I have many variables to compare?",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-07-26",
    "categories": [],
    "contents": "\n\nContents\nLoad Packages\nLoad dataset\nTransform into long format\nRun multiple t-test using rstatix package\nVisualization\nApplying it to a food science dataset\nDataset\nStatistical test\nVisualization\n\nReference:\n\nI was looking at a dateset at work, and was wondering how I can carry out t-test to check if there were any significant difference in flavor compounds between two different species of the same fruit.\nThe exercise below is from a website https://www.datanovia.com/en/blog/how-to-perform-multiple-t-test-in-r-for-different-variables/, and I used it on a dataset which was on the physicochemical properties of bananas (from Easy Statistics for Food Science with R)\nLoad Packages\n\n\nlibrary(tidyverse)\nlibrary(rstatix)\nlibrary(ggpubr)\n\n\n\nLoad dataset\n\n\ndata <- iris %>% \n  filter(Species != \"virginica\") %>% \n  as_tibble()\n\nglimpse(data)\n\n\nRows: 100\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa,…\n\nTransform into long format\n\n\ndata_long <- data %>% \n  pivot_longer(cols = Sepal.Length:Petal.Width,\n               names_to = \"variables\",\n               values_to = \"value\")\n\nglimpse(data_long)\n\n\nRows: 400\nColumns: 3\n$ Species   <fct> setosa, setosa, setosa, setosa, setosa, setosa, se…\n$ variables <chr> \"Sepal.Length\", \"Sepal.Width\", \"Petal.Length\", \"Pe…\n$ value     <dbl> 5.1, 3.5, 1.4, 0.2, 4.9, 3.0, 1.4, 0.2, 4.7, 3.2, …\n\nRun multiple t-test using rstatix package\n\n\nstat_test <- data_long %>% \n  group_by(variables) %>% \n  t_test(value ~ Species) %>% \n  adjust_pvalue(method = \"bonferroni\") %>% \n  add_significance()\n\n\n\nVisualization\n\n\nboxplot <- ggboxplot(data_long,\n          x = \"Species\",\n          y = \"value\",\n          fill = \"Species\",\n          palette = \"jco\",\n          legend = \"none\",\n          ggtheme = theme_pubr(border = T)) +\n  facet_wrap ( ~ variables, scales = \"free\" )\n\n# with sig diff\nstat_test_plot <- stat_test %>% \n  add_xy_position(x = \"Species\") # from r-statix\n\nboxplot + \n  stat_pvalue_manual(stat_test_plot, label = \"p.adj.signif\",\n                     hide.ns = T, tip.length = 0)\n\n\n\n\nApplying it to a food science dataset\nComparing the pH and TSS (Brix) of bananas (Cavendish vs Dream) (dataset is from Easy Statistics for Food Science with R, page 78)\nDataset\n\n\nbanana <- tribble(\n  ~ variety,  ~pH,   ~tss,\n  # --------#------#-------\n  \"cavendish\", 5.60, 4.33,\n  \"cavendish\", 5.57, 4.03,\n  \"cavendish\", 4.76, 3.77,\n  \"cavendish\", 5.56, 4.10,\n  \"cavendish\", 4.95, 3.97,\n  \"cavendish\", 4.84, 4.40,\n  \"cavendish\", 5.07, 4.50,\n  \"cavendish\", 4.94, 4.43,\n  \"cavendish\", 5.04, 4.30,\n  \"cavendish\", 4.93, 4.57,\n  \"cavendish\", 5.05, 4.30,\n  \"cavendish\", 5.21, 4.40,\n  \"dream\", 4.31, 3.67,\n  \"dream\", 4.41, 3.80,\n  \"dream\", 4.35, 3.00,\n  \"dream\", 4.49, 3.40,\n  \"dream\", 4.39, 3.67,\n  \"dream\", 4.43, 3.33,\n  \"dream\", 4.44, 3.47,\n  \"dream\", 4.44, 3.57,\n  \"dream\", 4.52, 3.20,\n  \"dream\", 4.79, 3.17,\n  \"dream\", 4.68, 3.27,\n  \"dream\", 4.83, 3.30\n) %>% \n  mutate(variety = factor(variety))\n\n\nglimpse(banana)\n\n\nRows: 24\nColumns: 3\n$ variety <fct> cavendish, cavendish, cavendish, cavendish, cavendis…\n$ pH      <dbl> 5.60, 5.57, 4.76, 5.56, 4.95, 4.84, 5.07, 4.94, 5.04…\n$ tss     <dbl> 4.33, 4.03, 3.77, 4.10, 3.97, 4.40, 4.50, 4.43, 4.30…\n\nStatistical test\n\n\nbanana_long <- banana %>% \n  pivot_longer(cols = pH:tss,\n               names_to = \"variables\",\n               values_to = \"values\")\n\nglimpse(banana_long)\n\n\nRows: 48\nColumns: 3\n$ variety   <fct> cavendish, cavendish, cavendish, cavendish, cavend…\n$ variables <chr> \"pH\", \"tss\", \"pH\", \"tss\", \"pH\", \"tss\", \"pH\", \"tss\"…\n$ values    <dbl> 5.60, 4.33, 5.57, 4.03, 4.76, 3.77, 5.56, 4.10, 4.…\n\nstat_test <- banana_long %>% \n  group_by(variables) %>% \n  t_test(values ~ variety) %>% \n  adjust_pvalue(method = \"none\") %>% \n  add_significance()\n\nstat_test\n\n\n# A tibble: 2 × 11\n  variables .y.    group1  group2    n1    n2 statistic    df        p\n* <chr>     <chr>  <chr>   <chr>  <int> <int>     <dbl> <dbl>    <dbl>\n1 pH        values cavend… dream     12    12      6.32  17.6  6.56e-6\n2 tss       values cavend… dream     12    12      8.75  22.0  1.28e-8\n# … with 2 more variables: p.adj <dbl>, p.adj.signif <chr>\n\nVisualization\n\n\nboxplot <- ggboxplot(banana_long,\n          x = \"variety\",\n          y = \"values\",\n          fill = \"variety\",\n          palette = \"jco\",\n          legend = \"none\",\n          ggtheme = theme_pubr(border = T)) +\n  facet_wrap ( ~ variables, scales = \"free\" ) +\n  labs(title = \"Cavendish Bananas have higher pH and total soluble solids. \")\n\n# with sig diff\nstat_test_plot <- stat_test %>% \n  add_xy_position(x = \"variety\") # from r-statix\n\nboxplot + \n  stat_pvalue_manual(stat_test_plot, \n                     label = \"p = {p.adj}{p.adj.signif}\",\n                     hide.ns = T, tip.length = 0)\n\n\n\n\nReference:\nhttps://www.datanovia.com/en/blog/how-to-perform-multiple-t-test-in-r-for-different-variables/\nhttps://www.datanovia.com/en/blog/how-to-add-p-values-to-ggplot-facets/\nhttps://books.google.com.sg/books?id=e0JvDwAAQBAJ&pg=PA77&lpg=PA77&dq=cavendish+dream+pH+tss&source=bl&ots=XHNZARp1_F&sig=ACfU3U1QE4DIuf936cwJ5HdUQlhfGmtZNQ&hl=en&sa=X&ved=2ahUKEwjt46PMwf_xAhUEWCsKHVhwBBgQ6AEwEHoECBMQAw#v=onepage&q=cavendish%20dream%20pH%20tss&f=false\n\n\n\n",
    "preview": "posts/20210726 Multiple t-test in R for different variables/Multiple-t-test-in-R-for-different-variables_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-08-10T19:23:57+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210620_Tidytuesday malaria data/",
    "title": "Tidy Tuesday Series",
    "description": "2018 Week 33 - Malaria Data",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-06-20",
    "categories": [],
    "contents": "\n\nContents\nLoad Packages\nLoad Data from tidytuesdayR package\nSummarise by year over time on a map\nIncidence\nDeaths\nReferences\n\nLoad Packages\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(ggthemes)\nlibrary(skimr)\nlibrary(malariaAtlas)\nlibrary(maps)\n\n\n\nLoad Data from tidytuesdayR package\n\n\n# to download data\ntt_data <- tt_load(2018, week = 33)\n\n\n# to view readme\nreadme(tt_data)\n\n# Save data as objects\nincidence <- tt_data$malaria_inc\ndeaths <- tt_data$malaria_deaths\ndeaths_age <- tt_data$malaria_deaths_age \n\n\n\n\n\ndata_deaths <- read_csv(\"malaria_deaths.csv\")\ndata_deaths_age <- read_csv(\"malaria_deaths_age.csv\")\ndata_incidence <- read_csv(\"malaria_incidence.csv\")\n\n\n\nSummarise by year over time on a map\n\n\n# prevelance/parasite rate\nkenya_pr <- getPR(ISO = \"KEN\", # KENYA\n            species = \"BOTH\") %>% \n  filter(!is.na(pr))\n\nglimpse(kenya_pr)\n\n\nRows: 1,855\nColumns: 28\n$ dhs_id                    <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n$ site_id                   <int> 13580, 8231, 22331, 16507, 4231, 1…\n$ site_name                 <chr> \"Kora Kora\", \"Ulutya Primary Schoo…\n$ latitude                  <dbl> -0.6097, -0.9724, -3.8442, -1.3149…\n$ longitude                 <dbl> 39.7807, 37.6902, 39.7527, 36.8112…\n$ rural_urban               <chr> \"UNKNOWN\", \"RURAL\", \"UNKNOWN\", \"UR…\n$ country                   <chr> \"Kenya\", \"Kenya\", \"Kenya\", \"Kenya\"…\n$ country_id                <chr> \"KEN\", \"KEN\", \"KEN\", \"KEN\", \"KEN\",…\n$ continent_id              <chr> \"Africa\", \"Africa\", \"Africa\", \"Afr…\n$ month_start               <int> 5, 10, 5, 7, 11, 3, 7, 8, 5, 9, 5,…\n$ year_start                <int> 1994, 2009, 2009, 2009, 2009, 1995…\n$ month_end                 <int> 5, 10, 5, 7, 11, 3, 7, 8, 5, 9, 5,…\n$ year_end                  <int> 1994, 2009, 2009, 2009, 2009, 1995…\n$ lower_age                 <dbl> 0.0, 5.0, 0.6, 4.0, 5.0, 0.0, 0.0,…\n$ upper_age                 <int> 6, 17, 8, 15, 17, 4, 9, 4, 14, 15,…\n$ examined                  <int> 270, 109, 11, 93, 110, 168, 133, 2…\n$ positive                  <dbl> 36, 0, 2, 2, 48, 107, 111, 1, 4, 4…\n$ pr                        <dbl> 0.1333, 0.0000, 0.1818, 0.0215, 0.…\n$ species                   <chr> \"P. falciparum\", \"P. falciparum\", …\n$ method                    <chr> \"Microscopy\", \"RDT\", \"Microscopy\",…\n$ rdt_type                  <chr> \"\", \"Paracheck PF - Rapid test for…\n$ pcr_type                  <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA…\n$ malaria_metrics_available <chr> \"true\", \"true\", \"true\", \"true\", \"t…\n$ location_available        <chr> \"true\", \"true\", \"true\", \"true\", \"t…\n$ permissions_info          <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n$ citation1                 <chr> \" (1994). <i>Vitamin A deficiency …\n$ citation2                 <chr> \"\", \"Gitonga, CW, Karanja, PN, Kih…\n$ citation3                 <chr> \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\", \"\"…\n\n# check\ntable(is.na(kenya_pr$pr))\n\n\n\nFALSE \n 1855 \n\n# plot\nkenya_pr %>% \n  group_by(year_start) %>% \n  dplyr::summarise(examined = sum(examined),\n            positive = sum(positive),\n            studies = n()) %>% \n  mutate (pr = positive/examined) %>% \n  ggplot(aes(year_start, pr)) +\n  geom_line() +\n  labs(title = \"Change in Prevalance Rate (Positive/Examined) rate over the years\",\n       subtitle = \"Prevalance rate decreased over the years\",\n       x = \"Year\",\n       y = \"Prevalance Rate\",\n       caption = \"Source: mariaAtlas package\") +\n  theme_few()\n\n\n\n\n\n\nkenya_pr %>% \n  arrange(pr) %>% \n  ggplot(aes(longitude, latitude, col = pr)) +\n  geom_point() +\n  borders(\"world\", regions = \"Kenya\") +\n  scale_colour_gradient2(low = \"blue\", high = \"red\", \n                         midpoint = 0.5, \n                         labels = scales::percent_format()) +\n  labs(title = \"Prevalence of Malaria in Kenya\",\n       caption = \"Source: mariaAtlas package\") +\n  coord_map() +\n  theme_void()\n\n\n\n\nAggregate Prevalence by decade\n\n\nkenya_pr %>% \n  group_by(decade = 10 * (year_start %/% 10)) %>% \n  arrange(pr) %>% \n  ggplot(aes(longitude, latitude, col = pr)) +\n  geom_point() +\n  borders(\"world\", regions = \"Kenya\") +\n  scale_colour_gradient2(low = \"blue\", high = \"red\", \n                         midpoint = 0.5, \n                         labels = scales::percent_format()) +\n  labs(title = \"Prevalence of Malaria in Kenya, by decade\",\n       caption = \"Source: mariaAtlas package\",\n       col = \"Prevalence\") +\n  coord_map() +\n  facet_wrap ( ~decade) +\n  theme_void()\n\n\n\n\nIncidence\nLooking at aggregated data\n\n\nglimpse(data_incidence)\n\n\nRows: 508\nColumns: 4\n$ Entity                                                                               <chr> …\n$ Code                                                                                 <chr> …\n$ Year                                                                                 <dbl> …\n$ `Incidence of malaria (per 1,000 population at risk) (per 1,000 population at risk)` <dbl> …\n\n# change column names\nmalaria_inc_processed <- data_incidence %>% \n  setNames(c(\"country\", \"code\", \"year\", \"incidence\")) %>% \n  mutate(incidence = incidence /1000)\n\nmalaria_inc_processed%>% \n  filter(country %in% sample(unique(country), 6)) %>% \n  ggplot(aes(year, incidence, col = country)) +\n  geom_line() +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme_few()\n\n\n\n\nLooking at 2015 levels and the change from 2000 to 2015\n\n\nmalaria_spread <- malaria_inc_processed %>% \n  mutate(year = paste0(\"Y\",year)) %>% \n  pivot_wider(names_from = year,\n              values_from = incidence) %>% \n  mutate(current = Y2015,\n         change = Y2015 - Y2000)\n\n\nmalaria_spread %>% \n  filter(country != \"Turkey\", # outlier\n         !is.na(code)) %>%  # no country code\n  ggplot(aes(current, change)) +\n  geom_point() +\n  geom_text(aes(label = code), vjust = 1, hjust = 1) +\n  theme_few()\n\n\n\n\n\n\n# what countries are not in the map data?\nmalaria_spread %>% \n  anti_join(map_data(\"world\"), by = c(country = \"region\"))\n\n\n# A tibble: 32 x 8\n   country          code   Y2000  Y2005  Y2010   Y2015 current  change\n   <chr>            <chr>  <dbl>  <dbl>  <dbl>   <dbl>   <dbl>   <dbl>\n 1 Congo            COG   0.364  0.350  0.217  0.173   0.173   -0.190 \n 2 Cote d'Ivoire    CIV   0.525  0.531  0.446  0.349   0.349   -0.177 \n 3 Democratic Repu… COD   0.508  0.525  0.427  0.246   0.246   -0.262 \n 4 Early-demograph… <NA>  0.0837 0.0616 0.0475 0.0289  0.0289  -0.0548\n 5 East Asia & Pac… <NA>  0.0227 0.0207 0.0201 0.00570 0.00570 -0.0170\n 6 East Asia & Pac… <NA>  0.0228 0.0211 0.0205 0.00580 0.00580 -0.0170\n 7 East Asia & Pac… <NA>  0.0228 0.0207 0.0202 0.00572 0.00572 -0.0171\n 8 Fragile and con… <NA>  0.319  0.305  0.247  0.180   0.180   -0.139 \n 9 Heavily indebte… <NA>  0.408  0.326  0.274  0.198   0.198   -0.209 \n10 IBRD only        <NA>  0.0355 0.0383 0.0269 0.0152  0.0152  -0.0203\n# … with 22 more rows\n\nmaps::iso3166 %>% \n  as_tibble()\n\n\n# A tibble: 269 x 5\n   a2    a3    ISOname            mapname           sovereignty       \n   <chr> <chr> <chr>              <chr>             <chr>             \n 1 AW    ABW   Aruba              Aruba             Netherlands       \n 2 AF    AFG   Afghanistan        Afghanistan       Afghanistan       \n 3 AO    AGO   Angola             Angola            Angola            \n 4 AI    AIA   Anguilla           Anguilla          Anguilla          \n 5 AX    ALA   Aland Islands      Finland:Aland Is… Finland           \n 6 AL    ALB   Albania            Albania           Albania           \n 7 AD    AND   Andorra            Andorra           Andorra           \n 8 AE    ARE   United Arab Emira… United Arab Emir… United Arab Emira…\n 9 AR    ARG   Argentina          Argentina         Argentina         \n10 AM    ARM   Armenia            Armenia           Armenia           \n# … with 259 more rows\n\nworld <- map_data(\"world\") %>% \n  filter(region != \"Antarctica\")\n\ndata_plot <- malaria_inc_processed %>% \n  filter(incidence < 1) %>% \n  inner_join(maps::iso3166 %>% \n              select(a3, mapname), \n              by = c(code = \"a3\")) %>% \n  inner_join(world, by = c(mapname = \"region\")) \n\nglimpse(data_plot)\n\n\nRows: 147,970\nColumns: 10\n$ country   <chr> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgh…\n$ code      <chr> \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"…\n$ year      <dbl> 2000, 2000, 2000, 2000, 2000, 2000, 2000, 2000, 20…\n$ incidence <dbl> 0.1071, 0.1071, 0.1071, 0.1071, 0.1071, 0.1071, 0.…\n$ mapname   <chr> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgh…\n$ long      <dbl> 74.89131, 74.84023, 74.76738, 74.73896, 74.72666, …\n$ lat       <dbl> 37.23164, 37.22505, 37.24917, 37.28564, 37.29072, …\n$ group     <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,…\n$ order     <int> 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24…\n$ subregion <chr> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA…\n\ndata_plot %>% \n  ggplot(aes(long, lat, group = group, fill = incidence)) +\n  geom_polygon() +\n  scale_fill_gradient2(low = \"blue\",\n                       high = \"red\",\n                       midpoint = 0.2,\n                       labels = scales::percent_format()) +\n  facet_wrap( ~ year) +\n  coord_map() +\n  labs(title = \"Malaria incidence over time around the world\",\n       subtitle = \"Malaria incidence had generally decreased over time.\", \n       fill = \"Incidence\",\n       caption = \"Source: malariaAtlas package\") +\n  theme_void() +\n  theme(strip.text = element_text(face = \"bold\", size = 14),\n        title = element_text(face = \"bold\", size = 16))\n\n\n\n\nDeaths\n\n\nglimpse(data_deaths)\n\n\nRows: 6,156\nColumns: 4\n$ Entity                                                                             <chr> …\n$ Code                                                                               <chr> …\n$ Year                                                                               <dbl> …\n$ `Deaths - Malaria - Sex: Both - Age: Age-standardized (Rate) (per 100,000 people)` <dbl> …\n\n# change column names\nmalaria_deaths_processed <- data_deaths %>% \n  setNames(c(\"country\", \"code\", \"year\", \"deaths\")) \n\nglimpse(malaria_deaths_processed)\n\n\nRows: 6,156\nColumns: 4\n$ country <chr> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afghan…\n$ code    <chr> \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AFG\", \"AF…\n$ year    <dbl> 1990, 1991, 1992, 1993, 1994, 1995, 1996, 1997, 1998…\n$ deaths  <dbl> 6.802930, 6.973494, 6.989882, 7.088983, 7.392472, 7.…\n\nReferences\nhttps://www.youtube.com/watch?v=5_6O2oDy5Jk&list=PL19ev-r1GBwkuyiwnxoHTRC8TTqP8OEi8&index=77\n\n\n\n",
    "preview": "posts/20210620_Tidytuesday malaria data/tidy-tuesday-y18w33_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-07-02T21:11:21+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210619_Tidytuesday wind data/",
    "title": "Tidy Tuesday Series",
    "description": "2018 Week 32 - US Wind Farm Locations",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-06-18",
    "categories": [],
    "contents": "\n\nContents\nLoad Packages\nLoad Data from tidytuesdayR package\nExplore Dataset\nCount state\nCount project names\nPlot longitude and latitude\n\nDistribution of wind turbines in US States\nProjects\nYear\nCapacity\nHow has turbine capacity changed over time?\nTurbine models\nLearning points:\nReferences\n\nLoad Packages\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(ggthemes)\nlibrary(mapproj)\n\n\n\nLoad Data from tidytuesdayR package\n\n\n# to download data\ntt_data <- tt_load(2018, week = 32)\n\n\n# to view readme\nreadme(tt_data)\n\nrecent_grads <- tt_data$us_wind\n\n\n\nExplore Dataset\n\n\nglimpse(us_wind)\n\n\nRows: 58,185\nColumns: 24\n$ case_id    <dbl> 3073429, 3071522, 3073425, 3071569, 3005252, 3003…\n$ faa_ors    <chr> \"missing\", \"missing\", \"missing\", \"missing\", \"miss…\n$ faa_asn    <chr> \"missing\", \"missing\", \"missing\", \"missing\", \"miss…\n$ usgs_pr_id <dbl> 4960, 4997, 4957, 5023, 5768, 5836, 4948, 5828, 4…\n$ t_state    <chr> \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"CA\", \"…\n$ t_county   <chr> \"Kern County\", \"Kern County\", \"Kern County\", \"Ker…\n$ t_fips     <chr> \"06029\", \"06029\", \"06029\", \"06029\", \"06029\", \"060…\n$ p_name     <chr> \"251 Wind\", \"251 Wind\", \"251 Wind\", \"251 Wind\", \"…\n$ p_year     <dbl> 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1987, 1…\n$ p_tnum     <dbl> 194, 194, 194, 194, 194, 194, 194, 194, 194, 194,…\n$ p_cap      <dbl> 18.43, 18.43, 18.43, 18.43, 18.43, 18.43, 18.43, …\n$ t_manu     <chr> \"Vestas\", \"Vestas\", \"Vestas\", \"Vestas\", \"Vestas\",…\n$ t_model    <chr> \"missing\", \"missing\", \"missing\", \"missing\", \"miss…\n$ t_cap      <dbl> 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 95, 9…\n$ t_hh       <dbl> -9999, -9999, -9999, -9999, -9999, -9999, -9999, …\n$ t_rd       <dbl> -9999, -9999, -9999, -9999, -9999, -9999, -9999, …\n$ t_rsa      <dbl> -9999, -9999, -9999, -9999, -9999, -9999, -9999, …\n$ t_ttlh     <dbl> -9999, -9999, -9999, -9999, -9999, -9999, -9999, …\n$ t_conf_atr <dbl> 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2…\n$ t_conf_loc <dbl> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3…\n$ t_img_date <chr> \"1/1/2012\", \"1/1/2012\", \"1/1/2012\", \"7/31/2016\", …\n$ t_img_srce <chr> \"NAIP\", \"NAIP\", \"NAIP\", \"Digital Globe\", \"Digital…\n$ xlong      <dbl> -118.3607, -118.3612, -118.3604, -118.3640, -118.…\n$ ylat       <dbl> 35.08378, 35.08151, 35.08471, 35.07942, 35.08559,…\n\nCount state\n\n\nus_wind %>% \n  count(t_state, sort = T)\n\n\n# A tibble: 45 x 2\n   t_state     n\n   <chr>   <int>\n 1 TX      13232\n 2 CA       9037\n 3 IA       4280\n 4 OK       3821\n 5 KS       2898\n 6 IL       2602\n 7 MN       2547\n 8 CO       2278\n 9 OR       1868\n10 WA       1744\n# … with 35 more rows\n\nCount project names\n\n\nus_wind %>% \n  count(p_name, sort = T)\n\n\n# A tibble: 1,479 x 2\n   p_name                                     n\n   <chr>                                  <int>\n 1 unknown Tehachapi Wind Resource Area 1  1831\n 2 Green Ridge Power                        516\n 3 Stateline Wind Project                   440\n 4 Mesa Wind Farm                           432\n 5 Sky River                                335\n 6 Cedar Creek                              274\n 7 Peetz Table                              267\n 8 Flat Ridge 2                             261\n 9 Rolling Hills                            259\n10 Woodward Mountain I & II                 242\n# … with 1,469 more rows\n\nPlot longitude and latitude\n\n\nus_wind %>% \n  filter(xlong<100) %>% # filter out outlier\n  ggplot(aes(xlong, ylat)) +\n  geom_point() +\n  borders(\"state\") +\n  coord_map() +\n  theme_void()\n\n\n\n\nDistribution of wind turbines in US States\n\n\ncount_states <- us_wind$t_state %>% factor() %>% fct_count() \n\nus_wind %>% \n  filter(!t_state %in% c(\"AK\", \"HI\", \"GU\", \"PR\")) %>% # Exclude Alaska, Hawaii, Guam, Puerto Rico\n  ggplot(aes(xlong, ylat)) +\n  geom_point() +\n  borders(\"state\") +\n  coord_map() +\n  labs(title = \"Distribution of wind turbines in US\",\n       subtitle = \"Most wind turbines are situated along middle of US.\",\n       caption = \"Source: USGS.gov\") +\n  theme_void()\n\n\n\n\nProjects\n\n\nus_wind_raw <- us_wind\n\nus_wind_processed <- us_wind %>%\n  filter(!t_state %in% c(\"AK\", \"HI\", \"GU\", \"PR\")) %>% \n  na_if(-9999) # replace -9999 as na\n\nwind_projects <- us_wind_processed %>% \n  group_by(p_name, t_state) %>% \n  summarise(turbines = n(),\n            long = mean(xlong),\n            lat = mean(ylat),\n            long_sd = sd(xlong),\n            lat_sd = sd(ylat))\n\nwind_projects %>% \n  ggplot(aes(long, lat, col = turbines, size = turbines)) +\n  geom_point(aes(size = turbines), show.legend = T) +\n  scale_color_continuous(type = \"viridis\") +\n  borders(\"state\") +\n  coord_map() +\n  labs(title = \"Distribution of projects in US\",\n       subtitle = \"The bigest project is 251 Wind, in California\",\n       caption = \"Source: usgs.gov\") +\n  theme_void()\n\n\n\n\nTo find out what is the biggest project:\n\n\nus_wind_processed %>% \n  count(p_name, t_state)\n\n\n# A tibble: 1,440 x 3\n   p_name                      t_state     n\n   <chr>                       <chr>   <int>\n 1 251 Wind                    CA        190\n 2 30 MW Iowa DG Portfolio     IA         10\n 3 6th Space Warning Squadron  MA          2\n 4 Adair                       IA         76\n 5 Adams                       IA         64\n 6 Adams Wind Generations, LLC MN         12\n 7 AFCEE MMR Turbines          MA          2\n 8 AG Land 1                   IA          1\n 9 AG Land 2                   IA          1\n10 AG Land 3                   IA          1\n# … with 1,430 more rows\n\nYear\n\n\nwind_projects <- us_wind_processed %>% \n  group_by(p_name, t_state) %>% \n  summarise(year = min(p_year, na.rm = T), # first year project started\n            turbines = n(),\n            total_capacity_kw = sum(t_cap, na.rm = T),\n            lon = mean(xlong),\n            lat = mean(ylat),\n            lon_sd = sd(xlong),\n            lat_sd = sd(ylat))\n\nwind_projects %>% \n  ggplot(aes(year)) +\n  geom_histogram(fill = \"deepskyblue4\") +\n  labs(title = \"Distribution of projects by year\",\n       subtitle = \"Wind Turbine Projects gained momentum after 2000\",\n       caption = \"Source: usgs.gov\") +\n  theme_clean()\n\n\n\n\n\n\nwind_projects %>% \n  ggplot(aes(lon, lat, size = turbines, col = year))+\n  geom_point(aes(size = turbines), show.legend = T) +\n  scale_color_continuous(type = \"viridis\") +\n  borders(\"state\") +\n  coord_map() +\n  labs(title = \"Age and Scale of US Wind Turbine Projects\",\n       subtitle = \"251 Wind in CA is the oldest project, and the newer projects are situated along middle of the country\",\n       caption = \"Source: usgs.gov\") +\n  theme_void()\n\n\n\n\nCapacity\n\n\nus_wind_processed %>% \n  distinct(p_name, p_cap) %>% # capacity\n  count(p_name, sort = T)\n\n\n# A tibble: 1,425 x 2\n   p_name                        n\n   <chr>                     <int>\n 1 McNeilus                      5\n 2 Bishop Hill I                 3\n 3 Blue Summit                   3\n 4 Capricorn Ridge               3\n 5 Capricorn Ridge expansion     3\n 6 Case Western University       3\n 7 Century Expansion             3\n 8 Crossroads                    3\n 9 Crow Lake                     3\n10 Horse Hollow II               3\n# … with 1,415 more rows\n\nus_wind_processed %>% \n  group_by(p_name, t_state) %>% \n  summarise(year = min(p_year, na.rm = T), # first year project started\n            turbines = n(),\n            total_capacity_kw = sum(t_cap, na.rm = T),\n            lon = mean(xlong),\n            lat = mean(ylat),\n            lon_sd = sd(xlong),\n            lat_sd = sd(ylat)) %>% \n  ungroup()\n\n\n# A tibble: 1,440 x 9\n   p_name         t_state  year turbines total_capacity_…    lon   lat\n   <chr>          <chr>   <dbl>    <int>            <dbl>  <dbl> <dbl>\n 1 251 Wind       CA       1987      190            18050 -118.   35.1\n 2 30 MW Iowa DG… IA       2017       10            30000  -93.4  42.0\n 3 6th Space War… MA       2013        2             3360  -70.5  41.8\n 4 Adair          IA       2008       76           174800  -94.7  41.5\n 5 Adams          IA       2016       64           154284  -94.7  40.9\n 6 Adams Wind Ge… MN       2011       12            20040  -94.7  44.9\n 7 AFCEE MMR Tur… MA       2011        2             3000  -70.5  41.8\n 8 AG Land 1      IA       2012        1             1600  -93.3  42.2\n 9 AG Land 2      IA       2012        1             1600  -93.4  42.1\n10 AG Land 3      IA       2012        1             1600  -93.4  42.1\n# … with 1,430 more rows, and 2 more variables: lon_sd <dbl>,\n#   lat_sd <dbl>\n\nHow has turbine capacity changed over time?\n\n\nturbine <- us_wind_processed %>% \n  group_by(p_name, t_state) %>% \n  summarise(year = min(p_year, na.rm = T), # first year project started\n            turbines = n(),\n            total_capacity_kw = sum(t_cap),\n            lon = mean(xlong),\n            lat = mean(ylat),\n            lon_sd = sd(xlong),\n            lat_sd = sd(ylat)) %>% \n  ungroup()\n\n\nturbine %>% \n  ggplot(aes(year, total_capacity_kw/turbines)) +\n  geom_point() +\n  geom_smooth(method= \"lm\") +\n  labs(title = \"Change in Total Capacity per Turbine over Time\",\n       subtitle = \"Total Capacity per Turbine increased over time\",\n       caption = \"Source: usgs.gov\") +\n  theme_few()\n\n\n\n\nTurbine models\n\n\nturbine_models <- us_wind_processed %>% \n  group_by(t_model) %>% \n  summarize(t_cap = median(t_cap), # turbine capacity (kW)\n            t_hh = median(t_hh), # turbine hub height (m)\n            t_rd = median(t_rd), # turbine rotor diameter (m)\n            t_rsw = median(t_rsa), # turbine rotor swept area (m2)\n            t_ttlh = median(t_ttlh), # turbine total height calculated (m)\n            turbines = n(), # number of turbines\n            projects = n_distinct(p_name)) %>%  # number of projects\n  arrange(desc(projects))\n\nturbine_models %>% \n  ggplot(aes(t_ttlh, t_cap)) +\n  geom_point() +\n  labs(title = \"Relationship between turbine height and capacity\",\n       subtitle = \"Taller Turbines have higher capacity\",\n       x = \"Turbine Total Height Calculated (m)\",\n       y = \"Turbine Capacity (kW)\") +\n  theme_clean()\n\n\n\n\nLearning points:\nPlotting a map of US using ggplot\nReplacing missing data using dplyr::na_if to replace all -9999\nData cleaning should be done at the start\nReferences\nhttps://www.youtube.com/watch?v=O1oDIQV6VKU&list=PL19ev-r1GBwkuyiwnxoHTRC8TTqP8OEi8&index=78\n\n\n\n",
    "preview": "posts/20210619_Tidytuesday wind data/tidy-tuesday-y18w32_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-06-20T08:54:53+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210601_Tidytuesday College Major/",
    "title": "Tidy Tuesday Series",
    "description": "2018 Week 29 - College Major",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-06-01",
    "categories": [],
    "contents": "\n\n\nknitr::opts_chunk$set(echo = TRUE)\n\n# load packages\nlibrary(tidytuesdayR)\nlibrary(tidyverse)\n\n# to download data\ntt_data <- tt_load(2018, week = 29)\ntt_data$`recent-grads`\n\n# to view readme\nreadme(tt_data)\n\n# to see available datasets:\nprint(tt_data) # recent-grads\n\nrecent_grads <- tt_data$`recent-grads`\n\n\n\nLoad Packages\n\n\nlibrary(tidyverse)\nlibrary(tidytuesdayR)\nlibrary(ggthemes)\nlibrary(ggrepel)\nlibrary(broom)\nlibrary(plotly)\n\n\n\nLoad Data from tidytuesdayR package\n\n\n# to download data\ntt_data <- tt_load(2018, week = 29)\ntt_data$`recent-grads`\n\n# to view readme\nreadme(tt_data)\n\nrecent_grads <- tt_data$`recent-grads`\n\n\n\nExplore Dataset\n\n\ngrads_processed <- recent_grads %>% \n  janitor::clean_names() %>% \n  mutate(major = str_to_title(major))\n\n# check for na values\nsum(is.na(grads_processed))\n\n\n[1] 4\n\n# identify location\nwhich(is.na(grads_processed))\n\n\n[1]  541  714  887 1233\n\ngrads_processed[541,]\n\n\n# A tibble: 1 x 21\n   rank major_code major total   men women major_category share_women\n  <dbl>      <dbl> <chr> <dbl> <dbl> <dbl> <chr>                <dbl>\n1    NA         NA <NA>     NA    NA    NA <NA>                    NA\n# … with 13 more variables: sample_size <dbl>, employed <dbl>,\n#   full_time <dbl>, part_time <dbl>, full_time_year_round <dbl>,\n#   unemployed <dbl>, unemployment_rate <dbl>, median <dbl>,\n#   p25th <dbl>, p75th <dbl>, college_jobs <dbl>,\n#   non_college_jobs <dbl>, low_wage_jobs <dbl>\n\ngrads_processed[714,]\n\n\n# A tibble: 1 x 21\n   rank major_code major total   men women major_category share_women\n  <dbl>      <dbl> <chr> <dbl> <dbl> <dbl> <chr>                <dbl>\n1    NA         NA <NA>     NA    NA    NA <NA>                    NA\n# … with 13 more variables: sample_size <dbl>, employed <dbl>,\n#   full_time <dbl>, part_time <dbl>, full_time_year_round <dbl>,\n#   unemployed <dbl>, unemployment_rate <dbl>, median <dbl>,\n#   p25th <dbl>, p75th <dbl>, college_jobs <dbl>,\n#   non_college_jobs <dbl>, low_wage_jobs <dbl>\n\ngrads_processed[887,]\n\n\n# A tibble: 1 x 21\n   rank major_code major total   men women major_category share_women\n  <dbl>      <dbl> <chr> <dbl> <dbl> <dbl> <chr>                <dbl>\n1    NA         NA <NA>     NA    NA    NA <NA>                    NA\n# … with 13 more variables: sample_size <dbl>, employed <dbl>,\n#   full_time <dbl>, part_time <dbl>, full_time_year_round <dbl>,\n#   unemployed <dbl>, unemployment_rate <dbl>, median <dbl>,\n#   p25th <dbl>, p75th <dbl>, college_jobs <dbl>,\n#   non_college_jobs <dbl>, low_wage_jobs <dbl>\n\ngrads_processed[1233, ]\n\n\n# A tibble: 1 x 21\n   rank major_code major total   men women major_category share_women\n  <dbl>      <dbl> <chr> <dbl> <dbl> <dbl> <chr>                <dbl>\n1    NA         NA <NA>     NA    NA    NA <NA>                    NA\n# … with 13 more variables: sample_size <dbl>, employed <dbl>,\n#   full_time <dbl>, part_time <dbl>, full_time_year_round <dbl>,\n#   unemployed <dbl>, unemployment_rate <dbl>, median <dbl>,\n#   p25th <dbl>, p75th <dbl>, college_jobs <dbl>,\n#   non_college_jobs <dbl>, low_wage_jobs <dbl>\n\n# remove NA\ngrads_processed_2 <- grads_processed %>% \n  drop_na()\n\nsum(is.na(grads_processed_2))\n\n\n[1] 0\n\nNote:\nchange CAPS to title for Major\ncheck for missing values\nremove NA values\nExploratory questions\nWhich majors make the most money?\nWhich majors are the most popular?\nBreakdown of popular majors by gender?\nHow does gender breakdown relate to typical earnings?\nSimplify the dataset again:\n\n\ncleaned_data <- grads_processed_2 %>% \n  select(rank, major_category, major, total, men, women, share_women, sample_size, median, p25th, p75th ) %>% \n  mutate(pct_sample = sample_size/total*100)\n\nglimpse(cleaned_data)\n\n\nRows: 172\nColumns: 12\n$ rank           <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n$ major_category <chr> \"Engineering\", \"Engineering\", \"Engineering\", …\n$ major          <chr> \"Petroleum Engineering\", \"Mining And Mineral …\n$ total          <dbl> 2339, 756, 856, 1258, 32260, 2573, 3777, 1792…\n$ men            <dbl> 2057, 679, 725, 1123, 21239, 2200, 2110, 832,…\n$ women          <dbl> 282, 77, 131, 135, 11021, 373, 1667, 960, 109…\n$ share_women    <dbl> 0.1205643, 0.1018519, 0.1530374, 0.1073132, 0…\n$ sample_size    <dbl> 36, 7, 3, 16, 289, 17, 51, 10, 1029, 631, 399…\n$ median         <dbl> 110000, 75000, 73000, 70000, 65000, 65000, 62…\n$ p25th          <dbl> 95000, 55000, 50000, 43000, 50000, 50000, 530…\n$ p75th          <dbl> 125000, 90000, 105000, 80000, 75000, 102000, …\n$ pct_sample     <dbl> 1.5391193, 0.9259259, 0.3504673, 1.2718601, 0…\n\n# 16 types of major category\ncleaned_data %>% \n  select(major_category) %>% \n  group_by(major_category) %>% \n  summarise(n = n())\n\n\n# A tibble: 16 x 2\n   major_category                          n\n   <chr>                               <int>\n 1 Agriculture & Natural Resources         9\n 2 Arts                                    8\n 3 Biology & Life Science                 14\n 4 Business                               13\n 5 Communications & Journalism             4\n 6 Computers & Mathematics                11\n 7 Education                              16\n 8 Engineering                            29\n 9 Health                                 12\n10 Humanities & Liberal Arts              15\n11 Industrial Arts & Consumer Services     7\n12 Interdisciplinary                       1\n13 Law & Public Policy                     5\n14 Physical Sciences                      10\n15 Psychology & Social Work                9\n16 Social Science                          9\n\n# 172 types of major\ncleaned_data %>% \n  select(major) %>% \n  group_by(major) %>% \n  summarise(n = n())\n\n\n# A tibble: 172 x 2\n   major                                     n\n   <chr>                                 <int>\n 1 Accounting                                1\n 2 Actuarial Science                         1\n 3 Advertising And Public Relations          1\n 4 Aerospace Engineering                     1\n 5 Agricultural Economics                    1\n 6 Agriculture Production And Management     1\n 7 Animal Sciences                           1\n 8 Anthropology And Archeology               1\n 9 Applied Mathematics                       1\n10 Architectural Engineering                 1\n# … with 162 more rows\n\n# see unique values for character columns\ncleaned_data %>% \n  select_if(is_character) %>% \n  lapply(., function(x) unique(x))\n\n\n$major_category\n [1] \"Engineering\"                        \n [2] \"Business\"                           \n [3] \"Physical Sciences\"                  \n [4] \"Law & Public Policy\"                \n [5] \"Computers & Mathematics\"            \n [6] \"Industrial Arts & Consumer Services\"\n [7] \"Arts\"                               \n [8] \"Health\"                             \n [9] \"Social Science\"                     \n[10] \"Biology & Life Science\"             \n[11] \"Education\"                          \n[12] \"Agriculture & Natural Resources\"    \n[13] \"Humanities & Liberal Arts\"          \n[14] \"Psychology & Social Work\"           \n[15] \"Communications & Journalism\"        \n[16] \"Interdisciplinary\"                  \n\n$major\n  [1] \"Petroleum Engineering\"                                            \n  [2] \"Mining And Mineral Engineering\"                                   \n  [3] \"Metallurgical Engineering\"                                        \n  [4] \"Naval Architecture And Marine Engineering\"                        \n  [5] \"Chemical Engineering\"                                             \n  [6] \"Nuclear Engineering\"                                              \n  [7] \"Actuarial Science\"                                                \n  [8] \"Astronomy And Astrophysics\"                                       \n  [9] \"Mechanical Engineering\"                                           \n [10] \"Electrical Engineering\"                                           \n [11] \"Computer Engineering\"                                             \n [12] \"Aerospace Engineering\"                                            \n [13] \"Biomedical Engineering\"                                           \n [14] \"Materials Science\"                                                \n [15] \"Engineering Mechanics Physics And Science\"                        \n [16] \"Biological Engineering\"                                           \n [17] \"Industrial And Manufacturing Engineering\"                         \n [18] \"General Engineering\"                                              \n [19] \"Architectural Engineering\"                                        \n [20] \"Court Reporting\"                                                  \n [21] \"Computer Science\"                                                 \n [22] \"Electrical Engineering Technology\"                                \n [23] \"Materials Engineering And Materials Science\"                      \n [24] \"Management Information Systems And Statistics\"                    \n [25] \"Civil Engineering\"                                                \n [26] \"Construction Services\"                                            \n [27] \"Operations Logistics And E-Commerce\"                              \n [28] \"Miscellaneous Engineering\"                                        \n [29] \"Public Policy\"                                                    \n [30] \"Environmental Engineering\"                                        \n [31] \"Engineering Technologies\"                                         \n [32] \"Miscellaneous Fine Arts\"                                          \n [33] \"Geological And Geophysical Engineering\"                           \n [34] \"Nursing\"                                                          \n [35] \"Finance\"                                                          \n [36] \"Economics\"                                                        \n [37] \"Business Economics\"                                               \n [38] \"Industrial Production Technologies\"                               \n [39] \"Nuclear, Industrial Radiology, And Biological Technologies\"       \n [40] \"Accounting\"                                                       \n [41] \"Mathematics\"                                                      \n [42] \"Computer And Information Systems\"                                 \n [43] \"Physics\"                                                          \n [44] \"Medical Technologies Technicians\"                                 \n [45] \"Information Sciences\"                                             \n [46] \"Statistics And Decision Science\"                                  \n [47] \"Applied Mathematics\"                                              \n [48] \"Pharmacology\"                                                     \n [49] \"Oceanography\"                                                     \n [50] \"Engineering And Industrial Management\"                            \n [51] \"Medical Assisting Services\"                                       \n [52] \"Mathematics And Computer Science\"                                 \n [53] \"Computer Programming And Data Processing\"                         \n [54] \"Cognitive Science And Biopsychology\"                              \n [55] \"School Student Counseling\"                                        \n [56] \"International Relations\"                                          \n [57] \"General Business\"                                                 \n [58] \"Architecture\"                                                     \n [59] \"International Business\"                                           \n [60] \"Pharmacy Pharmaceutical Sciences And Administration\"              \n [61] \"Molecular Biology\"                                                \n [62] \"Miscellaneous Business & Medical Administration\"                  \n [63] \"Agriculture Production And Management\"                            \n [64] \"General Agriculture\"                                              \n [65] \"Miscellaneous Engineering Technologies\"                           \n [66] \"Mechanical Engineering Related Technologies\"                      \n [67] \"Genetics\"                                                         \n [68] \"Miscellaneous Social Sciences\"                                    \n [69] \"United States History\"                                            \n [70] \"Industrial And Organizational Psychology\"                         \n [71] \"Agricultural Economics\"                                           \n [72] \"Physical Sciences\"                                                \n [73] \"Military Technologies\"                                            \n [74] \"Chemistry\"                                                        \n [75] \"Electrical, Mechanical, And Precision Technologies And Production\"\n [76] \"Business Management And Administration\"                           \n [77] \"Marketing And Marketing Research\"                                 \n [78] \"Political Science And Government\"                                 \n [79] \"Geography\"                                                        \n [80] \"Microbiology\"                                                     \n [81] \"Computer Administration Management And Security\"                  \n [82] \"Biochemical Sciences\"                                             \n [83] \"Botany\"                                                           \n [84] \"Computer Networking And Telecommunications\"                       \n [85] \"Geology And Earth Science\"                                        \n [86] \"Human Resources And Personnel Management\"                         \n [87] \"Pre-Law And Legal Studies\"                                        \n [88] \"Miscellaneous Health Medical Professions\"                         \n [89] \"Public Administration\"                                            \n [90] \"Geosciences\"                                                      \n [91] \"Social Psychology\"                                                \n [92] \"Environmental Science\"                                            \n [93] \"Communications\"                                                   \n [94] \"Criminal Justice And Fire Protection\"                             \n [95] \"Commercial Art And Graphic Design\"                                \n [96] \"Journalism\"                                                       \n [97] \"Multi-Disciplinary Or General Science\"                            \n [98] \"Advertising And Public Relations\"                                 \n [99] \"Area Ethnic And Civilization Studies\"                             \n[100] \"Special Needs Education\"                                          \n[101] \"Physiology\"                                                       \n[102] \"Criminology\"                                                      \n[103] \"Nutrition Sciences\"                                               \n[104] \"Health And Medical Administrative Services\"                       \n[105] \"Communication Technologies\"                                       \n[106] \"Transportation Sciences And Technologies\"                         \n[107] \"Natural Resources Management\"                                     \n[108] \"Neuroscience\"                                                     \n[109] \"Multi/Interdisciplinary Studies\"                                  \n[110] \"Atmospheric Sciences And Meteorology\"                             \n[111] \"Forestry\"                                                         \n[112] \"Soil Science\"                                                     \n[113] \"General Education\"                                                \n[114] \"History\"                                                          \n[115] \"French German Latin And Other Common Foreign Language Studies\"    \n[116] \"Intercultural And International Studies\"                          \n[117] \"Social Science Or History Teacher Education\"                      \n[118] \"Community And Public Health\"                                      \n[119] \"Mathematics Teacher Education\"                                    \n[120] \"Educational Administration And Supervision\"                       \n[121] \"Health And Medical Preparatory Programs\"                          \n[122] \"Miscellaneous Biology\"                                            \n[123] \"Biology\"                                                          \n[124] \"Sociology\"                                                        \n[125] \"Mass Media\"                                                       \n[126] \"Treatment Therapy Professions\"                                    \n[127] \"Hospitality Management\"                                           \n[128] \"Language And Drama Education\"                                     \n[129] \"Linguistics And Comparative Language And Literature\"              \n[130] \"Miscellaneous Education\"                                          \n[131] \"Interdisciplinary Social Sciences\"                                \n[132] \"Ecology\"                                                          \n[133] \"Secondary Teacher Education\"                                      \n[134] \"General Medical And Health Services\"                              \n[135] \"Philosophy And Religious Studies\"                                 \n[136] \"Art And Music Education\"                                          \n[137] \"English Language And Literature\"                                  \n[138] \"Elementary Education\"                                             \n[139] \"Physical Fitness Parks Recreation And Leisure\"                    \n[140] \"Liberal Arts\"                                                     \n[141] \"Film Video And Photographic Arts\"                                 \n[142] \"General Social Sciences\"                                          \n[143] \"Plant Science And Agronomy\"                                       \n[144] \"Science And Computer Teacher Education\"                           \n[145] \"Psychology\"                                                       \n[146] \"Music\"                                                            \n[147] \"Physical And Health Education Teaching\"                           \n[148] \"Art History And Criticism\"                                        \n[149] \"Fine Arts\"                                                        \n[150] \"Family And Consumer Sciences\"                                     \n[151] \"Social Work\"                                                      \n[152] \"Animal Sciences\"                                                  \n[153] \"Visual And Performing Arts\"                                       \n[154] \"Teacher Education: Multiple Levels\"                               \n[155] \"Miscellaneous Psychology\"                                         \n[156] \"Human Services And Community Organization\"                        \n[157] \"Humanities\"                                                       \n[158] \"Theology And Religious Vocations\"                                 \n[159] \"Studio Arts\"                                                      \n[160] \"Cosmetology Services And Culinary Arts\"                           \n[161] \"Miscellaneous Agriculture\"                                        \n[162] \"Anthropology And Archeology\"                                      \n[163] \"Communication Disorders Sciences And Services\"                    \n[164] \"Early Childhood Education\"                                        \n[165] \"Other Foreign Languages\"                                          \n[166] \"Drama And Theater Arts\"                                           \n[167] \"Composition And Rhetoric\"                                         \n[168] \"Zoology\"                                                          \n[169] \"Educational Psychology\"                                           \n[170] \"Clinical Psychology\"                                              \n[171] \"Counseling Psychology\"                                            \n[172] \"Library Science\"                                                  \n\n# see summary statistics for dbl columns\ncleaned_data %>% \n  select_if(is.numeric) %>% \n  lapply(., function(x) broom::tidy(summary(x)))\n\n\n$rank\n# A tibble: 1 x 6\n  minimum    q1 median  mean    q3 maximum\n    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1       1  44.8   87.5  87.4  130.     173\n\n$total\n# A tibble: 1 x 6\n  minimum    q1 median   mean     q3 maximum\n    <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1     124 4550.  15104 39370. 38910.  393735\n\n$men\n# A tibble: 1 x 6\n  minimum    q1 median   mean    q3 maximum\n    <dbl> <dbl>  <dbl>  <dbl> <dbl>   <dbl>\n1     119 2178.   5434 16723. 14631  173809\n\n$women\n# A tibble: 1 x 6\n  minimum    q1 median   mean     q3 maximum\n    <dbl> <dbl>  <dbl>  <dbl>  <dbl>   <dbl>\n1       0 1778.  8386. 22647. 22554.  307087\n\n$share_women\n# A tibble: 1 x 6\n  minimum    q1 median  mean    q3 maximum\n    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1       0 0.336  0.534 0.522 0.703   0.969\n\n$sample_size\n# A tibble: 1 x 6\n  minimum    q1 median  mean    q3 maximum\n    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1       2    42    131  358.   339    4212\n\n$median\n# A tibble: 1 x 6\n  minimum    q1 median   mean    q3 maximum\n    <dbl> <dbl>  <dbl>  <dbl> <dbl>   <dbl>\n1   22000 33000  36000 40077. 45000  110000\n\n$p25th\n# A tibble: 1 x 6\n  minimum    q1 median   mean    q3 maximum\n    <dbl> <dbl>  <dbl>  <dbl> <dbl>   <dbl>\n1   18500 24000  27000 29487. 33250   95000\n\n$p75th\n# A tibble: 1 x 6\n  minimum    q1 median   mean    q3 maximum\n    <dbl> <dbl>  <dbl>  <dbl> <dbl>   <dbl>\n1   22000 41750  47000 51387. 58500  125000\n\n$pct_sample\n# A tibble: 1 x 6\n  minimum    q1 median  mean    q3 maximum\n    <dbl> <dbl>  <dbl> <dbl> <dbl>   <dbl>\n1   0.161 0.660  0.881 0.909  1.12    3.23\n\nChecking Sample Size\nWhich of the survey results had higher sample size?\nPlot sample size (x) and median (x), label major\n\n\ncleaned_data %>% \n  ggplot(aes(sample_size, median, label = major)) +\n  geom_point() +\n  geom_text_repel(aes(label = major)) +\n  scale_x_log10()\n\n\n\n\nA larger sample size would mean that the survey results were more credible.\nWhich categories of major make the most money?\nRank median salary in descending order and fct_reorder, ggplot\n\n\ntheme_set(theme_few())\n\ncleaned_data %>% \n  select(major_category, median) %>% \n  group_by(major_category) %>% \n  summarise(median = median(median)) %>% \n  arrange(desc(median))\n\n\n# A tibble: 16 x 2\n   major_category                      median\n   <chr>                                <dbl>\n 1 Engineering                          57000\n 2 Computers & Mathematics              45000\n 3 Business                             40000\n 4 Physical Sciences                    39500\n 5 Social Science                       38000\n 6 Biology & Life Science               36300\n 7 Law & Public Policy                  36000\n 8 Agriculture & Natural Resources      35000\n 9 Communications & Journalism          35000\n10 Health                               35000\n11 Industrial Arts & Consumer Services  35000\n12 Interdisciplinary                    35000\n13 Education                            32750\n14 Humanities & Liberal Arts            32000\n15 Arts                                 30750\n16 Psychology & Social Work             30000\n\ncleaned_data %>% \n  ggplot(aes(fct_reorder(major_category, median), median)) +\n  geom_boxplot() +\n  labs(title = \"Distribution of median salary for different major categories\",\n       caption = \"Source: American Community Survey 2010-2012\",\n       x = \"\",\n       y = \"Median Salary\") +\n  scale_y_continuous(labels = scales::comma) +\n  expand_limits(y = 0) +\n  coord_flip()\n\n\n\n\nWhat are the highest earning mojors?\n\n\ncleaned_data %>% \n  filter(sample_size>100) %>% \n  arrange(desc(median)) %>% \n  slice_head(n=20) %>% \n  ggplot(aes(fct_reorder(major, median), median, col = major_category)) +\n  geom_point(size = 2) +\n  geom_errorbar(aes(ymin = p25th, ymax = p75th)) +\n  scale_y_continuous(labels = scales::comma) +\n  expand_limits(y = 0) +\n  labs(title = \"Top 20 Majors by Median Income, with at least 100 people surveyed\",\n       subtitle = \"Error bars represent 25th and 75th percentile\",\n       col = \"Major Categories\",\n       x = \"\",\n       y = \"Median Salary\",\n       caption = \"Source: Americal Community Survey 2010-2012\") +\n  coord_flip()\n\n\n\n\nWhich majors are the most popular?\n\n\ncleaned_data %>% \n  count(major, wt = total, sort = T) %>% \n  slice_head(n = 20) %>% \n  ggplot(aes(fct_reorder(major, n), n)) +\n  geom_col(fill = \"forestgreen\") +\n  coord_flip()\n\n\n\n\nBreakdown of popular majors by gender?\n\n\ncleaned_data %>% \n  pivot_longer(cols = c(\"men\", \"women\"),\n               names_to = \"gender\",\n               values_to = \"number\") %>% \n  arrange(desc(total)) %>% \n  head(20) %>% \n  ggplot(aes(major, number, fill = gender)) +\n  geom_col(position = \"stack\") +\n  coord_flip()\n\n\n\n\nHow does gender breakdown relate to typical earnings?\nx - share women y - median\n\n\ncleaned_data %>% \n  ggplot(aes(x = share_women, y = median, label = major,\n             col = major_category, size = sample_size)) +\n  geom_point() +\n  geom_smooth(aes(group = 1), method = \"lm\") +\n  geom_text_repel(aes(label = major, force = 0.2)) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nSINGAPORE 2018 GRADUATE SURVEY\nImport Singapore’s data, downloaded from Data.gov.sg\n\n\nsg <- read_csv(\"~/Desktop/r-tidyverse/graduate-employment-survey-ntu-nus-sit-smu-suss-sutd.csv\")\n\n\n\n\n\nsg <- read_csv(\"~/Desktop/r-tidyverse/graduate-employment-survey-ntu-nus-sit-smu-suss-sutd.csv\")\n\n\n\nExplore the dataset\n\n\nglimpse(sg)\n\n\nRows: 703\nColumns: 12\n$ year                      <dbl> 2013, 2013, 2013, 2013, 2013, 2013…\n$ university                <chr> \"Nanyang Technological University\"…\n$ school                    <chr> \"College of Business (Nanyang Busi…\n$ degree                    <chr> \"Accountancy and Business\", \"Accou…\n$ employment_rate_overall   <chr> \"97.4\", \"97.1\", \"90.9\", \"87.5\", \"9…\n$ employment_rate_ft_perm   <chr> \"96.1\", \"95.7\", \"85.7\", \"87.5\", \"9…\n$ basic_monthly_mean        <chr> \"3701\", \"2850\", \"3053\", \"3557\", \"3…\n$ basic_monthly_median      <chr> \"3200\", \"2700\", \"3000\", \"3400\", \"3…\n$ gross_monthly_mean        <chr> \"3727\", \"2938\", \"3214\", \"3615\", \"3…\n$ gross_monthly_median      <chr> \"3350\", \"2700\", \"3000\", \"3400\", \"3…\n$ gross_mthly_25_percentile <chr> \"2900\", \"2700\", \"2700\", \"3000\", \"3…\n$ gross_mthly_75_percentile <chr> \"4000\", \"2900\", \"3500\", \"4100\", \"3…\n\nsum(is.na(sg)) # no missing data\n\n\n[1] 0\n\n# convert to numeric\nsg_cleaned <- sg\n\nsg_cleaned <- sg_cleaned %>% \n  mutate(across(c(7:12), as.numeric))\n\nglimpse(sg_cleaned)\n\n\nRows: 703\nColumns: 12\n$ year                      <dbl> 2013, 2013, 2013, 2013, 2013, 2013…\n$ university                <chr> \"Nanyang Technological University\"…\n$ school                    <chr> \"College of Business (Nanyang Busi…\n$ degree                    <chr> \"Accountancy and Business\", \"Accou…\n$ employment_rate_overall   <chr> \"97.4\", \"97.1\", \"90.9\", \"87.5\", \"9…\n$ employment_rate_ft_perm   <chr> \"96.1\", \"95.7\", \"85.7\", \"87.5\", \"9…\n$ basic_monthly_mean        <dbl> 3701, 2850, 3053, 3557, 3494, 2952…\n$ basic_monthly_median      <dbl> 3200, 2700, 3000, 3400, 3500, 2900…\n$ gross_monthly_mean        <dbl> 3727, 2938, 3214, 3615, 3536, 3166…\n$ gross_monthly_median      <dbl> 3350, 2700, 3000, 3400, 3500, 3125…\n$ gross_mthly_25_percentile <dbl> 2900, 2700, 2700, 3000, 3100, 2893…\n$ gross_mthly_75_percentile <dbl> 4000, 2900, 3500, 4100, 3816, 3365…\n\nmax(sg_cleaned$year) # 2018\n\n\n[1] 2018\n\nTransform:\nFilter to see year 2018\nNeed to remove any missing values\nNeed to recode the degree into smaller number of categories using fct_collapse\nNeed to remove *, # and ^ using str_replace_all\nNeed to change to lower case for easier typing using str_to_lower\nNeed to remove white space in case there are any using trim_ws or str_trim\n\n\nsg_2018 <- sg_cleaned %>% \n  filter(year == 2018) %>% \n  drop_na() %>% \n  arrange(desc(basic_monthly_median)) %>% \n  mutate(school = str_replace_all(school, \"\\\\*\", \"\"),\n         degree = str_replace_all(degree, \"\\\\#\", \"\"),\n         degree = str_replace_all(degree, \"\\\\^\", \"\")) %>% \n  mutate(degree = str_to_lower(degree),\n         degree = trimws(degree)) %>% \n  mutate(degree_recode = fct_collapse(degree,\n                                      \n                  accountancy = c(\"accountancy\",\n                                  \"accountancy (cum laude and above)\",\n                                  \"accountancy and business\",\n                                  \"bachelor of accountancy\",\n                                  \"bachelor of accountancy with honours\",\n                                  \"bachelor of business administration (accountancy)\",\n                                  \"bachelor of business administration (accountancy) (hons)\"),\n                  \n                   arts = c(\"art, design & media\",\n                           \"arts (with education)\",\n                           \"bachelor of arts\",\n                           \"bachelor of arts (hons)\",\n                           \"bachelor of arts (industrial design)\",\n                           \"bachelor of arts in game design\",\n                           \"bachelor of arts with honours\",\n                           \"bachelor of arts with honours in communication design\",\n                           \"bachelor of arts with honours in interior design\",\n                           \"bachelor of fine arts in digital art and animation\",\n                           \"history\",\n                           \"philosophy\"),\n                  \n                  bizad = c(\"bachelor of business administration\",\n                            \"bachelor of business administration (hons)\",\n                            \"bachelor of business administration in food business management\",\n                            \"bachelor of hospitality business with honours\",\n                            \"bachelor of science in finance\",\n                            \"bachelor of science in marketing\",\n                            \"business\",\n                            \"business and computing\",\n                            \"business management\",\n                            \"business management (cum laude and above)\",\n                            \"sport science and management\"),\n                  \n                   building_real_estate = c(\"bachelor of science (project and facilities management)\",\n                                           \"bachelor of science (real estate)\"),\n                  \n                  comsci = c(\"bachelor of computing (computer science)\",\n                             \"bachelor of computing (information systems)\",\n                             \"bachelor of computing (information systems)\",\n                             \"bachelor of science in computer science and game design\",\n                             \"bachelor of science in computer science in real-time interactive simulation\",\n                             \"bachelor of science with honours in computing science\",\n                             \"computer science\"),\n                  \n                  communications = c(\"communication studies\"),\n                  \n                  early_childhood = c(\"bachelor of science in early childhood education\"),\n                  \n                  engineering = c(\"aerospace engineering\",\n                                  \"bachelor of engineering (biomedical engineering)\",\n                                  \"bachelor of engineering (chemical engineering)\",\n                                  \"bachelor of engineering (civil engineering)\",\n                                  \"bachelor of engineering (computer engineering)\",\n                                  \"bachelor of engineering (electrical engineering)\",\n                                  \"bachelor of engineering (environmental engineering)\",\n                                  \"bachelor of engineering (industrial and systems engineering)\",\n                                  \"bachelor of engineering (materials science and engineering)\",\n                                  \"bachelor of engineering (mechanical engineering)\",\n                                  \"bachelor of engineering with honours in aeronautical engineering\",\n                                  \"bachelor of engineering with honours in aerospace systems\",\n                                  \"bachelor of engineering with honours in chemical engineering\",\n                                  \"bachelor of engineering with honours in electrical power engineering\",\n                                  \"bachelor of engineering with honours in information & communications technology (information security)\",\n                                  \"bachelor of engineering with honours in information & communications technology (software engineering)\",\n                                  \"bachelor of engineering with honours in marine engineering\",\n                                  \"bachelor of engineering with honours in mechanical design and manufacturing engineering\",\n                                  \"bachelor of engineering with honours in mechanical design engineering\",\n                                  \"bachelor of engineering with honours in mechatronics\",\n                                  \"bachelor of engineering with honours in naval architecture\",\n                                  \"bachelor of engineering with honours in offshore engineering\",\n                                  \"bachelor of engineering with honours in sustainable infrastructure engineering (land)\",\n                                  \"bachelor of science in chemical engineering\",\n                                  \"bachelor of science in electrical engineering & information technology\",\n                                  \"bioengineering\",\n                                  \"chemical & biomolecular engineering\",\n                                  \"civil engineering\",\n                                  \"computer engineering\",\n                                  \"electrical & electronic engineering\",\n                                  \"environmental engineering\",\n                                  \"materials engineering\",\n                                  \"mechanical engineering\"),\n                  \n                  healthsci = c(\"bachelor in science (diagnostic radiography)\",\n                                \"bachelor in science (occupational therapy)\",\n                                \"bachelor in science (physiotherapy)\",\n                                \"bachelor of science (nursing)\",\n                                \"bachelor of science (nursing) (hons)\",\n                                \"bachelor of science (pharmacy)\",\n                                \"bachelor of science with honours in nursing\",\n                                \"bachelor of dental surgery\"),\n                  \n                  info_sys = c(\"information systems\",\n                              \"information engineering & media\",\n                              \"information systems (cum laude and above)\"),\n                  \n                  languages = c(\"english\",\n                                \"chinese\",\n                                \"linguistics & multilingual studies\"),\n                  \n                  law = c(\"bachelor of laws\",\n                          \"law\",\n                          \"law (cum laude and above)\"),\n                  \n                  music = c(\"bachelor of music\"),\n                  \n                   science = c(\"bachelor of environmental studies\",\n                              \"bachelor of science\",\n                              \"bachelor of science (business analytics)\",\n                              \"bachelor of science (hons)\",\n                              \"bachelor of science with honours\",\n                              \"bachelor of science with honours in food and human nutrition\",\n                              \"biological sciences\",\n                              \"biomedical sciences & chinese medicine\",\n                              \"chemistry & biological chemistry\",\n                              \"environmental earth systems science\",\n                              \"mathematical sciences\",\n                              \"physics & applied physics\",\n                              \"maritime studies\",\n                              \"science (with education)\"),\n                  \n                 \n                  social_science = c(\"bachelor of social sciences\",\n                                     \"bachelor of arts with honours in criminology and security\",\n                                     \"economics\",\n                                     \"economics (cum laude and above)\",\n                                     \"mathematics & economics\",\n                                     \"psychology\",\n                                     \"public policy and global affairs\",\n                                     \"social sciences\",\n                                     \"social sciences (cum laude and above)\",\n                                     \"sociology\")\n\n                  )) %>% \n  select(year, university, school, degree, degree_recode, employment_rate_ft_perm, \n         gross_monthly_median, gross_mthly_25_percentile, gross_mthly_75_percentile)\n\n# degree <- sg_2018 %>% \n # count(degree)\n\n# print(degree, n = 115)\n\n\nsg_2018\n\n\n# A tibble: 117 x 9\n    year university   school   degree   degree_recode employment_rate…\n   <dbl> <chr>        <chr>    <chr>    <fct>         <chr>           \n 1  2018 National Un… \"Facult… bachelo… law           92.6            \n 2  2018 National Un… \"Yale-N… bachelo… science       78.9            \n 3  2018 Singapore M… \"School… law (cu… law           92.6            \n 4  2018 Singapore M… \"School… informa… info_sys      97.2            \n 5  2018 Singapore M… \"School… law      law           90.8            \n 6  2018 Nanyang Tec… \"Colleg… busines… bizad         100             \n 7  2018 National Un… \"School… bachelo… science       93.5            \n 8  2018 National Un… \"School… bachelo… comsci        95.3            \n 9  2018 National Un… \"School… bachelo… comsci        91.8            \n10  2018 National Un… \"Facult… bachelo… healthsci     100             \n# … with 107 more rows, and 3 more variables:\n#   gross_monthly_median <dbl>, gross_mthly_25_percentile <dbl>,\n#   gross_mthly_75_percentile <dbl>\n\nsg_2018$employment_rate_ft_perm <- as.numeric(sg_2018$employment_rate_ft_perm)\n\nglimpse(sg_2018)\n\n\nRows: 117\nColumns: 9\n$ year                      <dbl> 2018, 2018, 2018, 2018, 2018, 2018…\n$ university                <chr> \"National University of Singapore\"…\n$ school                    <chr> \"Faculty of Law\", \"Yale-NUS Colleg…\n$ degree                    <chr> \"bachelor of laws\", \"bachelor of s…\n$ degree_recode             <fct> law, science, law, info_sys, law, …\n$ employment_rate_ft_perm   <dbl> 92.6, 78.9, 92.6, 97.2, 90.8, 100.…\n$ gross_monthly_median      <dbl> 5500, 4800, 5513, 4685, 4500, 4575…\n$ gross_mthly_25_percentile <dbl> 4500, 3600, 4500, 4200, 4050, 4200…\n$ gross_mthly_75_percentile <dbl> 5840, 6435, 6000, 5000, 5600, 4900…\n\nPlot\nx - median error - 25, 75 y - degree_code fill = school\n\n\nglimpse(sg_2018)\n\n\nRows: 117\nColumns: 9\n$ year                      <dbl> 2018, 2018, 2018, 2018, 2018, 2018…\n$ university                <chr> \"National University of Singapore\"…\n$ school                    <chr> \"Faculty of Law\", \"Yale-NUS Colleg…\n$ degree                    <chr> \"bachelor of laws\", \"bachelor of s…\n$ degree_recode             <fct> law, science, law, info_sys, law, …\n$ employment_rate_ft_perm   <dbl> 92.6, 78.9, 92.6, 97.2, 90.8, 100.…\n$ gross_monthly_median      <dbl> 5500, 4800, 5513, 4685, 4500, 4575…\n$ gross_mthly_25_percentile <dbl> 4500, 3600, 4500, 4200, 4050, 4200…\n$ gross_mthly_75_percentile <dbl> 5840, 6435, 6000, 5000, 5600, 4900…\n\nsg_2018 %>% \n  arrange(desc(gross_monthly_median)) %>% \n  slice_head(n=20) %>% \n  group_by(degree_recode) %>% \n  summarise(median = median(gross_monthly_median),\n            minp25 = min(gross_mthly_25_percentile),\n            maxp25 = max(gross_mthly_75_percentile)) %>% \n  \n  ggplot(aes(x = fct_reorder(str_to_title(degree_recode), median),\n             y = median)) +\n  geom_point(size = 1) +\n  geom_errorbar(aes(ymin = minp25,\n                    ymax = maxp25)) +\n  coord_flip() +\n  scale_y_continuous(labels = scales::dollar, \n                     n.breaks = 20) +\n  expand_limits(y = 0, x = 0) +\n  labs(x = \"\",\n       y = \"\",\n       title = \"Median Gross Monthly Salary for different Degree Majors\",\n       subtitle = \"Error bars represent minimum 25th and maximum 75th percentile\",\n       caption = \"Source: Data.gov.sg - Graduate Employment Survey\") +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\nlibrary(plotly)\n\np <- sg_2018 %>% \n  ggplot(aes(x = fct_reorder(str_to_title(degree_recode), gross_monthly_median),\n             y = gross_monthly_median ,\n             label = degree_recode,\n             col = university)) +\n  geom_point(aes(text = degree), size = 5) +\n  labs(title = \"Median Gross Monthly Salary for Different Majors for Year 2018 Graduates (with min 25th and max 75th percentile)\",\n       subtitle = \"There are differences among majors and among schools\",\n       x = \"\",\n       y = \"\",\n       caption = \"Source: Data.gov.sg - Graduate Employment Survey\") +\n  coord_flip() +\n  scale_y_continuous(labels = scales::dollar, n.breaks = 10,\n                     limits = c(1000, 6000)) +\n  theme(legend.position = \"none\")\n\nggplotly(p)\n\n\n\n{\"x\":{\"data\":[{\"x\":[4575,4000,4000,3800,3800,3750,3775,3800,3650,3500,3500,3600,3600,3500,3536,3500,3600,3440,3500,3500,3400,3350,3350,3370,3400,3200,3250,3300,3220,3200,3500,3000,3000,3000,3000,3000,3000],\"y\":[8,4,14,9,7,13,11,11,9,8,11,11,11,11,12,7,12,9,11,11,11,12,9,12,9,6,9,8,12,9,6,4,3,6,7,9,7],\"text\":[\"business and computing<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Bizad<br />gross_monthly_median: 4575<br />degree_recode: bizad<br />university: Nanyang Technological University\",\"accountancy and business<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Accountancy<br />gross_monthly_median: 4000<br />degree_recode: accountancy<br />university: Nanyang Technological University\",\"computer science<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Comsci<br />gross_monthly_median: 4000<br />degree_recode: comsci<br />university: Nanyang Technological University\",\"science (with education)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 3800<br />degree_recode: science<br />university: Nanyang Technological University\",\"arts (with education)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 3800<br />degree_recode: arts<br />university: Nanyang Technological University\",\"information engineering & media<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Info_sys<br />gross_monthly_median: 3750<br />degree_recode: info_sys<br />university: Nanyang Technological University\",\"computer engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3775<br />degree_recode: engineering<br />university: Nanyang Technological University\",\"aerospace engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3800<br />degree_recode: engineering<br />university: Nanyang Technological University\",\"mathematical sciences<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 3650<br />degree_recode: science<br />university: Nanyang Technological University\",\"business<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Bizad<br />gross_monthly_median: 3500<br />degree_recode: bizad<br />university: Nanyang Technological University\",\"bioengineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3500<br />degree_recode: engineering<br />university: Nanyang Technological University\",\"chemical & biomolecular engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3600<br />degree_recode: engineering<br />university: Nanyang Technological University\",\"electrical & electronic engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3600<br />degree_recode: engineering<br />university: Nanyang Technological University\",\"mechanical engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3500<br />degree_recode: engineering<br />university: Nanyang Technological University\",\"economics<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Social_science<br />gross_monthly_median: 3536<br />degree_recode: social_science<br />university: Nanyang Technological University\",\"history<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 3500<br />degree_recode: arts<br />university: Nanyang Technological University\",\"mathematics & economics<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Social_science<br />gross_monthly_median: 3600<br />degree_recode: social_science<br />university: Nanyang Technological University\",\"environmental earth systems science<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 3440<br />degree_recode: science<br />university: Nanyang Technological University\",\"civil engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3500<br />degree_recode: engineering<br />university: Nanyang Technological University\",\"materials engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3500<br />degree_recode: engineering<br />university: Nanyang Technological University\",\"environmental engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3400<br />degree_recode: engineering<br />university: Nanyang Technological University\",\"public policy and global affairs<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Social_science<br />gross_monthly_median: 3350<br />degree_recode: social_science<br />university: Nanyang Technological University\",\"maritime studies<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 3350<br />degree_recode: science<br />university: Nanyang Technological University\",\"psychology<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Social_science<br />gross_monthly_median: 3370<br />degree_recode: social_science<br />university: Nanyang Technological University\",\"physics & applied physics<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 3400<br />degree_recode: science<br />university: Nanyang Technological University\",\"linguistics & multilingual studies<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Languages<br />gross_monthly_median: 3200<br />degree_recode: languages<br />university: Nanyang Technological University\",\"biological sciences<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 3250<br />degree_recode: science<br />university: Nanyang Technological University\",\"sport science and management<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Bizad<br />gross_monthly_median: 3300<br />degree_recode: bizad<br />university: Nanyang Technological University\",\"sociology<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Social_science<br />gross_monthly_median: 3220<br />degree_recode: social_science<br />university: Nanyang Technological University\",\"chemistry & biological chemistry<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 3200<br />degree_recode: science<br />university: Nanyang Technological University\",\"chinese<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Languages<br />gross_monthly_median: 3500<br />degree_recode: languages<br />university: Nanyang Technological University\",\"accountancy<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Accountancy<br />gross_monthly_median: 3000<br />degree_recode: accountancy<br />university: Nanyang Technological University\",\"communication studies<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Communications<br />gross_monthly_median: 3000<br />degree_recode: communications<br />university: Nanyang Technological University\",\"english<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Languages<br />gross_monthly_median: 3000<br />degree_recode: languages<br />university: Nanyang Technological University\",\"philosophy<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 3000<br />degree_recode: arts<br />university: Nanyang Technological University\",\"biomedical sciences & chinese medicine<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 3000<br />degree_recode: science<br />university: Nanyang Technological University\",\"art, design & media<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 3000<br />degree_recode: arts<br />university: Nanyang Technological University\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(248,118,109,1)\",\"opacity\":1,\"size\":18.8976377952756,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(248,118,109,1)\"}},\"hoveron\":\"points\",\"name\":\"Nanyang Technological University\",\"legendgroup\":\"Nanyang Technological University\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[5500,4800,4500,4458,4320,4050,3895,3895,3975,4000,3700,3600,3630,3500,3700,3650,3414,3400,3550,3300,3389,3400,3400,3300,3450,3250,3225,3200,3200,3060,3100,3020,1800],\"y\":[15,9,9,14,14,10,11,8,11,11,11,11,7,11,11,10,9,12,10,7,11,11,9,5,10,7,8,4,7,5,9,4,1],\"text\":[\"bachelor of laws<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Law<br />gross_monthly_median: 5500<br />degree_recode: law<br />university: National University of Singapore\",\"bachelor of science with honours<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 4800<br />degree_recode: science<br />university: National University of Singapore\",\"bachelor of science (business analytics)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 4500<br />degree_recode: science<br />university: National University of Singapore\",\"bachelor of computing (information systems)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Comsci<br />gross_monthly_median: 4458<br />degree_recode: comsci<br />university: National University of Singapore\",\"bachelor of computing (computer science)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Comsci<br />gross_monthly_median: 4320<br />degree_recode: comsci<br />university: National University of Singapore\",\"bachelor of dental surgery<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Healthsci<br />gross_monthly_median: 4050<br />degree_recode: healthsci<br />university: National University of Singapore\",\"bachelor of engineering (industrial and systems engineering)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3895<br />degree_recode: engineering<br />university: National University of Singapore\",\"bachelor of business administration (hons)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Bizad<br />gross_monthly_median: 3895<br />degree_recode: bizad<br />university: National University of Singapore\",\"bachelor of engineering (computer engineering)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3975<br />degree_recode: engineering<br />university: National University of Singapore\",\"bachelor of engineering (chemical engineering)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 4000<br />degree_recode: engineering<br />university: National University of Singapore\",\"bachelor of engineering (electrical engineering)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3700<br />degree_recode: engineering<br />university: National University of Singapore\",\"bachelor of engineering (materials science and engineering)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3600<br />degree_recode: engineering<br />university: National University of Singapore\",\"bachelor of arts with honours<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 3630<br />degree_recode: arts<br />university: National University of Singapore\",\"bachelor of engineering (civil engineering)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3500<br />degree_recode: engineering<br />university: National University of Singapore\",\"bachelor of engineering (mechanical engineering)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3700<br />degree_recode: engineering<br />university: National University of Singapore\",\"bachelor of science (pharmacy)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Healthsci<br />gross_monthly_median: 3650<br />degree_recode: healthsci<br />university: National University of Singapore\",\"bachelor of environmental studies<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 3414<br />degree_recode: science<br />university: National University of Singapore\",\"bachelor of social sciences<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Social_science<br />gross_monthly_median: 3400<br />degree_recode: social_science<br />university: National University of Singapore\",\"bachelor of science (nursing) (hons)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Healthsci<br />gross_monthly_median: 3550<br />degree_recode: healthsci<br />university: National University of Singapore\",\"bachelor of arts (hons)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 3300<br />degree_recode: arts<br />university: National University of Singapore\",\"bachelor of engineering (biomedical engineering)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3389<br />degree_recode: engineering<br />university: National University of Singapore\",\"bachelor of engineering (environmental engineering)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3400<br />degree_recode: engineering<br />university: National University of Singapore\",\"bachelor of science (hons)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 3400<br />degree_recode: science<br />university: National University of Singapore\",\"bachelor of science (real estate)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Building_real_estate<br />gross_monthly_median: 3300<br />degree_recode: building_real_estate<br />university: National University of Singapore\",\"bachelor of science (nursing)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Healthsci<br />gross_monthly_median: 3450<br />degree_recode: healthsci<br />university: National University of Singapore\",\"bachelor of arts (industrial design)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 3250<br />degree_recode: arts<br />university: National University of Singapore\",\"bachelor of business administration<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Bizad<br />gross_monthly_median: 3225<br />degree_recode: bizad<br />university: National University of Singapore\",\"bachelor of business administration (accountancy) (hons)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Accountancy<br />gross_monthly_median: 3200<br />degree_recode: accountancy<br />university: National University of Singapore\",\"bachelor of arts<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 3200<br />degree_recode: arts<br />university: National University of Singapore\",\"bachelor of science (project and facilities management)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Building_real_estate<br />gross_monthly_median: 3060<br />degree_recode: building_real_estate<br />university: National University of Singapore\",\"bachelor of science<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 3100<br />degree_recode: science<br />university: National University of Singapore\",\"bachelor of business administration (accountancy)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Accountancy<br />gross_monthly_median: 3020<br />degree_recode: accountancy<br />university: National University of Singapore\",\"bachelor of music<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Music<br />gross_monthly_median: 1800<br />degree_recode: music<br />university: National University of Singapore\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(163,165,0,1)\",\"opacity\":1,\"size\":18.8976377952756,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(163,165,0,1)\"}},\"hoveron\":\"points\",\"name\":\"National University of Singapore\",\"legendgroup\":\"National University of Singapore\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[4016,4045,4000,4000,3600,3500,3600,3729,3550,3460,3500,3500,3300,3343,3400,3350,3300,3380,3250,3253,3200,3300,3510,3075,3200,2925,2950,2800,2900,2825,2775,2663],\"y\":[14,11,11,14,14,11,11,12,11,10,11,10,7,11,11,10,10,11,11,11,11,11,11,4,11,2,7,7,9,8,7,8],\"text\":[\"bachelor of science in computer science in real-time interactive simulation<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Comsci<br />gross_monthly_median: 4016<br />degree_recode: comsci<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in information & communications technology (information security)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 4045<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in information & communications technology (software engineering)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 4000<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of science with honours in computing science<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Comsci<br />gross_monthly_median: 4000<br />degree_recode: comsci<br />university: Singapore Institute of Technology\",\"bachelor of science in computer science and game design<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Comsci<br />gross_monthly_median: 3600<br />degree_recode: comsci<br />university: Singapore Institute of Technology\",\"bachelor of science in electrical engineering & information technology<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3500<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in mechatronics<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3600<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of arts with honours in criminology and security<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Social_science<br />gross_monthly_median: 3729<br />degree_recode: social_science<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in sustainable infrastructure engineering (land)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3550<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor in science (occupational therapy)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Healthsci<br />gross_monthly_median: 3460<br />degree_recode: healthsci<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in mechanical design engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3500<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of science with honours in nursing<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Healthsci<br />gross_monthly_median: 3500<br />degree_recode: healthsci<br />university: Singapore Institute of Technology\",\"bachelor of arts in game design<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 3300<br />degree_recode: arts<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in electrical power engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3343<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in naval architecture<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3400<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor in science (diagnostic radiography)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Healthsci<br />gross_monthly_median: 3350<br />degree_recode: healthsci<br />university: Singapore Institute of Technology\",\"bachelor in science (physiotherapy)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Healthsci<br />gross_monthly_median: 3300<br />degree_recode: healthsci<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in aerospace systems<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3380<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in marine engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3250<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in mechanical design and manufacturing engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3253<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in aeronautical engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3200<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in chemical engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3300<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of engineering with honours in offshore engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3510<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of accountancy with honours<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Accountancy<br />gross_monthly_median: 3075<br />degree_recode: accountancy<br />university: Singapore Institute of Technology\",\"bachelor of science in chemical engineering<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Engineering<br />gross_monthly_median: 3200<br />degree_recode: engineering<br />university: Singapore Institute of Technology\",\"bachelor of science in early childhood education<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Early_childhood<br />gross_monthly_median: 2925<br />degree_recode: early_childhood<br />university: Singapore Institute of Technology\",\"bachelor of arts with honours in communication design<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 2950<br />degree_recode: arts<br />university: Singapore Institute of Technology\",\"bachelor of fine arts in digital art and animation<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 2800<br />degree_recode: arts<br />university: Singapore Institute of Technology\",\"bachelor of science with honours in food and human nutrition<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Science<br />gross_monthly_median: 2900<br />degree_recode: science<br />university: Singapore Institute of Technology\",\"bachelor of hospitality business with honours<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Bizad<br />gross_monthly_median: 2825<br />degree_recode: bizad<br />university: Singapore Institute of Technology\",\"bachelor of arts with honours in interior design<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Arts<br />gross_monthly_median: 2775<br />degree_recode: arts<br />university: Singapore Institute of Technology\",\"bachelor of business administration in food business management<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Bizad<br />gross_monthly_median: 2663<br />degree_recode: bizad<br />university: Singapore Institute of Technology\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,191,125,1)\",\"opacity\":1,\"size\":18.8976377952756,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,191,125,1)\"}},\"hoveron\":\"points\",\"name\":\"Singapore Institute of Technology\",\"legendgroup\":\"Singapore Institute of Technology\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[5513,4685,4500,4000,4000,4000,3700,3600,3700,3550,3210,3000],\"y\":[15,13,15,8,12,13,8,12,12,4,12,4],\"text\":[\"law (cum laude and above)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Law<br />gross_monthly_median: 5513<br />degree_recode: law<br />university: Singapore Management University\",\"information systems (cum laude and above)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Info_sys<br />gross_monthly_median: 4685<br />degree_recode: info_sys<br />university: Singapore Management University\",\"law<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Law<br />gross_monthly_median: 4500<br />degree_recode: law<br />university: Singapore Management University\",\"business management (cum laude and above)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Bizad<br />gross_monthly_median: 4000<br />degree_recode: bizad<br />university: Singapore Management University\",\"economics (cum laude and above)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Social_science<br />gross_monthly_median: 4000<br />degree_recode: social_science<br />university: Singapore Management University\",\"information systems<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Info_sys<br />gross_monthly_median: 4000<br />degree_recode: info_sys<br />university: Singapore Management University\",\"business management<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Bizad<br />gross_monthly_median: 3700<br />degree_recode: bizad<br />university: Singapore Management University\",\"social sciences (cum laude and above)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Social_science<br />gross_monthly_median: 3600<br />degree_recode: social_science<br />university: Singapore Management University\",\"economics<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Social_science<br />gross_monthly_median: 3700<br />degree_recode: social_science<br />university: Singapore Management University\",\"accountancy (cum laude and above)<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Accountancy<br />gross_monthly_median: 3550<br />degree_recode: accountancy<br />university: Singapore Management University\",\"social sciences<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Social_science<br />gross_monthly_median: 3210<br />degree_recode: social_science<br />university: Singapore Management University\",\"accountancy<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Accountancy<br />gross_monthly_median: 3000<br />degree_recode: accountancy<br />university: Singapore Management University\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(0,176,246,1)\",\"opacity\":1,\"size\":18.8976377952756,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(0,176,246,1)\"}},\"hoveron\":\"points\",\"name\":\"Singapore Management University\",\"legendgroup\":\"Singapore Management University\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null},{\"x\":[3400,3000,3000],\"y\":[8,8,4],\"text\":[\"bachelor of science in finance<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Bizad<br />gross_monthly_median: 3400<br />degree_recode: bizad<br />university: Singapore University of Social Sciences\",\"bachelor of science in marketing<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Bizad<br />gross_monthly_median: 3000<br />degree_recode: bizad<br />university: Singapore University of Social Sciences\",\"bachelor of accountancy<br />fct_reorder(str_to_title(degree_recode), gross_monthly_median): Accountancy<br />gross_monthly_median: 3000<br />degree_recode: accountancy<br />university: Singapore University of Social Sciences\"],\"type\":\"scatter\",\"mode\":\"markers\",\"marker\":{\"autocolorscale\":false,\"color\":\"rgba(231,107,243,1)\",\"opacity\":1,\"size\":18.8976377952756,\"symbol\":\"circle\",\"line\":{\"width\":1.88976377952756,\"color\":\"rgba(231,107,243,1)\"}},\"hoveron\":\"points\",\"name\":\"Singapore University of Social Sciences\",\"legendgroup\":\"Singapore University of Social Sciences\",\"showlegend\":true,\"xaxis\":\"x\",\"yaxis\":\"y\",\"hoverinfo\":\"text\",\"frame\":null}],\"layout\":{\"margin\":{\"t\":67.0087173100872,\"r\":7.97011207970112,\"b\":48.6176836861768,\"l\":139.47696139477},\"plot_bgcolor\":\"rgba(255,255,255,1)\",\"paper_bgcolor\":\"rgba(255,255,255,1)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":15.9402241594022},\"title\":{\"text\":\"Median Gross Monthly Salary for Different Majors for Year 2018 Graduates (with min 25th and max 75th percentile)\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":19.1282689912827},\"x\":0,\"xref\":\"paper\"},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[750,6250],\"tickmode\":\"array\",\"ticktext\":[\"$1,000\",\"$2,000\",\"$3,000\",\"$4,000\",\"$5,000\",\"$6,000\"],\"tickvals\":[1000,2000,3000,4000,5000,6000],\"categoryorder\":\"array\",\"categoryarray\":[\"$1,000\",\"$2,000\",\"$3,000\",\"$4,000\",\"$5,000\",\"$6,000\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(77,77,77,1)\",\"ticklen\":3.98505603985056,\"tickwidth\":0.724555643609193,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":12.7521793275218},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"y\",\"title\":{\"text\":\"\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":15.9402241594022}},\"hoverformat\":\".2f\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"type\":\"linear\",\"autorange\":false,\"range\":[0.4,15.6],\"tickmode\":\"array\",\"ticktext\":[\"Music\",\"Early_childhood\",\"Communications\",\"Accountancy\",\"Building_real_estate\",\"Languages\",\"Arts\",\"Bizad\",\"Science\",\"Healthsci\",\"Engineering\",\"Social_science\",\"Info_sys\",\"Comsci\",\"Law\"],\"tickvals\":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\"categoryorder\":\"array\",\"categoryarray\":[\"Music\",\"Early_childhood\",\"Communications\",\"Accountancy\",\"Building_real_estate\",\"Languages\",\"Arts\",\"Bizad\",\"Science\",\"Healthsci\",\"Engineering\",\"Social_science\",\"Info_sys\",\"Comsci\",\"Law\"],\"nticks\":null,\"ticks\":\"outside\",\"tickcolor\":\"rgba(77,77,77,1)\",\"ticklen\":3.98505603985056,\"tickwidth\":0.724555643609193,\"showticklabels\":true,\"tickfont\":{\"color\":\"rgba(77,77,77,1)\",\"family\":\"\",\"size\":12.7521793275218},\"tickangle\":-0,\"showline\":false,\"linecolor\":null,\"linewidth\":0,\"showgrid\":false,\"gridcolor\":null,\"gridwidth\":0,\"zeroline\":false,\"anchor\":\"x\",\"title\":{\"text\":\"\",\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":15.9402241594022}},\"hoverformat\":\".2f\"},\"shapes\":[{\"type\":\"rect\",\"fillcolor\":\"transparent\",\"line\":{\"color\":\"rgba(77,77,77,1)\",\"width\":0.724555643609193,\"linetype\":\"solid\"},\"yref\":\"paper\",\"xref\":\"paper\",\"x0\":0,\"x1\":1,\"y0\":0,\"y1\":1}],\"showlegend\":false,\"legend\":{\"bgcolor\":\"rgba(255,255,255,1)\",\"bordercolor\":\"transparent\",\"borderwidth\":2.06156048675734,\"font\":{\"color\":\"rgba(0,0,0,1)\",\"family\":\"\",\"size\":12.7521793275218}},\"hovermode\":\"closest\",\"barmode\":\"relative\"},\"config\":{\"doubleClick\":\"reset\",\"showSendToCloud\":false},\"source\":\"A\",\"attrs\":{\"d5e31b90e97d\":{\"text\":{},\"x\":{},\"y\":{},\"label\":{},\"colour\":{},\"type\":\"scatter\"}},\"cur_data\":\"d5e31b90e97d\",\"visdat\":{\"d5e31b90e97d\":[\"function (y) \",\"x\"]},\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.2,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}\nHow should a 18 year old, or 20 year old, choose a major? Should it be by salary, or by interest?\nIt would be great if you land a job you are passionate about, and also pays you well… but I think it is more important to find a job in an industry that you have interest in, and slowly work your way to have higher salary, if that is a very deal-breaker/deal-maker for you.\nReferences\nhttps://www.youtube.com/watch?v=nx5yhXAQLxw&list=PL19ev-r1GBwkuyiwnxoHTRC8TTqP8OEi8&index=81\nhttps://data.gov.sg/dataset/graduate-employment-survey-ntu-nus-sit-smu-suss-sutd?resource_id=9326ca53-9153-4a9c-b93f-8ae032637b70\n\n\n\n",
    "preview": "posts/20210601_Tidytuesday College Major/tidy-tuesday-y18w29_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-06-01T21:09:46+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210617 Tidytuesday horror/",
    "title": "Tidy Tuesday Series",
    "description": "2018 Week 30 - Horro Movies",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-06-01",
    "categories": [],
    "contents": "\n\nContents\nLOAD PACKAGES\nIMPORT\nTIDY/TRANSFORM\nWhat are the most common genres over time\n\nREFERENCES\n\nLOAD PACKAGES\n\n\nlibrary(pacman)\np_load(tidyverse, lubridate, ggrepel, ggthemes, janitor, tidytuesdayR, broom, \n       scales,skimr)\n\n\n\nIMPORT\n\n\n# horror_tt <- tt_load(\"2018-10-23\")\n# horror_raw <- horror_tt$movie_profit\n# write_csv(horror_raw, \"horror_raw.csv\")\n\n\n\n\n\nhorror_raw <- read_csv(\"horror_raw.csv\")\n\nglimpse(horror_raw)\n\n\nRows: 3,401\nColumns: 9\n$ X1                <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13,…\n$ release_date      <chr> \"6/22/2007\", \"7/28/1995\", \"5/12/2017\", \"12…\n$ movie             <chr> \"Evan Almighty\", \"Waterworld\", \"King Arthu…\n$ production_budget <dbl> 1.75e+08, 1.75e+08, 1.75e+08, 1.75e+08, 1.…\n$ domestic_gross    <dbl> 100289690, 88246220, 39175066, 38362475, 4…\n$ worldwide_gross   <dbl> 174131329, 264246220, 139950708, 151716815…\n$ distributor       <chr> \"Universal\", \"Universal\", \"Warner Bros.\", …\n$ mpaa_rating       <chr> \"PG\", \"PG-13\", \"PG-13\", \"PG-13\", \"PG-13\", …\n$ genre             <chr> \"Comedy\", \"Action\", \"Adventure\", \"Action\",…\n\nSave as a cleaned version:\nremove first row\nparse date (currently in chr, in month-day-year format)\n\n\ndata <- horror_raw %>% \n  select(-1) %>% \n  filter(worldwide_gross>0)\n\nglimpse(data)\n\n\nRows: 3,365\nColumns: 8\n$ release_date      <chr> \"6/22/2007\", \"7/28/1995\", \"5/12/2017\", \"12…\n$ movie             <chr> \"Evan Almighty\", \"Waterworld\", \"King Arthu…\n$ production_budget <dbl> 1.75e+08, 1.75e+08, 1.75e+08, 1.75e+08, 1.…\n$ domestic_gross    <dbl> 100289690, 88246220, 39175066, 38362475, 4…\n$ worldwide_gross   <dbl> 174131329, 264246220, 139950708, 151716815…\n$ distributor       <chr> \"Universal\", \"Universal\", \"Warner Bros.\", …\n$ mpaa_rating       <chr> \"PG\", \"PG-13\", \"PG-13\", \"PG-13\", \"PG-13\", …\n$ genre             <chr> \"Comedy\", \"Action\", \"Adventure\", \"Action\",…\n\n# check NA values\n\ndata %>% \n  lapply(., function(x) sum(is.na(x)))\n\n\n$release_date\n[1] 0\n\n$movie\n[1] 0\n\n$production_budget\n[1] 0\n\n$domestic_gross\n[1] 0\n\n$worldwide_gross\n[1] 0\n\n$distributor\n[1] 43\n\n$mpaa_rating\n[1] 132\n\n$genre\n[1] 0\n\n# check summary values\ndata %>% \n  select_if(is.numeric) %>% \n  lapply(., function(x) summary(x))\n\n\n$production_budget\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n   250000   9600000  20000000  33512830  45000000 175000000 \n\n$domestic_gross\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n        0   6600000  25885000  45907732  60786269 474544677 \n\n$worldwide_gross\n     Min.   1st Qu.    Median      Mean   3rd Qu.      Max. \n4.230e+02 1.112e+07 4.117e+07 9.512e+07 1.195e+08 1.305e+09 \n\nTIDY/TRANSFORM\nrelease date is as chr, need to change to date.\n\n\ndata_trans <- data %>% \n  mutate(release_date = parse_date(release_date, \"%m/%d/%Y\")) %>% \n  arrange(desc(row_number())) %>% \n  distinct(movie, release_date, .keep_all = T) %>% \n  filter(release_date <= \"2018-01-01\") %>% \n  mutate(distributor_new = fct_lump(distributor, n = 5))  # top 6 distributors)\n\nglimpse(data_trans)\n\n\nRows: 3,310\nColumns: 9\n$ release_date      <date> 2005-07-22, 1998-08-28, 1997-03-28, 2000-…\n$ movie             <chr> \"November\", \"I Married a Strange Person\", …\n$ production_budget <dbl> 250000, 250000, 250000, 250000, 250000, 25…\n$ domestic_gross    <dbl> 191862, 203134, 212285, 1055671, 3395391, …\n$ worldwide_gross   <dbl> 191862, 203134, 743216, 1157672, 3728400, …\n$ distributor       <chr> \"Sony Pictures Classics\", \"Lionsgate\", \"Fo…\n$ mpaa_rating       <chr> \"R\", NA, \"R\", \"R\", \"PG-13\", \"R\", \"R\", \"R\",…\n$ genre             <chr> \"Drama\", \"Comedy\", \"Comedy\", \"Drama\", \"Dra…\n$ distributor_new   <fct> Other, Other, Other, Other, Paramount Pict…\n\n# check that there are no duplicates\ndata_trans %>% \n  count(movie, release_date, sort = T)\n\n\n# A tibble: 3,310 x 3\n   movie                 release_date     n\n   <chr>                 <date>       <int>\n 1 10 Days in a Madhouse 2015-11-11       1\n 2 10,000 B.C.           2008-03-07       1\n 3 102 Dalmatians        2000-11-22       1\n 4 12 Rounds             2009-03-27       1\n 5 12 Years a Slave      2013-10-18       1\n 6 127 Hours             2010-11-05       1\n 7 13 Going On 30        2004-04-23       1\n 8 16 Blocks             2006-03-03       1\n 9 17 Again              2009-04-17       1\n10 2 Fast 2 Furious      2003-06-06       1\n# … with 3,300 more rows\n\n\n\ntheme_set(theme_few())\n\ndata_trans %>% \n  ggplot(aes(production_budget)) +\n  geom_histogram() +\n  scale_x_log10()\n\n\n\n# use log scale for dollars\ndata_trans %>% \n  ggplot(aes(fct_rev(distributor_new), production_budget)) +\n  geom_boxplot() +\n  scale_y_log10(labels = dollar) +\n  coord_flip()\n\n\n\ndata_trans %>% \n  ggplot(aes(fct_rev(distributor_new), worldwide_gross)) +\n  geom_boxplot() +\n  scale_y_log10(labels = dollar) +\n  coord_flip()\n\n\n\n# which genre did the best?\n\ndata_trans %>% \n  count(genre, sort = T) # 5 genre\n\n\n# A tibble: 5 x 2\n  genre         n\n  <chr>     <int>\n1 Drama      1209\n2 Comedy      798\n3 Action      547\n4 Adventure   467\n5 Horror      289\n\ndata_trans %>% \n  ggplot(aes(fct_reorder(genre, production_budget), production_budget)) +\n  geom_boxplot() +\n  scale_y_log10(labels = dollar) +\n  coord_flip()\n\n\n\nglimpse(data_trans)\n\n\nRows: 3,310\nColumns: 9\n$ release_date      <date> 2005-07-22, 1998-08-28, 1997-03-28, 2000-…\n$ movie             <chr> \"November\", \"I Married a Strange Person\", …\n$ production_budget <dbl> 250000, 250000, 250000, 250000, 250000, 25…\n$ domestic_gross    <dbl> 191862, 203134, 212285, 1055671, 3395391, …\n$ worldwide_gross   <dbl> 191862, 203134, 743216, 1157672, 3728400, …\n$ distributor       <chr> \"Sony Pictures Classics\", \"Lionsgate\", \"Fo…\n$ mpaa_rating       <chr> \"R\", NA, \"R\", \"R\", \"PG-13\", \"R\", \"R\", \"R\",…\n$ genre             <chr> \"Drama\", \"Comedy\", \"Comedy\", \"Drama\", \"Dra…\n$ distributor_new   <fct> Other, Other, Other, Other, Paramount Pict…\n\ndata_trans %>% \n  ggplot(aes(fct_reorder(genre, worldwide_gross), worldwide_gross)) +\n  geom_boxplot() +\n  scale_y_log10(labels = dollar) +\n  coord_flip()\n\n\n\ndata_trans %>% \n  filter(!is.na(distributor)) %>% \n  ggplot(aes(fct_reorder(genre, production_budget), production_budget)) +\n  geom_boxplot() +\n  scale_y_log10(labels = dollar) +\n  coord_flip() +\n  facet_wrap ( ~distributor_new)\n\n\n\ndata_trans %>% \n  filter(!is.na(distributor)) %>% \n  ggplot(aes(fct_reorder(genre, worldwide_gross), worldwide_gross)) +\n  geom_boxplot() +\n  scale_y_log10(labels = dollar) +\n  coord_flip() +\n  facet_wrap ( ~distributor_new)\n\n\n\n# what are typical budgets over time\n\ndata_trans %>%\n  mutate(decade = 10*floor(year(release_date)/10)) %>% \n  group_by(decade) %>%\n  arrange(decade) %>% \n  view()\n\ndata_trans %>%\n  mutate(decade = 10*floor(year(release_date)/10)) %>% \n  group_by(decade) %>%\n  summarise(across(c(3,4,5), ~ median(.x, na.rm = TRUE))) %>% \n  pivot_longer(cols = c(-decade),\n               names_to = \"metric\",\n               values_to = \"value\"\n                 ) %>% \n  ggplot(aes(decade, value, col = metric)) +\n  geom_line(size = 2) +\n  scale_y_continuous(labels = dollar) # Gone with the Wind, 1930s\n\n\n\n# top 20 movies\ndata_trans %>% arrange(desc(worldwide_gross)) %>% \n  select(release_date, movie, genre, worldwide_gross) %>% \n  head(20)\n\n\n# A tibble: 20 x 4\n   release_date movie                         genre    worldwide_gross\n   <date>       <chr>                         <chr>              <dbl>\n 1 2015-07-10   Minions                       Adventu…      1162781621\n 2 1993-06-11   Jurassic Park                 Action        1038812584\n 3 2017-06-30   Despicable Me 3               Adventu…      1034520868\n 4 1999-05-19   Star Wars Ep. I: The Phantom… Adventu…      1027044677\n 5 2016-03-04   Zootopia                      Adventu…      1019706594\n 6 1994-06-15   The Lion King                 Adventu…       986332275\n 7 2013-07-03   Despicable Me 2               Adventu…       975216835\n 8 2017-12-20   Jumanji: Welcome to the Jung… Adventu…       961758540\n 9 2004-05-19   Shrek 2                       Adventu…       937008132\n10 2003-05-30   Finding Nemo                  Adventu…       936429370\n11 2005-11-18   Harry Potter and the Goblet … Adventu…       896911078\n12 2016-07-08   The Secret Life of Pets       Adventu…       886767422\n13 2012-07-13   Ice Age: Continental Drift    Adventu…       879765137\n14 2013-11-22   The Hunger Games: Catching F… Adventu…       864868047\n15 2009-07-01   Ice Age: Dawn of the Dinosau… Adventu…       859701857\n16 2002-05-03   Spider-Man                    Adventu…       821706375\n17 2017-06-02   Wonder Woman                  Action         821133378\n18 1996-07-02   Independence Day              Adventu…       817400878\n19 2007-05-17   Shrek the Third               Adventu…       807330936\n20 2016-02-12   Deadpool                      Action         801029249\n\n\n\n# Difference between worldwise gross and production budget = profit\n# Which genres have the biggest payoff?\n\ndata_trans %>% \n  mutate(profit_ratio = worldwide_gross/production_budget) %>% \n  arrange(desc(profit_ratio)) %>%  # mostly horror movies topping the chart\n  head(10)\n\n\n# A tibble: 10 x 10\n   release_date movie  production_budg… domestic_gross worldwide_gross\n   <date>       <chr>             <dbl>          <dbl>           <dbl>\n 1 2009-09-25   Paran…           450000      107918810       194183034\n 2 1999-07-14   The B…           600000      140539099       248300000\n 3 1942-08-13   Bambi            858000      102797000       268000000\n 4 1976-11-21   Rocky           1000000      117235147       225000000\n 5 1978-10-17   Hallo…           325000       47000000        70000000\n 6 1973-08-11   Ameri…           777000      115000000       140000000\n 7 2004-06-11   Napol…           400000       44540956        46122713\n 8 1980-05-09   Frida…           550000       39754601        59754601\n 9 2012-01-06   The D…          1000000       53262945       101759490\n10 1939-12-15   Gone …          3900000      198680470       390525192\n# … with 5 more variables: distributor <chr>, mpaa_rating <chr>,\n#   genre <chr>, distributor_new <fct>, profit_ratio <dbl>\n\ndata_trans %>% \n  mutate(profit_ratio = worldwide_gross/production_budget) %>% \n  arrange(desc(profit_ratio)) %>%\n  ggplot(aes(profit_ratio)) +\n  geom_histogram() +\n  scale_x_log10()\n\n\n\ndata_trans %>% \n  mutate(profit_ratio = worldwide_gross/production_budget) %>% \n  arrange(desc(profit_ratio)) %>%\n  group_by(genre) %>% \n  summarize(median_profit_ratio = median(profit_ratio)) %>%\n  arrange(desc(median_profit_ratio))\n\n\n# A tibble: 5 x 2\n  genre     median_profit_ratio\n  <chr>                   <dbl>\n1 Horror                   2.63\n2 Adventure                2.49\n3 Action                   2.19\n4 Comedy                   1.87\n5 Drama                    1.52\n\ndata_trans %>% \n  mutate(profit_ratio = worldwide_gross/production_budget) %>% \n  arrange(desc(profit_ratio)) %>%\n  group_by(genre) %>% \n  summarize(median_profit_ratio = median(profit_ratio)) %>%\n  arrange(desc(median_profit_ratio)) %>% \n  \n  ggplot(aes(fct_reorder(genre, median_profit_ratio), median_profit_ratio)) +\n  geom_col() +\n  scale_y_log10() +\n  coord_flip()\n\n\n\ndata_trans %>% \n  mutate(profit_ratio = worldwide_gross/production_budget) %>% \n  arrange(desc(profit_ratio)) %>%\n  group_by(genre, year = year(release_date)) %>% \n  summarize(median_profit_ratio = median(profit_ratio),\n            movies = n()) %>%\n  ungroup() %>% \n  filter(year>2000) %>% \n  arrange(desc(median_profit_ratio)) %>% \n  \n  ggplot(aes(year, \n             median_profit_ratio,\n             col = genre)) +\n  geom_line(size = 2) +\n  scale_y_log10() \n\n\n\n# horror makes the most profit\n\n\n\nHorror movies start being more profitable around 2013.\n\n\n# look at genre, year, distributor, and look at the data by decade because sometimes per year there are less than 10 horror movies\n\ndata_trans %>% \n  mutate(profit_ratio = worldwide_gross/production_budget,\n         decade = 10*floor(year(release_date)/10)) %>% \n  group_by(genre, distributor_new, decade) %>% \n  summarize(median_profit_ratio = median(profit_ratio),\n            movies = n()) %>%\n  ungroup() %>% \n  filter( decade >= 1990,\n          !is.na(distributor_new)) %>% \n  arrange(desc(median_profit_ratio)) %>% \n  \n  ggplot(aes(decade, \n             median_profit_ratio,\n             col = genre)) +\n  geom_line(size = 2) +\n  facet_wrap( ~distributor_new) +\n  scale_y_log10() \n\n\n\n\nWhat are the most common genres over time\n\n\ndata_trans_2 <- data_trans %>% \n  mutate(profit_ratio = worldwide_gross/production_budget,\n         decade = 10*floor(year(release_date)/10)) \n\nglimpse(data_trans_2) # with profit ratio and decade\n\n\nRows: 3,310\nColumns: 11\n$ release_date      <date> 2005-07-22, 1998-08-28, 1997-03-28, 2000-…\n$ movie             <chr> \"November\", \"I Married a Strange Person\", …\n$ production_budget <dbl> 250000, 250000, 250000, 250000, 250000, 25…\n$ domestic_gross    <dbl> 191862, 203134, 212285, 1055671, 3395391, …\n$ worldwide_gross   <dbl> 191862, 203134, 743216, 1157672, 3728400, …\n$ distributor       <chr> \"Sony Pictures Classics\", \"Lionsgate\", \"Fo…\n$ mpaa_rating       <chr> \"R\", NA, \"R\", \"R\", \"PG-13\", \"R\", \"R\", \"R\",…\n$ genre             <chr> \"Drama\", \"Comedy\", \"Comedy\", \"Drama\", \"Dra…\n$ distributor_new   <fct> Other, Other, Other, Other, Paramount Pict…\n$ profit_ratio      <dbl> 0.76744800, 0.81253600, 2.97286400, 4.6306…\n$ decade            <dbl> 2000, 1990, 1990, 2000, 2010, 2000, 2010, …\n\ndata_trans_2 %>% \n  count(decade, genre) %>%\n  group_by(decade) %>% \n  mutate(percent = n/sum(n)) %>% \n  ggplot(aes(decade, percent, col = genre)) +\n  geom_line(size = 2) +\n  scale_y_continuous(labels = percent)\n\n\n\n# what distributors make the genre?\n\ndata_trans_2 %>% \n  filter(!is.na(distributor_new)) %>% \n  count(distributor_new, genre) %>% \n  ggplot(aes(genre, n, \n             fill = genre)) +\n  geom_col() +\n  facet_wrap( ~ distributor_new, scales = \"free_x\") +\n  coord_flip()\n\n\n\n# horror movies are the least common genre produced\n\n\n\nWhat were some of the profitable horror movies?\n\n\nglimpse(data_trans_2)\n\n\nRows: 3,310\nColumns: 11\n$ release_date      <date> 2005-07-22, 1998-08-28, 1997-03-28, 2000-…\n$ movie             <chr> \"November\", \"I Married a Strange Person\", …\n$ production_budget <dbl> 250000, 250000, 250000, 250000, 250000, 25…\n$ domestic_gross    <dbl> 191862, 203134, 212285, 1055671, 3395391, …\n$ worldwide_gross   <dbl> 191862, 203134, 743216, 1157672, 3728400, …\n$ distributor       <chr> \"Sony Pictures Classics\", \"Lionsgate\", \"Fo…\n$ mpaa_rating       <chr> \"R\", NA, \"R\", \"R\", \"PG-13\", \"R\", \"R\", \"R\",…\n$ genre             <chr> \"Drama\", \"Comedy\", \"Comedy\", \"Drama\", \"Dra…\n$ distributor_new   <fct> Other, Other, Other, Other, Paramount Pict…\n$ profit_ratio      <dbl> 0.76744800, 0.81253600, 2.97286400, 4.6306…\n$ decade            <dbl> 2000, 1990, 1990, 2000, 2010, 2000, 2010, …\n\ndata_trans_2 %>% \n  filter(genre == \"Horror\") %>% \n  arrange(desc(profit_ratio)) %>% \n  select(release_date, movie, worldwide_gross, distributor) %>% \n  head(10)\n\n\n# A tibble: 10 x 4\n   release_date movie               worldwide_gross distributor       \n   <date>       <chr>                         <dbl> <chr>             \n 1 2009-09-25   Paranormal Activity       194183034 Paramount Pictures\n 2 1999-07-14   The Blair Witch Pr…       248300000 Artisan           \n 3 1978-10-17   Halloween                  70000000 Compass Internati…\n 4 1980-05-09   Friday the 13th            59754601 Paramount Pictures\n 5 2012-01-06   The Devil Inside          101759490 Paramount Pictures\n 6 2004-10-29   Saw                       103880027 Lionsgate         \n 7 2011-04-01   Insidious                  99870886 FilmDistrict      \n 8 2015-04-17   Unfriended                 62869004 Universal         \n 9 2010-10-20   Paranormal Activit…       177512032 Paramount Pictures\n10 2017-01-20   Split                     278306227 Universal         \n\ndata_horror <- data_trans_2 %>% \n  filter(genre == \"Horror\") %>% \n  arrange(desc(profit_ratio)) \n\n\ndata_horror %>% \n  head(10) %>% \n  ggplot(aes(fct_reorder(movie, profit_ratio), \n             profit_ratio,\n             fill = distributor_new)) +\n  geom_col(aes(fill = distributor_new)) +\n  labs(x = \"\",\n       y = \"Ratio of Worldwide Gross/Production Budget\",\n       title = \"Top 10 horror movies that outgrossed their budget\") +\n  coord_flip()\n\n\n\n\nTop 20 movies that outgrossed their budget\n\n\ndata_trans_2 %>% \n  arrange(desc(profit_ratio)) %>% \n  head(20) %>% \n  mutate(movie_year = paste0(movie, \" (\", year(release_date), \")\")) %>% \n    ggplot(aes(fct_reorder(movie_year, profit_ratio), \n             profit_ratio,\n             fill = genre)) +\n  geom_col(aes(fill = genre)) +\n  labs(x = \"\",\n       y = \"Ratio of Worldwide Gross/Production Budget\",\n       title = \"Top 20 movies that outgrossed their budget\") +\n # facet_wrap( ~ distributor_new, scales = \"free\") +\n  coord_flip()\n\n\n\n\nREFERENCES\nhttps://www.youtube.com/watch?v=3-DRwg9yeNA&list=PL19ev-r1GBwkuyiwnxoHTRC8TTqP8OEi8&index=80\n\n\n\n",
    "preview": "posts/20210617 Tidytuesday horror/practice---y18w30-horro-movies_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-19T08:20:49+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210526_Practicing/",
    "title": "Practice questions",
    "description": "Practice questions for R Studio Exams",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-27",
    "categories": [],
    "contents": "\nThe codes below are all taken from websites listed in the Reference Section. This post is to let me practice on the past exam questions.\nLoad required packages\n\n\nlibrary(tidyverse)\n\n\n\nBrendan Cullen’s website\nhttps://tidyverse-exam-v2-solutions.netlify.app/\nBasic Operations\n\n\n# Read in person.csv and store result in a tibble called person\n\nperson <- read_csv(\"https://tidyverse-exam-v2-solutions.netlify.app/person.csv\") \n\nperson\n\n\n# A tibble: 5 x 3\n  person_id personal_name family_name\n  <chr>     <chr>         <chr>      \n1 dyer      William       Dyer       \n2 pb        Frank         Pabodie    \n3 lake      Anderson      Lake       \n4 roe       Valentina     Roerich    \n5 danforth  Frank         Danforth   \n\n# Create a tibble containing only family name and personal names.\n\nperson %>% \n  select(family_name, personal_name)\n\n\n# A tibble: 5 x 2\n  family_name personal_name\n  <chr>       <chr>        \n1 Dyer        William      \n2 Pabodie     Frank        \n3 Lake        Anderson     \n4 Roerich     Valentina    \n5 Danforth    Frank        \n\n# Create a new tibble containing only the rows in which family names come before the letter M. \n\nperson %>% \n  filter(family_name < \"M\")\n\n\n# A tibble: 3 x 3\n  person_id personal_name family_name\n  <chr>     <chr>         <chr>      \n1 dyer      William       Dyer       \n2 lake      Anderson      Lake       \n3 danforth  Frank         Danforth   \n\n# Display all the rows in person sorted by family name length with the longest name first\n\nperson\n\n\n# A tibble: 5 x 3\n  person_id personal_name family_name\n  <chr>     <chr>         <chr>      \n1 dyer      William       Dyer       \n2 pb        Frank         Pabodie    \n3 lake      Anderson      Lake       \n4 roe       Valentina     Roerich    \n5 danforth  Frank         Danforth   \n\nperson %>% \n  arrange(desc(str_length(family_name)))\n\n\n# A tibble: 5 x 3\n  person_id personal_name family_name\n  <chr>     <chr>         <chr>      \n1 danforth  Frank         Danforth   \n2 pb        Frank         Pabodie    \n3 roe       Valentina     Roerich    \n4 dyer      William       Dyer       \n5 lake      Anderson      Lake       \n\nCleaning and counting\n\n\n# Read the file measurement.csv to create a tibble called measurements\n\nmeasurements <- read_csv(\"https://tidyverse-exam-v2-solutions.netlify.app/measurements.csv\")\n\nglimpse(measurements)\n\n\nRows: 21\nColumns: 4\n$ visit_id <dbl> 619, 619, 622, 622, 734, 734, 734, 735, 735, 735, 7…\n$ visitor  <chr> \"dyer\", \"dyer\", \"dyer\", \"dyer\", \"pb\", \"lake\", \"pb\",…\n$ quantity <chr> \"rad\", \"sal\", \"rad\", \"sal\", \"rad\", \"sal\", \"temp\", \"…\n$ reading  <dbl> 9.82, 0.13, 7.80, 0.09, 8.41, 0.05, -21.50, 7.22, 0…\n\n# Create a tibble containing only rows where none of the values are NA and save in a tibble called cleaned\n\ncleaned <- measurements %>% \n  drop_na()\n\ncleaned\n\n\n# A tibble: 18 x 4\n   visit_id visitor quantity reading\n      <dbl> <chr>   <chr>      <dbl>\n 1      619 dyer    rad         9.82\n 2      619 dyer    sal         0.13\n 3      622 dyer    rad         7.8 \n 4      622 dyer    sal         0.09\n 5      734 pb      rad         8.41\n 6      734 lake    sal         0.05\n 7      734 pb      temp      -21.5 \n 8      735 pb      rad         7.22\n 9      751 pb      rad         4.35\n10      751 pb      temp      -18.5 \n11      752 lake    rad         2.19\n12      752 lake    sal         0.09\n13      752 lake    temp      -16   \n14      752 roe     sal        41.6 \n15      837 lake    rad         1.46\n16      837 lake    sal         0.21\n17      837 roe     sal        22.5 \n18      844 roe     rad        11.2 \n\n# Count the number of measurements of each type of quantity in cleaned. Your result should have one row for each quantity \"rad\", \"sal\", \"temp\"\n\ncleaned %>% \n  group_by(quantity) %>% \n  summarise(n = n())\n\n\n# A tibble: 3 x 2\n  quantity     n\n  <chr>    <int>\n1 rad          8\n2 sal          7\n3 temp         3\n\n# Display the minimum and maximum value or reading separately for each quantity in cleaned\n\ncleaned %>% \n  group_by(quantity) %>% \n  summarise(min_reading = min(reading),\n            max_reading = max(reading))\n\n\n# A tibble: 3 x 3\n  quantity min_reading max_reading\n  <chr>          <dbl>       <dbl>\n1 rad             1.46        11.2\n2 sal             0.05        41.6\n3 temp          -21.5        -16  \n\n# Create a tibble in which all salinity readings greater than 1 are divided by 100.\n\ncleaned %>% \n  mutate(reading = case_when(\n    quantity == \"sal\" & reading > 1 ~ reading/100,\n    T ~ reading\n  ))\n\n\n# A tibble: 18 x 4\n   visit_id visitor quantity reading\n      <dbl> <chr>   <chr>      <dbl>\n 1      619 dyer    rad        9.82 \n 2      619 dyer    sal        0.13 \n 3      622 dyer    rad        7.8  \n 4      622 dyer    sal        0.09 \n 5      734 pb      rad        8.41 \n 6      734 lake    sal        0.05 \n 7      734 pb      temp     -21.5  \n 8      735 pb      rad        7.22 \n 9      751 pb      rad        4.35 \n10      751 pb      temp     -18.5  \n11      752 lake    rad        2.19 \n12      752 lake    sal        0.09 \n13      752 lake    temp     -16    \n14      752 roe     sal        0.416\n15      837 lake    rad        1.46 \n16      837 lake    sal        0.21 \n17      837 roe     sal        0.225\n18      844 roe     rad       11.2  \n\nCombining Data\n\n\n# Read visited.csv and drop rows containing any NAs, assigning the results to a new tibble called visited\n\nvisited <- read_csv(\"https://tidyverse-exam-v2-solutions.netlify.app/visited.csv\") %>% \n  drop_na()\n\nvisited\n\n\n# A tibble: 7 x 3\n  visit_id site_id visit_date\n     <dbl> <chr>   <date>    \n1      619 DR-1    1927-02-08\n2      622 DR-1    1927-02-10\n3      734 DR-3    1930-01-07\n4      735 DR-3    1930-01-12\n5      751 DR-3    1930-02-26\n6      837 MSK-4   1932-01-14\n7      844 DR-1    1932-03-22\n\n# Use an innerjoin to combine visited with cleaned using visit_id for match\ncleaned\n\n\n# A tibble: 18 x 4\n   visit_id visitor quantity reading\n      <dbl> <chr>   <chr>      <dbl>\n 1      619 dyer    rad         9.82\n 2      619 dyer    sal         0.13\n 3      622 dyer    rad         7.8 \n 4      622 dyer    sal         0.09\n 5      734 pb      rad         8.41\n 6      734 lake    sal         0.05\n 7      734 pb      temp      -21.5 \n 8      735 pb      rad         7.22\n 9      751 pb      rad         4.35\n10      751 pb      temp      -18.5 \n11      752 lake    rad         2.19\n12      752 lake    sal         0.09\n13      752 lake    temp      -16   \n14      752 roe     sal        41.6 \n15      837 lake    rad         1.46\n16      837 lake    sal         0.21\n17      837 roe     sal        22.5 \n18      844 roe     rad        11.2 \n\ncombined <- visited %>% \n  inner_join(cleaned, by = \"visit_id\")\n\ncombined\n\n\n# A tibble: 14 x 6\n   visit_id site_id visit_date visitor quantity reading\n      <dbl> <chr>   <date>     <chr>   <chr>      <dbl>\n 1      619 DR-1    1927-02-08 dyer    rad         9.82\n 2      619 DR-1    1927-02-08 dyer    sal         0.13\n 3      622 DR-1    1927-02-10 dyer    rad         7.8 \n 4      622 DR-1    1927-02-10 dyer    sal         0.09\n 5      734 DR-3    1930-01-07 pb      rad         8.41\n 6      734 DR-3    1930-01-07 lake    sal         0.05\n 7      734 DR-3    1930-01-07 pb      temp      -21.5 \n 8      735 DR-3    1930-01-12 pb      rad         7.22\n 9      751 DR-3    1930-02-26 pb      rad         4.35\n10      751 DR-3    1930-02-26 pb      temp      -18.5 \n11      837 MSK-4   1932-01-14 lake    rad         1.46\n12      837 MSK-4   1932-01-14 lake    sal         0.21\n13      837 MSK-4   1932-01-14 roe     sal        22.5 \n14      844 DR-1    1932-03-22 roe     rad        11.2 \n\n# find the highest rad reading at each site. \n\nmax_rad <- combined %>% \n  filter(quantity == \"rad\") %>% \n  group_by(site_id) %>% \n  summarise(max_rad = max(reading))\n\n# Find the date of the highest radiation reading at each site\n\ncombined %>% \n  filter(quantity == \"rad\") %>% \n  group_by(site_id, visit_date) %>% \n  summarize(max_rad = max(reading)) %>% \n  semi_join(max_rad) %>%  # returns all rows from x with a match in y\n  select(visit_date, everything())\n\n\n# A tibble: 3 x 3\n# Groups:   site_id [3]\n  visit_date site_id max_rad\n  <date>     <chr>     <dbl>\n1 1932-03-22 DR-1      11.2 \n2 1930-01-07 DR-3       8.41\n3 1932-01-14 MSK-4      1.46\n\nFunctional Programming\n\n\n# Write a function called summarize_table that takes a title string and a tibble as input and returns a string that says something like \"title has n rows and n columns\n\nsummarize_table <- function(title, df) {\n  nrow <- nrow(df)\n  ncol <- ncol(df)\n  \n  glue::glue(\"{title} has {nrow} rows and {ncol} columns.\" )\n}\n\nsummarize_table(\"mtcars\", mtcars)\n\n\nmtcars has 32 rows and 11 columns.\n\n# Write another function called show_columns that takes a string and a tibble as input and returns a string that says something like, “table has columns name, name, name”. For example, show_columns('person', person) should return the string \"person has columns person_id, personal_name, family_name\".\n\nshow_columns <- function(title, df) { \n  col_names <- names(df) %>% \n    str_c(collapse = \", \")\n\nglue::glue(\"{title} has columns {col_names}\")\n}\n\nshow_columns('person', person)\n\n\nperson has columns person_id, personal_name, family_name\n\n# The function long_name checks whether a string is longer than 4 characters. Use this function and a function from purrr to create a logical vector that contains the value TRUE where family names in the tibble person are longer than 4 characters, and FALSE where they are 4 characters or less.\n\n\nlong_name <- function(name) {\n      stringr::str_length(name) > 4\n}\n\nperson %>% \n  mutate(long_family_name = map_lgl(family_name, long_name))\n\n\n# A tibble: 5 x 4\n  person_id personal_name family_name long_family_name\n  <chr>     <chr>         <chr>       <lgl>           \n1 dyer      William       Dyer        FALSE           \n2 pb        Frank         Pabodie     TRUE            \n3 lake      Anderson      Lake        FALSE           \n4 roe       Valentina     Roerich     TRUE            \n5 danforth  Frank         Danforth    TRUE            \n\nMarly Gotti’s website\nhttps://marlycormar.github.io/tidyverse_sample_exam/sample_exam_sols/sols.html\nQ1\nThe file at_health_facilities.csv contains a tidy dataset with four columns:\nThe ISO3 code of the country that reported data. The year for which data was reported. The percentage of HIV-positive children born to HIV-positive mothers age 15–17. The percentage of HIV-positive children born to HIV-positive mothers age 20–34. Please answer the following questions:\nHow many countries reported data? What is the difference between the minimum and maximum year with valid data for each country? How many countries reported data in 3 or more years? Which countries reported 100% incidence for at least one year in either age group?\n\n\nhiv <- read_csv(\"https://education.rstudio.com/blog/2020/02/instructor-certification-exams/at_health_facilities.csv\") %>% \n  janitor::clean_names()\n\nglimpse(hiv)\n\n\nRows: 225\nColumns: 4\n$ iso3      <chr> \"AFG\", \"ALB\", \"ALB\", \"ARG\", \"ARM\", \"ARM\", \"ARM\", \"…\n$ year      <dbl> 2010, 2005, 2008, 2012, 2000, 2005, 2010, 2006, 20…\n$ age_15_17 <dbl> 33, 98, 98, 100, 93, 99, 100, 81, 12, 16, 19, 31, …\n$ age_20_34 <chr> \"29\", \"96\", \"98\", \"100\", \"87\", \"100\", \"100\", \"87\",…\n\n# how many countries reported data?\n\nhiv %>% \n  select(iso3) %>% \n  unique() %>% # 100 countries\n  nrow()\n\n\n[1] 100\n\n# what is the difference between min and max year with valid data for each country?\n\n# age 20-34 has some dashes to be filtered out\n\nhiv %>% \n  filter(age_20_34 != \"-\") %>% \n  group_by(iso3) %>% \n  summarise(diff = max(year) - min(year)) \n\n\n# A tibble: 100 x 2\n   iso3   diff\n   <chr> <dbl>\n 1 AFG       0\n 2 ALB       3\n 3 ARG       0\n 4 ARM      10\n 5 AZE       0\n 6 BDI       5\n 7 BEN      10\n 8 BFA       7\n 9 BGD       8\n10 BIH       5\n# … with 90 more rows\n\n# how many countries reported data in 3 or more years?\n\nhiv %>% \n  group_by(iso3) %>% \n  summarise(years_count = n_distinct(year)) %>% \n  filter(years_count >=3) # 34 countries\n\n\n# A tibble: 34 x 2\n   iso3  years_count\n   <chr>       <int>\n 1 ARM             3\n 2 BEN             3\n 3 BFA             3\n 4 BGD             6\n 5 CMR             3\n 6 COD             3\n 7 COL             3\n 8 DOM             3\n 9 EGY             5\n10 ETH             3\n# … with 24 more rows\n\n# which countries reported 100% incidence rate for at least one year in either age group?\n\nhiv %>% \n  filter(age_15_17 == 100 | age_20_34 == 100) %>% \n  distinct(iso3) # 18 countries\n\n\n# A tibble: 18 x 1\n   iso3 \n   <chr>\n 1 ARG  \n 2 ARM  \n 3 BRB  \n 4 BLR  \n 5 BIH  \n 6 CUB  \n 7 DOM  \n 8 JAM  \n 9 KAZ  \n10 KGZ  \n11 MKD  \n12 MDA  \n13 MNE  \n14 LCA  \n15 SRB  \n16 THA  \n17 UKR  \n18 URY  \n\nQ2\nA student has sent you the file rmd-country-profile.Rmd, which is an R Markdown document analyzing the data in at_health_facilities.csv for Bangladesh. They could not knit the file, and are providing you with the raw .Rmd file instead of a rendered file.\nGo through the file, fixing things that are preventing it from knitting cleanly. Change the two lines of bold text to H2-level headers to organize the document, and add a table of contents. Convert this R Markdown report for Bangladesh into a parameterized report with the country’s iso3 code as its parameter. Knit a new country profile for Egypt (ISO3 code “EGY”).\nQ3\n\n\ninfant_hiv <- read_csv(\"https://education.rstudio.com/blog/2020/02/instructor-certification-exams/infant_hiv.csv\")\n\n# tidy the data into 3 col: iso3, year, state, number\n\ntidy <- infant_hiv %>% \n  pivot_longer(!ISO3,\n               names_to = c(\"year\", \"state\"),\n               names_pattern = \"(.*) (.*)\") %>% \n  mutate(value = case_when(\n    value == \"-\" | value == \">95%\" ~NA_character_,\n    TRUE ~ str_replace(value, pattern = \"%\", replacement = \"\")\n  ))\n\ntidy\n\n\n# A tibble: 5,184 x 4\n   ISO3  year  state value\n   <chr> <chr> <chr> <chr>\n 1 AFG   2009  est   <NA> \n 2 AFG   2009  hi    <NA> \n 3 AFG   2009  lo    <NA> \n 4 AFG   2010  est   <NA> \n 5 AFG   2010  hi    <NA> \n 6 AFG   2010  lo    <NA> \n 7 AFG   2011  est   <NA> \n 8 AFG   2011  hi    <NA> \n 9 AFG   2011  lo    <NA> \n10 AFG   2012  est   <NA> \n# … with 5,174 more rows\n\n# write the function\n\ntidy_data <- function(file) {\n  \n  # Import data\n  raw_data <- read_csv(file)\n  \n  # Tidy data\n  tidy <- raw_data %>% \n  pivot_longer(!ISO3,\n               names_to = c(\"year\", \"state\"),\n               names_pattern = \"(.*) (.*)\") %>% \n  mutate(value = case_when(\n    value == \"-\" | value == \">95%\" ~NA_character_,\n    TRUE ~ str_replace(value, pattern = \"%\", replacement = \"\")\n  ))\n\ntidy\n}\n\ntidy_data(\"https://education.rstudio.com/blog/2020/02/instructor-certification-exams/infant_hiv.csv\")\n\n\n# A tibble: 5,184 x 4\n   ISO3  year  state value\n   <chr> <chr> <chr> <chr>\n 1 AFG   2009  est   <NA> \n 2 AFG   2009  hi    <NA> \n 3 AFG   2009  lo    <NA> \n 4 AFG   2010  est   <NA> \n 5 AFG   2010  hi    <NA> \n 6 AFG   2010  lo    <NA> \n 7 AFG   2011  est   <NA> \n 8 AFG   2011  hi    <NA> \n 9 AFG   2011  lo    <NA> \n10 AFG   2012  est   <NA> \n# … with 5,174 more rows\n\nQ4\n\n\nranking <- read_csv(\"https://education.rstudio.com/blog/2020/02/instructor-certification-exams/ranking.csv\")\n\nranking %>% \n  group_by(item) %>% \n  count(rank) %>% \n  pivot_wider(names_from = rank,\n              values_from = n) %>% \n  mutate(num = sum(positive, negative, indifferent, na.rm = T)) %>% \n  mutate_at(vars(-item, -num), .funs = list(~ round(./num, digits = 2))) %>% \n  ggplot(aes(negative, positive, size = num)) +\n  geom_point(aes(alpha = 0.25)) +\n  geom_smooth(method = \"lm\")\n\n\n\n\nReferences\nhttps://tidyverse-exam-v2-solutions.netlify.app/\n\n\n\n",
    "preview": "posts/20210526_Practicing/Practicing-for-Exam_files/figure-html5/unnamed-chunk-8-1.png",
    "last_modified": "2021-05-27T20:47:00+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210526_Tidyverse Chap 17 - Iteration with purrr/",
    "title": "Iteration with purrr",
    "description": "R4DS 17 - purrr",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-27",
    "categories": [],
    "contents": "\nR4DS Practice 17: Iteration with purrr\nThe codes below are from the practice exercises in https://r4ds.had.co.nz/, and are taken with reference from: https://jrnold.github.io/r4ds-exercise-solutions/\nLet’s begin now\nLoading tidyverse package.\n\n\nlibrary(tidyverse)\n\n\n\nFor Loops\n\n\ndf <- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\n# to compute the median of each column:\n\nmedian(df$a)\n\n\n[1] 0.2939953\n\nmedian(df$b)\n\n\n[1] -0.6834013\n\n# use a for loop:\n\noutput <- vector(\"double\", ncol(df))\n\n  for(i in seq_along(df)) {  # df[[1]] , df[[2]], df[[3]], df[[4]]\n  \n  output[[i]] <-  median(df[[i]])\n  \n}\n\noutput\n\n\n[1]  0.2939953 -0.6834013 -0.3643358  0.3220519\n\nWrite for loops to compute the mean of every column in mtcars\n\n\n# to compute the mean for every column in mtcars:\n\nglimpse(mtcars)\n\n\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 1…\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4…\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7,…\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180…\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3…\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190,…\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,…\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1…\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1…\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4…\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2…\n\nmtcars[[1]] # first column\n\n\n [1] 21.0 21.0 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 17.8 16.4 17.3\n[14] 15.2 10.4 10.4 14.7 32.4 30.4 33.9 21.5 15.5 15.2 13.3 19.2 27.3\n[27] 26.0 30.4 15.8 19.7 15.0 21.4\n\nmtcars[[2]] # second column\n\n\n [1] 6 6 4 6 8 6 8 4 4 6 6 8 8 8 8 8 8 4 4 4 4 8 8 8 8 4 4 4 8 6 8 4\n\nmean(mtcars[[1]]) # mean  = 20.09062\n\n\n[1] 20.09062\n\n# for loop\n\noutput <-  vector(\"double\", ncol(mtcars))\n\nfor (i in seq_along(mtcars))\n  output[[i]] <-  mean(mtcars[[i]])\n\noutput\n\n\n [1]  20.090625   6.187500 230.721875 146.687500   3.596563   3.217250\n [7]  17.848750   0.437500   0.406250   3.687500   2.812500\n\noutput <- vector(\"double\", ncol(mtcars))\nnames(output) <- names(mtcars)\nfor (i in names(mtcars)) {\n  output[i] <- mean(mtcars[[i]])\n}\noutput\n\n\n       mpg        cyl       disp         hp       drat         wt \n 20.090625   6.187500 230.721875 146.687500   3.596563   3.217250 \n      qsec         vs         am       gear       carb \n 17.848750   0.437500   0.406250   3.687500   2.812500 \n\nWrite for loops to determine the type of each column in nycflights13::flights\n\n\nlibrary(nycflights13)\nglimpse(flights)\n\n\nRows: 336,776\nColumns: 19\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 201…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, …\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, …\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838,…\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846,…\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2,…\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV…\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, …\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EW…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FL…\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 1…\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, …\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, …\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 20…\n\n# to find out the class of column:\nclass(flights$year)\n\n\n[1] \"integer\"\n\nclass(flights[[1]]) # integer\n\n\n[1] \"integer\"\n\noutput <- vector(\"list\", ncol(flights)) # output is a list\n\nnames(output) <- names(flights) # set name of output\n\nfor (i in names(flights)) {\n  output[[i]] <- class(flights[[i]])\n}\noutput\n\n\n$year\n[1] \"integer\"\n\n$month\n[1] \"integer\"\n\n$day\n[1] \"integer\"\n\n$dep_time\n[1] \"integer\"\n\n$sched_dep_time\n[1] \"integer\"\n\n$dep_delay\n[1] \"numeric\"\n\n$arr_time\n[1] \"integer\"\n\n$sched_arr_time\n[1] \"integer\"\n\n$arr_delay\n[1] \"numeric\"\n\n$carrier\n[1] \"character\"\n\n$flight\n[1] \"integer\"\n\n$tailnum\n[1] \"character\"\n\n$origin\n[1] \"character\"\n\n$dest\n[1] \"character\"\n\n$air_time\n[1] \"numeric\"\n\n$distance\n[1] \"numeric\"\n\n$hour\n[1] \"numeric\"\n\n$minute\n[1] \"numeric\"\n\n$time_hour\n[1] \"POSIXct\" \"POSIXt\" \n\nCompute the number of unique values in each column of iris\n\n\nglimpse(iris)\n\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa,…\n\niris[[1]] # first col of iris: sepal.length\n\n\n  [1] 5.1 4.9 4.7 4.6 5.0 5.4 4.6 5.0 4.4 4.9 5.4 4.8 4.8 4.3 5.8 5.7\n [17] 5.4 5.1 5.7 5.1 5.4 5.1 4.6 5.1 4.8 5.0 5.0 5.2 5.2 4.7 4.8 5.4\n [33] 5.2 5.5 4.9 5.0 5.5 4.9 4.4 5.1 5.0 4.5 4.4 5.0 5.1 4.8 5.1 4.6\n [49] 5.3 5.0 7.0 6.4 6.9 5.5 6.5 5.7 6.3 4.9 6.6 5.2 5.0 5.9 6.0 6.1\n [65] 5.6 6.7 5.6 5.8 6.2 5.6 5.9 6.1 6.3 6.1 6.4 6.6 6.8 6.7 6.0 5.7\n [81] 5.5 5.5 5.8 6.0 5.4 6.0 6.7 6.3 5.6 5.5 5.5 6.1 5.8 5.0 5.6 5.7\n [97] 5.7 6.2 5.1 5.7 6.3 5.8 7.1 6.3 6.5 7.6 4.9 7.3 6.7 7.2 6.5 6.4\n[113] 6.8 5.7 5.8 6.4 6.5 7.7 7.7 6.0 6.9 5.6 7.7 6.3 6.7 7.2 6.2 6.1\n[129] 6.4 7.2 7.4 7.9 6.4 6.3 6.1 7.7 6.3 6.4 6.0 6.9 6.7 6.9 5.8 6.8\n[145] 6.7 6.7 6.3 6.5 6.2 5.9\n\nn_distinct(iris[[1]]) # number of unique values\n\n\n[1] 35\n\niris_unique <- vector(\"double\", ncol(iris)) # vector produces a vector of the given length and mode\nnames(iris_unique) <-  names(iris) # set the name of vector according to the iris dataset\n\nfor(i in names(iris)) {\n  iris_unique[i] <- n_distinct(iris[[i]])\n}\n\niris_unique\n\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n          35           23           43           22            3 \n\nGenerate 10 random normals for each of miu = -10, 0, 10 and 100\n\n\n# generate 10 numbers\nn <- 10\n\n# pre-set values for mean\n\nmu <- c(-10, 0, 10, 100)\n\n\n# write the forloop\n\nnormals <- vector(\"list\", length(mu))\nfor (i in seq_along(normals)) {\n  \n  normals[[i]] <- rnorm(n, mean = mu[[i]])\n}\n\n# this example requires me to create the list myself\n\n\n\nFor Loop Variations - Modifying an Existing Object\n\n\ndf\n\n\n# A tibble: 10 x 4\n        a      b       c      d\n    <dbl>  <dbl>   <dbl>  <dbl>\n 1 -0.148 -0.623 -0.767   0.707\n 2  1.10  -1.62   0.569   1.27 \n 3  2.14  -1.47  -1.03    0.137\n 4  0.256  0.193  0.389  -2.11 \n 5  1.59  -0.792 -2.03    0.542\n 6 -1.41   1.00  -0.628   0.385\n 7  0.331 -0.751  0.0282  1.41 \n 8 -1.30  -0.744 -1.86    0.259\n 9  0.422  0.683 -0.101  -0.325\n10  0.257 -0.359  0.692  -0.741\n\n# rescale function\n\nrescale01 <- function(x){\n  range <- range(x, na.rm = T)\n  (x-range[1] / (range[2] - range[1]))\n  \n}\n\nrescale01(df$a)\n\n\n [1]  0.2491601  1.4979938  2.5344229  0.6530507  1.9901675 -1.0088461\n [7]  0.7271936 -0.9065514  0.8184003  0.6541429\n\nrescale01(df$b)\n\n\n [1] -0.005368567 -0.997429229 -0.855877454  0.810609489 -0.173857050\n [6]  1.617268846 -0.133204444 -0.125996565  1.300225157  0.258333590\n\n# to solve this with a for loop,\n\n# output: same as input\n# sequence: seq_along(df)\n# body: apply rescale01\n\nseq_along(df) # generate sequences\n\n\n[1] 1 2 3 4\n\nfor(i in seq_along(df)) {\n  df[[i]] <- rescale01(df[[i]]) # use double square brackets\n}\n\ndf\n\n\n# A tibble: 10 x 4\n        a        b       c      d\n    <dbl>    <dbl>   <dbl>  <dbl>\n 1  0.249 -0.00537 -0.0211  1.31 \n 2  1.50  -0.997    1.31    1.87 \n 3  2.53  -0.856   -0.282   0.737\n 4  0.653  0.811    1.14   -1.51 \n 5  1.99  -0.174   -1.29    1.14 \n 6 -1.01   1.62     0.118   0.984\n 7  0.727 -0.133    0.774   2.01 \n 8 -0.907 -0.126   -1.11    0.859\n 9  0.818  1.30     0.645   0.275\n10  0.654  0.258    1.44   -0.141\n\nFor Loops vs Functionals\nR is a functional programming language, meaning it is possible to wrap up for loops in a function, and call that function instead of using for loop directly.\nSolutions that make use of loops are less efficient than vectorized solutions that make use of apply functions, such as lapply and sapply.\nThere is also more clarity with using map() in purrr, rather than using for loops.\n\n\nx <- list(a = 1:10,\n          beta = exp(-3:3),\n          logic = c(T, F, F, T))\nx\n\n\n$a\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$beta\n[1]  0.04978707  0.13533528  0.36787944  1.00000000  2.71828183\n[6]  7.38905610 20.08553692\n\n$logic\n[1]  TRUE FALSE FALSE  TRUE\n\n# lapply returns a list of the same length as x\nlapply(x, mean)  # variable, function\n\n\n$a\n[1] 5.5\n\n$beta\n[1] 4.535125\n\n$logic\n[1] 0.5\n\n# sapply returns a vector by default\nsapply(x, quantile)\n\n\n         a        beta logic\n0%    1.00  0.04978707   0.0\n25%   3.25  0.25160736   0.0\n50%   5.50  1.00000000   0.5\n75%   7.75  5.05366896   1.0\n100% 10.00 20.08553692   1.0\n\n# use sapply to find missing values\nsapply(x, function(x) sum(is.na(x))) # creating an anonymous function\n\n\n    a  beta logic \n    0     0     0 \n\nMap Functions\nThe focus is on the operation being performed, not the steps/codes to loop over which element and store as which output. The first argument is always the data object you want to map over, and the second argument is the function that you want to apply.\n\n\ndf\n\n\n# A tibble: 10 x 4\n        a        b       c      d\n    <dbl>    <dbl>   <dbl>  <dbl>\n 1  0.249 -0.00537 -0.0211  1.31 \n 2  1.50  -0.997    1.31    1.87 \n 3  2.53  -0.856   -0.282   0.737\n 4  0.653  0.811    1.14   -1.51 \n 5  1.99  -0.174   -1.29    1.14 \n 6 -1.01   1.62     0.118   0.984\n 7  0.727 -0.133    0.774   2.01 \n 8 -0.907 -0.126   -1.11    0.859\n 9  0.818  1.30     0.645   0.275\n10  0.654  0.258    1.44   -0.141\n\n# calculate mean for each column in df\nmap_dbl(df, mean) # df, function\n\n\n        a         b         c         d \n0.7209134 0.1694704 0.2723914 0.7529901 \n\nmap(df, mean) # returns a list\n\n\n$a\n[1] 0.7209134\n\n$b\n[1] 0.1694704\n\n$c\n[1] 0.2723914\n\n$d\n[1] 0.7529901\n\n# calculate median for each column in df\nmap_dbl(df, median)\n\n\n          a           b           c           d \n 0.69066825 -0.06568257  0.38151973  0.92170995 \n\ndf %>% \n  map_dbl(mean) # calculate mean for each col in df\n\n\n        a         b         c         d \n0.7209134 0.1694704 0.2723914 0.7529901 \n\nTo apply a linear model to each group in a dataset:\n\n\nmodels <- mtcars %>% \n  split(.$cyl) %>% # divides the data in the vector into groups\n  map( ~lm(mpg ~wt, data = .))\n\nmodels  \n\n\n$`4`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n     39.571       -5.647  \n\n\n$`6`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n      28.41        -2.78  \n\n\n$`8`\n\nCall:\nlm(formula = mpg ~ wt, data = .)\n\nCoefficients:\n(Intercept)           wt  \n     23.868       -2.192  \n\n# to extract model summary:\n\nmodels %>% \n  map(summary) %>% \n  map_dbl(\"r.squared\") # provide the name of the element to extract\n\n\n        4         6         8 \n0.5086326 0.4645102 0.4229655 \n\nCompute the mean of every column in mtcars\n\n\nglimpse(mtcars)\n\n\nRows: 32\nColumns: 11\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 1…\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4, 4…\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146.7,…\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, 180…\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92, 3…\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.190,…\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20.00,…\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1…\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1…\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4, 4…\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1, 2…\n\nmtcars %>% \n  map_dbl(mean)\n\n\n       mpg        cyl       disp         hp       drat         wt \n 20.090625   6.187500 230.721875 146.687500   3.596563   3.217250 \n      qsec         vs         am       gear       carb \n 17.848750   0.437500   0.406250   3.687500   2.812500 \n\nDetermine the type of column in flights dataset\n\n\nglimpse(flights)\n\n\nRows: 336,776\nColumns: 19\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 201…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, …\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, …\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838,…\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846,…\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2,…\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV…\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, …\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EW…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FL…\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 1…\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, …\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, …\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 20…\n\n# find a function to determine type of column\n\ntypeof(flights$year)\n\n\n[1] \"integer\"\n\nflights %>% \n  map_chr(typeof)\n\n\n          year          month            day       dep_time \n     \"integer\"      \"integer\"      \"integer\"      \"integer\" \nsched_dep_time      dep_delay       arr_time sched_arr_time \n     \"integer\"       \"double\"      \"integer\"      \"integer\" \n     arr_delay        carrier         flight        tailnum \n      \"double\"    \"character\"      \"integer\"    \"character\" \n        origin           dest       air_time       distance \n   \"character\"    \"character\"       \"double\"       \"double\" \n          hour         minute      time_hour \n      \"double\"       \"double\"       \"double\" \n\nCompute the number of unique values in each column of iris\n\n\nglimpse(iris)\n\n\nRows: 150\nColumns: 5\n$ Sepal.Length <dbl> 5.1, 4.9, 4.7, 4.6, 5.0, 5.4, 4.6, 5.0, 4.4, 4.…\n$ Sepal.Width  <dbl> 3.5, 3.0, 3.2, 3.1, 3.6, 3.9, 3.4, 3.4, 2.9, 3.…\n$ Petal.Length <dbl> 1.4, 1.4, 1.3, 1.5, 1.4, 1.7, 1.4, 1.5, 1.4, 1.…\n$ Petal.Width  <dbl> 0.2, 0.2, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.…\n$ Species      <fct> setosa, setosa, setosa, setosa, setosa, setosa,…\n\niris %>% \n  map_int(n_distinct)\n\n\nSepal.Length  Sepal.Width Petal.Length  Petal.Width      Species \n          35           23           43           22            3 \n\nGenerate 10 random normals for each of miu = -10, 0, 10 and 100.\n\n\nmap(c(-10, 0, 10, 100), ~rnorm(n = 10, mean = .))\n\n\n[[1]]\n [1]  -8.501952  -8.017926  -9.954449 -10.527896 -11.367753  -8.518719\n [7]  -8.973262 -10.295413  -8.666817 -10.657593\n\n[[2]]\n [1]  1.0622306  0.1728670 -1.8079931  0.5542649  1.2390780  0.2565406\n [7]  0.3427404  1.8666900 -0.6743676 -1.1976806\n\n[[3]]\n [1]  8.951252  9.671973  9.780482  9.520685  9.916101 10.088140\n [7]  9.475330 11.045631 10.755672  8.077710\n\n[[4]]\n [1]  98.83447 100.34102 100.06690 100.40858 100.76098 101.69539\n [7]  99.91942 100.75977 100.04051 100.52748\n\nHow can you create a single vector that for each column in a data frame indicates whether or not it is a factor?\n\n\n# function:\ndiamonds \n\n\n# A tibble: 53,940 x 10\n   carat cut       color clarity depth table price     x     y     z\n   <dbl> <ord>     <ord> <ord>   <dbl> <dbl> <int> <dbl> <dbl> <dbl>\n 1  0.23 Ideal     E     SI2      61.5    55   326  3.95  3.98  2.43\n 2  0.21 Premium   E     SI1      59.8    61   326  3.89  3.84  2.31\n 3  0.23 Good      E     VS1      56.9    65   327  4.05  4.07  2.31\n 4  0.29 Premium   I     VS2      62.4    58   334  4.2   4.23  2.63\n 5  0.31 Good      J     SI2      63.3    58   335  4.34  4.35  2.75\n 6  0.24 Very Good J     VVS2     62.8    57   336  3.94  3.96  2.48\n 7  0.24 Very Good I     VVS1     62.3    57   336  3.95  3.98  2.47\n 8  0.26 Very Good H     SI1      61.9    55   337  4.07  4.11  2.53\n 9  0.22 Fair      E     VS2      65.1    61   337  3.87  3.78  2.49\n10  0.23 Very Good H     VS1      59.4    61   338  4     4.05  2.39\n# … with 53,930 more rows\n\nis.factor(diamonds$color)\n\n\n[1] TRUE\n\n# to check if all the columns are factors:\n\ndiamonds %>% \n  map_lgl(is.factor)\n\n\n  carat     cut   color clarity   depth   table   price       x \n  FALSE    TRUE    TRUE    TRUE   FALSE   FALSE   FALSE   FALSE \n      y       z \n  FALSE   FALSE \n\nMapping over Multiple Arguments\nuse map2() or pmap()\nReplicating worked example on Rebecca Barter’s website\nLet me try to practice more using worked examples from http://www.rebeccabarter.com/blog/2019-08-19_purrr/.\n\n\n# Create a function to add ten\n\nadd_ten <- function(x) {\n  return(x + 10)\n}\n\nadd_ten(10)\n\n\n[1] 20\n\n# Use it for map\n\nnumbers <- c(5,15,25)\n\nnumbers %>% \n  map_dbl(add_ten)\n\n\n[1] 15 25 35\n\n# if you want the object returned to be the same as the input:\n\nnumbers %>% \n  modify(add_ten)\n\n\n[1] 15 25 35\n\nGapminder dataset\n\n\nlibrary(gapminder)\n\nglimpse(gapminder)\n\n\nRows: 1,704\nColumns: 6\n$ country   <fct> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afgh…\n$ continent <fct> Asia, Asia, Asia, Asia, Asia, Asia, Asia, Asia, As…\n$ year      <int> 1952, 1957, 1962, 1967, 1972, 1977, 1982, 1987, 19…\n$ lifeExp   <dbl> 28.801, 30.332, 31.997, 34.020, 36.088, 38.438, 39…\n$ pop       <int> 8425333, 9240934, 10267083, 11537966, 13079460, 14…\n$ gdpPercap <dbl> 779.4453, 820.8530, 853.1007, 836.1971, 739.9811, …\n\n# To identify the class of each column:\n\ngapminder %>% \n  map_chr(class)\n\n\n  country continent      year   lifeExp       pop gdpPercap \n \"factor\"  \"factor\" \"integer\" \"numeric\" \"integer\" \"numeric\" \n\n# To identify the number of distinct values in the columnn:\n\ngapminder %>% \n  map_dbl(n_distinct)\n\n\n  country continent      year   lifeExp       pop gdpPercap \n      142         5        12      1626      1704      1704 \n\nTip:\nFirst figure out what code to use for single element of the data frame. Then paste it into map_df()\n\n\n# Extract single element (.x)\n.x <- gapminder %>% \n  pluck(1) %>%  # to take first element of list ie country\n  head()\n\n.x\n\n\n[1] Afghanistan Afghanistan Afghanistan Afghanistan Afghanistan\n[6] Afghanistan\n142 Levels: Afghanistan Albania Algeria Angola Argentina ... Zimbabwe\n\n# Create code (.f)\ndata.frame(n_distinct(.x),\n           class(.x))\n\n\n  n_distinct..x. class..x.\n1              1    factor\n\n# Paste into code for map\n\ngapminder %>% \n  map_df(~data.frame(n_distinct(.x),\n           class(.x)),\n         .id = \"variable\")\n\n\n   variable n_distinct..x. class..x.\n1   country            142    factor\n2 continent              5    factor\n3      year             12   integer\n4   lifeExp           1626   numeric\n5       pop           1704   integer\n6 gdpPercap           1704   numeric\n\nMaps with multiple input objects\nmap2(.x = object1, .y = object2, .f = function)\nList columns and Nested Data Frames\n\n\ngapminder_nested <- gapminder %>% \n  group_by(continent) %>% \n  nest()\n\ngapminder_nested\n\n\n# A tibble: 5 x 2\n# Groups:   continent [5]\n  continent data              \n  <fct>     <list>            \n1 Asia      <tibble [396 × 5]>\n2 Europe    <tibble [360 × 5]>\n3 Africa    <tibble [624 × 5]>\n4 Americas  <tibble [300 × 5]>\n5 Oceania   <tibble [24 × 5]> \n\ngapminder_nested %>% pluck(\"data\", 1)\n\n\n# A tibble: 396 x 5\n   country      year lifeExp      pop gdpPercap\n   <fct>       <int>   <dbl>    <int>     <dbl>\n 1 Afghanistan  1952    28.8  8425333      779.\n 2 Afghanistan  1957    30.3  9240934      821.\n 3 Afghanistan  1962    32.0 10267083      853.\n 4 Afghanistan  1967    34.0 11537966      836.\n 5 Afghanistan  1972    36.1 13079460      740.\n 6 Afghanistan  1977    38.4 14880372      786.\n 7 Afghanistan  1982    39.9 12881816      978.\n 8 Afghanistan  1987    40.8 13867957      852.\n 9 Afghanistan  1992    41.7 16317921      649.\n10 Afghanistan  1997    41.8 22227415      635.\n# … with 386 more rows\n\n# Calculate the average life expectancy within each continent and add it as a new column using mutate( ).\n\n.x <- gapminder_nested %>% \n  pluck(\"data\", 1)\n\n.f <- mean(.x$lifeExp)\n.f\n\n\n[1] 60.0649\n\n# put into map\n\ngapminder_nested %>% \n  mutate(av_life_exp = map_dbl(data, \n                               ~mean(.x$lifeExp)))\n\n\n# A tibble: 5 x 3\n# Groups:   continent [5]\n  continent data               av_life_exp\n  <fct>     <list>                   <dbl>\n1 Asia      <tibble [396 × 5]>        60.1\n2 Europe    <tibble [360 × 5]>        71.9\n3 Africa    <tibble [624 × 5]>        48.9\n4 Americas  <tibble [300 × 5]>        64.7\n5 Oceania   <tibble [24 × 5]>         74.3\n\nFitting a linear model separately for each continent\n\n\ngapminder_nested <- gapminder_nested %>% \n  mutate(lm_obj = map(data, ~lm(lifeExp ~ pop + gdpPercap + year, data = .x)))\n\ngapminder_nested\n\n\n# A tibble: 5 x 3\n# Groups:   continent [5]\n  continent data               lm_obj\n  <fct>     <list>             <list>\n1 Asia      <tibble [396 × 5]> <lm>  \n2 Europe    <tibble [360 × 5]> <lm>  \n3 Africa    <tibble [624 × 5]> <lm>  \n4 Americas  <tibble [300 × 5]> <lm>  \n5 Oceania   <tibble [24 × 5]>  <lm>  \n\ngapminder_nested %>% pluck(\"lm_obj\", 1)\n\n\n\nCall:\nlm(formula = lifeExp ~ pop + gdpPercap + year, data = .x)\n\nCoefficients:\n(Intercept)          pop    gdpPercap         year  \n -7.833e+02    4.228e-11    2.510e-04    4.251e-01  \n\n# predict the response for each continent\n\ngapminder_nested <- gapminder_nested %>% \n  mutate(pred = map2(lm_obj,\n                     data,\n                     function(.lm, .data) predict(.lm, .data)))\n\ngapminder_nested\n\n\n# A tibble: 5 x 4\n# Groups:   continent [5]\n  continent data               lm_obj pred       \n  <fct>     <list>             <list> <list>     \n1 Asia      <tibble [396 × 5]> <lm>   <dbl [396]>\n2 Europe    <tibble [360 × 5]> <lm>   <dbl [360]>\n3 Africa    <tibble [624 × 5]> <lm>   <dbl [624]>\n4 Americas  <tibble [300 × 5]> <lm>   <dbl [300]>\n5 Oceania   <tibble [24 × 5]>  <lm>   <dbl [24]> \n\n# then calculate the correlation between observed and predicted response for each continent\n\ngapminder_nested <- gapminder_nested %>% \n  mutate(cor = map2_dbl(pred,\n                        data,\n                        function(.pred, .data) cor(.pred, .data$lifeExp)))\n\n\ngapminder_nested\n\n\n# A tibble: 5 x 5\n# Groups:   continent [5]\n  continent data               lm_obj pred          cor\n  <fct>     <list>             <list> <list>      <dbl>\n1 Asia      <tibble [396 × 5]> <lm>   <dbl [396]> 0.723\n2 Europe    <tibble [360 × 5]> <lm>   <dbl [360]> 0.834\n3 Africa    <tibble [624 × 5]> <lm>   <dbl [624]> 0.645\n4 Americas  <tibble [300 × 5]> <lm>   <dbl [300]> 0.779\n5 Oceania   <tibble [24 × 5]>  <lm>   <dbl [24]>  0.987\n\n# advanced exercise\n\ngapminder %>% \n  group_by(continent) %>% \n  nest() %>% \n  mutate(lm_obj = map(data, ~lm(lifeExp ~ pop + year + gdpPercap, data = .))) %>% \n  mutate(lm_tidy = map(lm_obj, broom::tidy)) %>% \n  ungroup() %>% \n  transmute(continent, lm_tidy) %>%  # create new columns and dropping old\n  unnest(cols = c(lm_tidy))\n\n\n# A tibble: 20 x 6\n   continent term         estimate std.error statistic  p.value\n   <fct>     <chr>           <dbl>     <dbl>     <dbl>    <dbl>\n 1 Asia      (Intercept) -7.83e+ 2   4.83e+1  -16.2    1.22e-45\n 2 Asia      pop          4.23e-11   2.04e-9    0.0207 9.83e- 1\n 3 Asia      year         4.25e- 1   2.44e-2   17.4    1.13e-50\n 4 Asia      gdpPercap    2.51e- 4   3.01e-5    8.34   1.31e-15\n 5 Europe    (Intercept) -1.61e+ 2   2.28e+1   -7.09   7.44e-12\n 6 Europe    pop         -8.18e- 9   7.80e-9   -1.05   2.95e- 1\n 7 Europe    year         1.16e- 1   1.16e-2    9.96   8.88e-21\n 8 Europe    gdpPercap    3.25e- 4   2.15e-5   15.2    2.21e-40\n 9 Africa    (Intercept) -4.70e+ 2   3.39e+1  -13.9    2.17e-38\n10 Africa    pop         -3.68e- 9   1.89e-8   -0.195  8.45e- 1\n11 Africa    year         2.61e- 1   1.71e-2   15.2    1.07e-44\n12 Africa    gdpPercap    1.12e- 3   1.01e-4   11.1    2.46e-26\n13 Americas  (Intercept) -5.33e+ 2   4.10e+1  -13.0    6.40e-31\n14 Americas  pop         -2.15e- 8   8.62e-9   -2.49   1.32e- 2\n15 Americas  year         3.00e- 1   2.08e-2   14.4    3.79e-36\n16 Americas  gdpPercap    6.75e- 4   7.15e-5    9.44   1.13e-18\n17 Oceania   (Intercept) -2.10e+ 2   5.12e+1   -4.10   5.61e- 4\n18 Oceania   pop          8.37e- 9   3.34e-8    0.251  8.05e- 1\n19 Oceania   year         1.42e- 1   2.65e-2    5.34   3.19e- 5\n20 Oceania   gdpPercap    2.03e- 4   8.47e-5    2.39   2.66e- 2\n\nAnother tutorial from Jenny Bryan\nThe worked examples below are from: https://jennybc.github.io/purrr-tutorial/\n\n\nlibrary(purrr)\nlibrary(repurrrsive)\nlibrary(listviewer)\n\n# this is a list\nstr(wesanderson)\n\n\nList of 15\n $ GrandBudapest : chr [1:4] \"#F1BB7B\" \"#FD6467\" \"#5B1A18\" \"#D67236\"\n $ Moonrise1     : chr [1:4] \"#F3DF6C\" \"#CEAB07\" \"#D5D5D3\" \"#24281A\"\n $ Royal1        : chr [1:4] \"#899DA4\" \"#C93312\" \"#FAEFD1\" \"#DC863B\"\n $ Moonrise2     : chr [1:4] \"#798E87\" \"#C27D38\" \"#CCC591\" \"#29211F\"\n $ Cavalcanti    : chr [1:5] \"#D8B70A\" \"#02401B\" \"#A2A475\" \"#81A88D\" ...\n $ Royal2        : chr [1:5] \"#9A8822\" \"#F5CDB4\" \"#F8AFA8\" \"#FDDDA0\" ...\n $ GrandBudapest2: chr [1:4] \"#E6A0C4\" \"#C6CDF7\" \"#D8A499\" \"#7294D4\"\n $ Moonrise3     : chr [1:5] \"#85D4E3\" \"#F4B5BD\" \"#9C964A\" \"#CDC08C\" ...\n $ Chevalier     : chr [1:4] \"#446455\" \"#FDD262\" \"#D3DDDC\" \"#C7B19C\"\n $ Zissou        : chr [1:5] \"#3B9AB2\" \"#78B7C5\" \"#EBCC2A\" \"#E1AF00\" ...\n $ FantasticFox  : chr [1:5] \"#DD8D29\" \"#E2D200\" \"#46ACC8\" \"#E58601\" ...\n $ Darjeeling    : chr [1:5] \"#FF0000\" \"#00A08A\" \"#F2AD00\" \"#F98400\" ...\n $ Rushmore      : chr [1:5] \"#E1BD6D\" \"#EABE94\" \"#0B775E\" \"#35274A\" ...\n $ BottleRocket  : chr [1:7] \"#A42820\" \"#5F5647\" \"#9B110E\" \"#3F5151\" ...\n $ Darjeeling2   : chr [1:5] \"#ECCBAE\" \"#046C9A\" \"#D69C4E\" \"#ABDDDE\" ...\n\n# use listviewer to view the list\njsonedit(wesanderson)\n\n\n\n{\"x\":{\"data\":{\"GrandBudapest\":[\"#F1BB7B\",\"#FD6467\",\"#5B1A18\",\"#D67236\"],\"Moonrise1\":[\"#F3DF6C\",\"#CEAB07\",\"#D5D5D3\",\"#24281A\"],\"Royal1\":[\"#899DA4\",\"#C93312\",\"#FAEFD1\",\"#DC863B\"],\"Moonrise2\":[\"#798E87\",\"#C27D38\",\"#CCC591\",\"#29211F\"],\"Cavalcanti\":[\"#D8B70A\",\"#02401B\",\"#A2A475\",\"#81A88D\",\"#972D15\"],\"Royal2\":[\"#9A8822\",\"#F5CDB4\",\"#F8AFA8\",\"#FDDDA0\",\"#74A089\"],\"GrandBudapest2\":[\"#E6A0C4\",\"#C6CDF7\",\"#D8A499\",\"#7294D4\"],\"Moonrise3\":[\"#85D4E3\",\"#F4B5BD\",\"#9C964A\",\"#CDC08C\",\"#FAD77B\"],\"Chevalier\":[\"#446455\",\"#FDD262\",\"#D3DDDC\",\"#C7B19C\"],\"Zissou\":[\"#3B9AB2\",\"#78B7C5\",\"#EBCC2A\",\"#E1AF00\",\"#F21A00\"],\"FantasticFox\":[\"#DD8D29\",\"#E2D200\",\"#46ACC8\",\"#E58601\",\"#B40F20\"],\"Darjeeling\":[\"#FF0000\",\"#00A08A\",\"#F2AD00\",\"#F98400\",\"#5BBCD6\"],\"Rushmore\":[\"#E1BD6D\",\"#EABE94\",\"#0B775E\",\"#35274A\",\"#F2300F\"],\"BottleRocket\":[\"#A42820\",\"#5F5647\",\"#9B110E\",\"#3F5151\",\"#4E2A1E\",\"#550307\",\"#0C1707\"],\"Darjeeling2\":[\"#ECCBAE\",\"#046C9A\",\"#D69C4E\",\"#ABDDDE\",\"#000000\"]},\"options\":{\"mode\":\"tree\",\"modes\":[\"code\",\"form\",\"text\",\"tree\",\"view\"]}},\"evals\":[],\"jsHooks\":[]}\n# list for Game of Thrones dataset\nstr(got_chars)\n\n\nList of 30\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1022\"\n  ..$ id         : int 1022\n  ..$ name       : chr \"Theon Greyjoy\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Ironborn\"\n  ..$ born       : chr \"In 278 AC or 279 AC, at Pyke\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:3] \"Prince of Winterfell\" \"Captain of Sea Bitch\" \"Lord of the Iron Islands (by law of the green lands)\"\n  ..$ aliases    : chr [1:4] \"Prince of Fools\" \"Theon Turncloak\" \"Reek\" \"Theon Kinslayer\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Greyjoy of Pyke\"\n  ..$ books      : chr [1:3] \"A Game of Thrones\" \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ povBooks   : chr [1:2] \"A Clash of Kings\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Alfie Allen\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1052\"\n  ..$ id         : int 1052\n  ..$ name       : chr \"Tyrion Lannister\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"In 273 AC, at Casterly Rock\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:2] \"Acting Hand of the King (former)\" \"Master of Coin (former)\"\n  ..$ aliases    : chr [1:11] \"The Imp\" \"Halfman\" \"The boyman\" \"Giant of Lannister\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/2044\"\n  ..$ allegiances: chr \"House Lannister of Casterly Rock\"\n  ..$ books      : chr [1:2] \"A Feast for Crows\" \"The World of Ice and Fire\"\n  ..$ povBooks   : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Peter Dinklage\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1074\"\n  ..$ id         : int 1074\n  ..$ name       : chr \"Victarion Greyjoy\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Ironborn\"\n  ..$ born       : chr \"In 268 AC or before, at Pyke\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:2] \"Lord Captain of the Iron Fleet\" \"Master of the Iron Victory\"\n  ..$ aliases    : chr \"The Iron Captain\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Greyjoy of Pyke\"\n  ..$ books      : chr [1:3] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\"\n  ..$ povBooks   : chr [1:2] \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1109\"\n  ..$ id         : int 1109\n  ..$ name       : chr \"Will\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"\"\n  ..$ died       : chr \"In 297 AC, at Haunted Forest\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: list()\n  ..$ books      : chr \"A Clash of Kings\"\n  ..$ povBooks   : chr \"A Game of Thrones\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"Bronson Webb\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1166\"\n  ..$ id         : int 1166\n  ..$ name       : chr \"Areo Hotah\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Norvoshi\"\n  ..$ born       : chr \"In 257 AC or before, at Norvos\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Captain of the Guard at Sunspear\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Nymeros Martell of Sunspear\"\n  ..$ books      : chr [1:3] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\"\n  ..$ povBooks   : chr [1:2] \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:2] \"Season 5\" \"Season 6\"\n  ..$ playedBy   : chr \"DeObia Oparei\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1267\"\n  ..$ id         : int 1267\n  ..$ name       : chr \"Chett\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"At Hag's Mire\"\n  ..$ died       : chr \"In 299 AC, at Fist of the First Men\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: list()\n  ..$ books      : chr [1:2] \"A Game of Thrones\" \"A Clash of Kings\"\n  ..$ povBooks   : chr \"A Storm of Swords\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1295\"\n  ..$ id         : int 1295\n  ..$ name       : chr \"Cressen\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"In 219 AC or 220 AC\"\n  ..$ died       : chr \"In 299 AC, at Dragonstone\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"Maester\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: list()\n  ..$ books      : chr [1:2] \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ povBooks   : chr \"A Clash of Kings\"\n  ..$ tvSeries   : chr \"Season 2\"\n  ..$ playedBy   : chr \"Oliver Ford\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/130\"\n  ..$ id         : int 130\n  ..$ name       : chr \"Arianne Martell\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Dornish\"\n  ..$ born       : chr \"In 276 AC, at Sunspear\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Princess of Dorne\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Nymeros Martell of Sunspear\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr \"A Feast for Crows\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1303\"\n  ..$ id         : int 1303\n  ..$ name       : chr \"Daenerys Targaryen\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Valyrian\"\n  ..$ born       : chr \"In 284 AC, at Dragonstone\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:5] \"Queen of the Andals and the Rhoynar and the First Men, Lord of the Seven Kingdoms\" \"Khaleesi of the Great Grass Sea\" \"Breaker of Shackles/Chains\" \"Queen of Meereen\" ...\n  ..$ aliases    : chr [1:11] \"Dany\" \"Daenerys Stormborn\" \"The Unburnt\" \"Mother of Dragons\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/1346\"\n  ..$ allegiances: chr \"House Targaryen of King's Landing\"\n  ..$ books      : chr \"A Feast for Crows\"\n  ..$ povBooks   : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Emilia Clarke\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/1319\"\n  ..$ id         : int 1319\n  ..$ name       : chr \"Davos Seaworth\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Westeros\"\n  ..$ born       : chr \"In 260 AC or before, at King's Landing\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:4] \"Ser\" \"Lord of the Rainwood\" \"Admiral of the Narrow Sea\" \"Hand of the King\"\n  ..$ aliases    : chr [1:5] \"Onion Knight\" \"Davos Shorthand\" \"Ser Onions\" \"Onion Lord\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/1676\"\n  ..$ allegiances: chr [1:2] \"House Baratheon of Dragonstone\" \"House Seaworth of Cape Wrath\"\n  ..$ books      : chr \"A Feast for Crows\"\n  ..$ povBooks   : chr [1:3] \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:5] \"Season 2\" \"Season 3\" \"Season 4\" \"Season 5\" ...\n  ..$ playedBy   : chr \"Liam Cunningham\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/148\"\n  ..$ id         : int 148\n  ..$ name       : chr \"Arya Stark\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Northmen\"\n  ..$ born       : chr \"In 289 AC, at Winterfell\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Princess\"\n  ..$ aliases    : chr [1:16] \"Arya Horseface\" \"Arya Underfoot\" \"Arry\" \"Lumpyface\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Stark of Winterfell\"\n  ..$ books      : list()\n  ..$ povBooks   : chr [1:5] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\" ...\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Maisie Williams\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/149\"\n  ..$ id         : int 149\n  ..$ name       : chr \"Arys Oakheart\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Reach\"\n  ..$ born       : chr \"At Old Oak\"\n  ..$ died       : chr \"In 300 AC, at the Greenblood\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"Ser\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Oakheart of Old Oak\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr \"A Feast for Crows\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/150\"\n  ..$ id         : int 150\n  ..$ name       : chr \"Asha Greyjoy\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Ironborn\"\n  ..$ born       : chr \"In 275 AC or 276 AC, at Pyke\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:3] \"Princess\" \"Captain of the Black Wind\" \"Conqueror of Deepwood Motte\"\n  ..$ aliases    : chr [1:2] \"Esgred\" \"The Kraken's Daughter\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/1372\"\n  ..$ allegiances: chr [1:2] \"House Greyjoy of Pyke\" \"House Ironmaker\"\n  ..$ books      : chr [1:2] \"A Game of Thrones\" \"A Clash of Kings\"\n  ..$ povBooks   : chr [1:2] \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:3] \"Season 2\" \"Season 3\" \"Season 4\"\n  ..$ playedBy   : chr \"Gemma Whelan\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/168\"\n  ..$ id         : int 168\n  ..$ name       : chr \"Barristan Selmy\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Westeros\"\n  ..$ born       : chr \"In 237 AC\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:2] \"Ser\" \"Hand of the Queen\"\n  ..$ aliases    : chr [1:5] \"Barristan the Bold\" \"Arstan Whitebeard\" \"Ser Grandfather\" \"Barristan the Old\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr [1:2] \"House Selmy of Harvest Hall\" \"House Targaryen of King's Landing\"\n  ..$ books      : chr [1:5] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\" ...\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:4] \"Season 1\" \"Season 3\" \"Season 4\" \"Season 5\"\n  ..$ playedBy   : chr \"Ian McElhinney\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/2066\"\n  ..$ id         : int 2066\n  ..$ name       : chr \"Varamyr\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Free Folk\"\n  ..$ born       : chr \"At a village Beyond the Wall\"\n  ..$ died       : chr \"In 300 AC, at a village Beyond the Wall\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr [1:3] \"Varamyr Sixskins\" \"Haggon\" \"Lump\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: list()\n  ..$ books      : chr \"A Storm of Swords\"\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/208\"\n  ..$ id         : int 208\n  ..$ name       : chr \"Brandon Stark\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Northmen\"\n  ..$ born       : chr \"In 290 AC, at Winterfell\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Prince of Winterfell\"\n  ..$ aliases    : chr [1:3] \"Bran\" \"Bran the Broken\" \"The Winged Wolf\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Stark of Winterfell\"\n  ..$ books      : chr \"A Feast for Crows\"\n  ..$ povBooks   : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:5] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Isaac Hempstead-Wright\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/216\"\n  ..$ id         : int 216\n  ..$ name       : chr \"Brienne of Tarth\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"In 280 AC\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr [1:3] \"The Maid of Tarth\" \"Brienne the Beauty\" \"Brienne the Blue\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr [1:3] \"House Baratheon of Storm's End\" \"House Stark of Winterfell\" \"House Tarth of Evenfall Hall\"\n  ..$ books      : chr [1:3] \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr \"A Feast for Crows\"\n  ..$ tvSeries   : chr [1:5] \"Season 2\" \"Season 3\" \"Season 4\" \"Season 5\" ...\n  ..$ playedBy   : chr \"Gwendoline Christie\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/232\"\n  ..$ id         : int 232\n  ..$ name       : chr \"Catelyn Stark\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Rivermen\"\n  ..$ born       : chr \"In 264 AC, at Riverrun\"\n  ..$ died       : chr \"In 299 AC, at the Twins\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"Lady of Winterfell\"\n  ..$ aliases    : chr [1:5] \"Catelyn Tully\" \"Lady Stoneheart\" \"The Silent Sistet\" \"Mother Mercilesr\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/339\"\n  ..$ allegiances: chr [1:2] \"House Stark of Winterfell\" \"House Tully of Riverrun\"\n  ..$ books      : chr [1:2] \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr [1:3] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\"\n  ..$ tvSeries   : chr [1:3] \"Season 1\" \"Season 2\" \"Season 3\"\n  ..$ playedBy   : chr \"Michelle Fairley\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/238\"\n  ..$ id         : int 238\n  ..$ name       : chr \"Cersei Lannister\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Westerman\"\n  ..$ born       : chr \"In 266 AC, at Casterly Rock\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:5] \"Light of the West\" \"Queen Dowager\" \"Protector of the Realm\" \"Lady of Casterly Rock\" ...\n  ..$ aliases    : list()\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/901\"\n  ..$ allegiances: chr \"House Lannister of Casterly Rock\"\n  ..$ books      : chr [1:3] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\"\n  ..$ povBooks   : chr [1:2] \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Lena Headey\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/339\"\n  ..$ id         : int 339\n  ..$ name       : chr \"Eddard Stark\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Northmen\"\n  ..$ born       : chr \"In 263 AC, at Winterfell\"\n  ..$ died       : chr \"In 299 AC, at Great Sept of Baelor in King's Landing\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr [1:5] \"Lord of Winterfell\" \"Warden of the North\" \"Hand of the King\" \"Protector of the Realm\" ...\n  ..$ aliases    : chr [1:3] \"Ned\" \"The Ned\" \"The Quiet Wolf\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/232\"\n  ..$ allegiances: chr \"House Stark of Winterfell\"\n  ..$ books      : chr [1:5] \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\" \"A Dance with Dragons\" ...\n  ..$ povBooks   : chr \"A Game of Thrones\"\n  ..$ tvSeries   : chr [1:2] \"Season 1\" \"Season 6\"\n  ..$ playedBy   : chr [1:3] \"Sean Bean\" \"Sebastian Croft\" \"Robert Aramayo\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/529\"\n  ..$ id         : int 529\n  ..$ name       : chr \"Jaime Lannister\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Westerlands\"\n  ..$ born       : chr \"In 266 AC, at Casterly Rock\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:3] \"Ser\" \"Lord Commander of the Kingsguard\" \"Warden of the East (formerly)\"\n  ..$ aliases    : chr [1:4] \"The Kingslayer\" \"The Lion of Lannister\" \"The Young Lion\" \"Cripple\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Lannister of Casterly Rock\"\n  ..$ books      : chr [1:2] \"A Game of Thrones\" \"A Clash of Kings\"\n  ..$ povBooks   : chr [1:3] \"A Storm of Swords\" \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:5] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Nikolaj Coster-Waldau\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/576\"\n  ..$ id         : int 576\n  ..$ name       : chr \"Jon Connington\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Stormlands\"\n  ..$ born       : chr \"In or between 263 AC and 265 AC\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:3] \"Lord of Griffin's Roost\" \"Hand of the King\" \"Hand of the True King\"\n  ..$ aliases    : chr \"Griffthe Mad King's Hand\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr [1:2] \"House Connington of Griffin's Roost\" \"House Targaryen of King's Landing\"\n  ..$ books      : chr [1:3] \"A Storm of Swords\" \"A Feast for Crows\" \"The World of Ice and Fire\"\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/583\"\n  ..$ id         : int 583\n  ..$ name       : chr \"Jon Snow\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Northmen\"\n  ..$ born       : chr \"In 283 AC\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Lord Commander of the Night's Watch\"\n  ..$ aliases    : chr [1:8] \"Lord Snow\" \"Ned Stark's Bastard\" \"The Snow of Winterfell\" \"The Crow-Come-Over\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Stark of Winterfell\"\n  ..$ books      : chr \"A Feast for Crows\"\n  ..$ povBooks   : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Kit Harington\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/60\"\n  ..$ id         : int 60\n  ..$ name       : chr \"Aeron Greyjoy\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Ironborn\"\n  ..$ born       : chr \"In or between 269 AC and 273 AC, at Pyke\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr [1:2] \"Priest of the Drowned God\" \"Captain of the Golden Storm (formerly)\"\n  ..$ aliases    : chr [1:2] \"The Damphair\" \"Aeron Damphair\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Greyjoy of Pyke\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr \"A Feast for Crows\"\n  ..$ tvSeries   : chr \"Season 6\"\n  ..$ playedBy   : chr \"Michael Feast\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/605\"\n  ..$ id         : int 605\n  ..$ name       : chr \"Kevan Lannister\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"\"\n  ..$ born       : chr \"In 244 AC\"\n  ..$ died       : chr \"In 300 AC, at King's Landing\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr [1:4] \"Ser\" \"Master of laws\" \"Lord Regent\" \"Protector of the Realm\"\n  ..$ aliases    : chr \"\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/327\"\n  ..$ allegiances: chr \"House Lannister of Casterly Rock\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:4] \"Season 1\" \"Season 2\" \"Season 5\" \"Season 6\"\n  ..$ playedBy   : chr \"Ian Gelder\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/743\"\n  ..$ id         : int 743\n  ..$ name       : chr \"Melisandre\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Asshai\"\n  ..$ born       : chr \"At Unknown\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr [1:5] \"The Red Priestess\" \"The Red Woman\" \"The King's Red Shadow\" \"Lady Red\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: list()\n  ..$ books      : chr [1:3] \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr [1:5] \"Season 2\" \"Season 3\" \"Season 4\" \"Season 5\" ...\n  ..$ playedBy   : chr \"Carice van Houten\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/751\"\n  ..$ id         : int 751\n  ..$ name       : chr \"Merrett Frey\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Rivermen\"\n  ..$ born       : chr \"In 262 AC\"\n  ..$ died       : chr \"In 300 AC, at Near Oldstones\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr \"Merrett Muttonhead\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/712\"\n  ..$ allegiances: chr \"House Frey of the Crossing\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Feast for Crows\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr \"A Storm of Swords\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/844\"\n  ..$ id         : int 844\n  ..$ name       : chr \"Quentyn Martell\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Dornish\"\n  ..$ born       : chr \"In 281 AC, at Sunspear, Dorne\"\n  ..$ died       : chr \"In 300 AC, at Meereen\"\n  ..$ alive      : logi FALSE\n  ..$ titles     : chr \"Prince\"\n  ..$ aliases    : chr [1:4] \"Frog\" \"Prince Frog\" \"The prince who came too late\" \"The Dragonrider\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Nymeros Martell of Sunspear\"\n  ..$ books      : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ povBooks   : chr \"A Dance with Dragons\"\n  ..$ tvSeries   : chr \"\"\n  ..$ playedBy   : chr \"\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/954\"\n  ..$ id         : int 954\n  ..$ name       : chr \"Samwell Tarly\"\n  ..$ gender     : chr \"Male\"\n  ..$ culture    : chr \"Andal\"\n  ..$ born       : chr \"In 283 AC, at Horn Hill\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"\"\n  ..$ aliases    : chr [1:7] \"Sam\" \"Ser Piggy\" \"Prince Pork-chop\" \"Lady Piggy\" ...\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"\"\n  ..$ allegiances: chr \"House Tarly of Horn Hill\"\n  ..$ books      : chr [1:3] \"A Game of Thrones\" \"A Clash of Kings\" \"A Dance with Dragons\"\n  ..$ povBooks   : chr [1:2] \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"John Bradley-West\"\n $ :List of 18\n  ..$ url        : chr \"https://www.anapioficeandfire.com/api/characters/957\"\n  ..$ id         : int 957\n  ..$ name       : chr \"Sansa Stark\"\n  ..$ gender     : chr \"Female\"\n  ..$ culture    : chr \"Northmen\"\n  ..$ born       : chr \"In 286 AC, at Winterfell\"\n  ..$ died       : chr \"\"\n  ..$ alive      : logi TRUE\n  ..$ titles     : chr \"Princess\"\n  ..$ aliases    : chr [1:3] \"Little bird\" \"Alayne Stone\" \"Jonquil\"\n  ..$ father     : chr \"\"\n  ..$ mother     : chr \"\"\n  ..$ spouse     : chr \"https://www.anapioficeandfire.com/api/characters/1052\"\n  ..$ allegiances: chr [1:2] \"House Baelish of Harrenhal\" \"House Stark of Winterfell\"\n  ..$ books      : chr \"A Dance with Dragons\"\n  ..$ povBooks   : chr [1:4] \"A Game of Thrones\" \"A Clash of Kings\" \"A Storm of Swords\" \"A Feast for Crows\"\n  ..$ tvSeries   : chr [1:6] \"Season 1\" \"Season 2\" \"Season 3\" \"Season 4\" ...\n  ..$ playedBy   : chr \"Sophie Turner\"\n\njsonedit(got_chars)\n\n\n\n{\"x\":{\"data\":[{\"url\":\"https://www.anapioficeandfire.com/api/characters/1022\",\"id\":1022,\"name\":\"Theon Greyjoy\",\"gender\":\"Male\",\"culture\":\"Ironborn\",\"born\":\"In 278 AC or 279 AC, at Pyke\",\"died\":\"\",\"alive\":true,\"titles\":[\"Prince of Winterfell\",\"Captain of Sea Bitch\",\"Lord of the Iron Islands (by law of the green lands)\"],\"aliases\":[\"Prince of Fools\",\"Theon Turncloak\",\"Reek\",\"Theon Kinslayer\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Greyjoy of Pyke\",\"books\":[\"A Game of Thrones\",\"A Storm of Swords\",\"A Feast for Crows\"],\"povBooks\":[\"A Clash of Kings\",\"A Dance with Dragons\"],\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\"],\"playedBy\":\"Alfie Allen\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/1052\",\"id\":1052,\"name\":\"Tyrion Lannister\",\"gender\":\"Male\",\"culture\":\"\",\"born\":\"In 273 AC, at Casterly Rock\",\"died\":\"\",\"alive\":true,\"titles\":[\"Acting Hand of the King (former)\",\"Master of Coin (former)\"],\"aliases\":[\"The Imp\",\"Halfman\",\"The boyman\",\"Giant of Lannister\",\"Lord Tywin's Doom\",\"Lord Tywin's Bane\",\"Yollo\",\"Hugor Hill\",\"No-Nose\",\"Freak\",\"Dwarf\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"https://www.anapioficeandfire.com/api/characters/2044\",\"allegiances\":\"House Lannister of Casterly Rock\",\"books\":[\"A Feast for Crows\",\"The World of Ice and Fire\"],\"povBooks\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Dance with Dragons\"],\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\"],\"playedBy\":\"Peter Dinklage\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/1074\",\"id\":1074,\"name\":\"Victarion Greyjoy\",\"gender\":\"Male\",\"culture\":\"Ironborn\",\"born\":\"In 268 AC or before, at Pyke\",\"died\":\"\",\"alive\":true,\"titles\":[\"Lord Captain of the Iron Fleet\",\"Master of the Iron Victory\"],\"aliases\":\"The Iron Captain\",\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Greyjoy of Pyke\",\"books\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\"],\"povBooks\":[\"A Feast for Crows\",\"A Dance with Dragons\"],\"tvSeries\":\"\",\"playedBy\":\"\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/1109\",\"id\":1109,\"name\":\"Will\",\"gender\":\"Male\",\"culture\":\"\",\"born\":\"\",\"died\":\"In 297 AC, at Haunted Forest\",\"alive\":false,\"titles\":\"\",\"aliases\":\"\",\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":[],\"books\":\"A Clash of Kings\",\"povBooks\":\"A Game of Thrones\",\"tvSeries\":\"\",\"playedBy\":\"Bronson Webb\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/1166\",\"id\":1166,\"name\":\"Areo Hotah\",\"gender\":\"Male\",\"culture\":\"Norvoshi\",\"born\":\"In 257 AC or before, at Norvos\",\"died\":\"\",\"alive\":true,\"titles\":\"Captain of the Guard at Sunspear\",\"aliases\":\"\",\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Nymeros Martell of Sunspear\",\"books\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\"],\"povBooks\":[\"A Feast for Crows\",\"A Dance with Dragons\"],\"tvSeries\":[\"Season 5\",\"Season 6\"],\"playedBy\":\"DeObia Oparei\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/1267\",\"id\":1267,\"name\":\"Chett\",\"gender\":\"Male\",\"culture\":\"\",\"born\":\"At Hag's Mire\",\"died\":\"In 299 AC, at Fist of the First Men\",\"alive\":false,\"titles\":\"\",\"aliases\":\"\",\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":[],\"books\":[\"A Game of Thrones\",\"A Clash of Kings\"],\"povBooks\":\"A Storm of Swords\",\"tvSeries\":\"\",\"playedBy\":\"\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/1295\",\"id\":1295,\"name\":\"Cressen\",\"gender\":\"Male\",\"culture\":\"\",\"born\":\"In 219 AC or 220 AC\",\"died\":\"In 299 AC, at Dragonstone\",\"alive\":false,\"titles\":\"Maester\",\"aliases\":\"\",\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":[],\"books\":[\"A Storm of Swords\",\"A Feast for Crows\"],\"povBooks\":\"A Clash of Kings\",\"tvSeries\":\"Season 2\",\"playedBy\":\"Oliver Ford\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/130\",\"id\":130,\"name\":\"Arianne Martell\",\"gender\":\"Female\",\"culture\":\"Dornish\",\"born\":\"In 276 AC, at Sunspear\",\"died\":\"\",\"alive\":true,\"titles\":\"Princess of Dorne\",\"aliases\":\"\",\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Nymeros Martell of Sunspear\",\"books\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Dance with Dragons\"],\"povBooks\":\"A Feast for Crows\",\"tvSeries\":\"\",\"playedBy\":\"\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/1303\",\"id\":1303,\"name\":\"Daenerys Targaryen\",\"gender\":\"Female\",\"culture\":\"Valyrian\",\"born\":\"In 284 AC, at Dragonstone\",\"died\":\"\",\"alive\":true,\"titles\":[\"Queen of the Andals and the Rhoynar and the First Men, Lord of the Seven Kingdoms\",\"Khaleesi of the Great Grass Sea\",\"Breaker of Shackles/Chains\",\"Queen of Meereen\",\"Princess of Dragonstone\"],\"aliases\":[\"Dany\",\"Daenerys Stormborn\",\"The Unburnt\",\"Mother of Dragons\",\"Mother\",\"Mhysa\",\"The Silver Queen\",\"Silver Lady\",\"Dragonmother\",\"The Dragon Queen\",\"The Mad King's daughter\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"https://www.anapioficeandfire.com/api/characters/1346\",\"allegiances\":\"House Targaryen of King's Landing\",\"books\":\"A Feast for Crows\",\"povBooks\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Dance with Dragons\"],\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\"],\"playedBy\":\"Emilia Clarke\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/1319\",\"id\":1319,\"name\":\"Davos Seaworth\",\"gender\":\"Male\",\"culture\":\"Westeros\",\"born\":\"In 260 AC or before, at King's Landing\",\"died\":\"\",\"alive\":true,\"titles\":[\"Ser\",\"Lord of the Rainwood\",\"Admiral of the Narrow Sea\",\"Hand of the King\"],\"aliases\":[\"Onion Knight\",\"Davos Shorthand\",\"Ser Onions\",\"Onion Lord\",\"Smuggler\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"https://www.anapioficeandfire.com/api/characters/1676\",\"allegiances\":[\"House Baratheon of Dragonstone\",\"House Seaworth of Cape Wrath\"],\"books\":\"A Feast for Crows\",\"povBooks\":[\"A Clash of Kings\",\"A Storm of Swords\",\"A Dance with Dragons\"],\"tvSeries\":[\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\"],\"playedBy\":\"Liam Cunningham\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/148\",\"id\":148,\"name\":\"Arya Stark\",\"gender\":\"Female\",\"culture\":\"Northmen\",\"born\":\"In 289 AC, at Winterfell\",\"died\":\"\",\"alive\":true,\"titles\":\"Princess\",\"aliases\":[\"Arya Horseface\",\"Arya Underfoot\",\"Arry\",\"Lumpyface\",\"Lumpyhead\",\"Stickboy\",\"Weasel\",\"Nymeria\",\"Squan\",\"Saltb\",\"Cat of the Canaly\",\"Bets\",\"The Blind Girh\",\"The Ugly Little Girl\",\"Mercedenl\",\"Mercye\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Stark of Winterfell\",\"books\":[],\"povBooks\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Feast for Crows\",\"A Dance with Dragons\"],\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\"],\"playedBy\":\"Maisie Williams\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/149\",\"id\":149,\"name\":\"Arys Oakheart\",\"gender\":\"Male\",\"culture\":\"Reach\",\"born\":\"At Old Oak\",\"died\":\"In 300 AC, at the Greenblood\",\"alive\":false,\"titles\":\"Ser\",\"aliases\":\"\",\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Oakheart of Old Oak\",\"books\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Dance with Dragons\"],\"povBooks\":\"A Feast for Crows\",\"tvSeries\":\"\",\"playedBy\":\"\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/150\",\"id\":150,\"name\":\"Asha Greyjoy\",\"gender\":\"Female\",\"culture\":\"Ironborn\",\"born\":\"In 275 AC or 276 AC, at Pyke\",\"died\":\"\",\"alive\":true,\"titles\":[\"Princess\",\"Captain of the Black Wind\",\"Conqueror of Deepwood Motte\"],\"aliases\":[\"Esgred\",\"The Kraken's Daughter\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"https://www.anapioficeandfire.com/api/characters/1372\",\"allegiances\":[\"House Greyjoy of Pyke\",\"House Ironmaker\"],\"books\":[\"A Game of Thrones\",\"A Clash of Kings\"],\"povBooks\":[\"A Feast for Crows\",\"A Dance with Dragons\"],\"tvSeries\":[\"Season 2\",\"Season 3\",\"Season 4\"],\"playedBy\":\"Gemma Whelan\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/168\",\"id\":168,\"name\":\"Barristan Selmy\",\"gender\":\"Male\",\"culture\":\"Westeros\",\"born\":\"In 237 AC\",\"died\":\"\",\"alive\":true,\"titles\":[\"Ser\",\"Hand of the Queen\"],\"aliases\":[\"Barristan the Bold\",\"Arstan Whitebeard\",\"Ser Grandfather\",\"Barristan the Old\",\"Old Ser\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":[\"House Selmy of Harvest Hall\",\"House Targaryen of King's Landing\"],\"books\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Feast for Crows\",\"The World of Ice and Fire\"],\"povBooks\":\"A Dance with Dragons\",\"tvSeries\":[\"Season 1\",\"Season 3\",\"Season 4\",\"Season 5\"],\"playedBy\":\"Ian McElhinney\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/2066\",\"id\":2066,\"name\":\"Varamyr\",\"gender\":\"Male\",\"culture\":\"Free Folk\",\"born\":\"At a village Beyond the Wall\",\"died\":\"In 300 AC, at a village Beyond the Wall\",\"alive\":false,\"titles\":\"\",\"aliases\":[\"Varamyr Sixskins\",\"Haggon\",\"Lump\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":[],\"books\":\"A Storm of Swords\",\"povBooks\":\"A Dance with Dragons\",\"tvSeries\":\"\",\"playedBy\":\"\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/208\",\"id\":208,\"name\":\"Brandon Stark\",\"gender\":\"Male\",\"culture\":\"Northmen\",\"born\":\"In 290 AC, at Winterfell\",\"died\":\"\",\"alive\":true,\"titles\":\"Prince of Winterfell\",\"aliases\":[\"Bran\",\"Bran the Broken\",\"The Winged Wolf\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Stark of Winterfell\",\"books\":\"A Feast for Crows\",\"povBooks\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Dance with Dragons\"],\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 6\"],\"playedBy\":\"Isaac Hempstead-Wright\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/216\",\"id\":216,\"name\":\"Brienne of Tarth\",\"gender\":\"Female\",\"culture\":\"\",\"born\":\"In 280 AC\",\"died\":\"\",\"alive\":true,\"titles\":\"\",\"aliases\":[\"The Maid of Tarth\",\"Brienne the Beauty\",\"Brienne the Blue\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":[\"House Baratheon of Storm's End\",\"House Stark of Winterfell\",\"House Tarth of Evenfall Hall\"],\"books\":[\"A Clash of Kings\",\"A Storm of Swords\",\"A Dance with Dragons\"],\"povBooks\":\"A Feast for Crows\",\"tvSeries\":[\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\"],\"playedBy\":\"Gwendoline Christie\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/232\",\"id\":232,\"name\":\"Catelyn Stark\",\"gender\":\"Female\",\"culture\":\"Rivermen\",\"born\":\"In 264 AC, at Riverrun\",\"died\":\"In 299 AC, at the Twins\",\"alive\":false,\"titles\":\"Lady of Winterfell\",\"aliases\":[\"Catelyn Tully\",\"Lady Stoneheart\",\"The Silent Sistet\",\"Mother Mercilesr\",\"The Hangwomans\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"https://www.anapioficeandfire.com/api/characters/339\",\"allegiances\":[\"House Stark of Winterfell\",\"House Tully of Riverrun\"],\"books\":[\"A Feast for Crows\",\"A Dance with Dragons\"],\"povBooks\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\"],\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 3\"],\"playedBy\":\"Michelle Fairley\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/238\",\"id\":238,\"name\":\"Cersei Lannister\",\"gender\":\"Female\",\"culture\":\"Westerman\",\"born\":\"In 266 AC, at Casterly Rock\",\"died\":\"\",\"alive\":true,\"titles\":[\"Light of the West\",\"Queen Dowager\",\"Protector of the Realm\",\"Lady of Casterly Rock\",\"Queen Regent\"],\"aliases\":[],\"father\":\"\",\"mother\":\"\",\"spouse\":\"https://www.anapioficeandfire.com/api/characters/901\",\"allegiances\":\"House Lannister of Casterly Rock\",\"books\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\"],\"povBooks\":[\"A Feast for Crows\",\"A Dance with Dragons\"],\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\"],\"playedBy\":\"Lena Headey\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/339\",\"id\":339,\"name\":\"Eddard Stark\",\"gender\":\"Male\",\"culture\":\"Northmen\",\"born\":\"In 263 AC, at Winterfell\",\"died\":\"In 299 AC, at Great Sept of Baelor in King's Landing\",\"alive\":false,\"titles\":[\"Lord of Winterfell\",\"Warden of the North\",\"Hand of the King\",\"Protector of the Realm\",\"Regent\"],\"aliases\":[\"Ned\",\"The Ned\",\"The Quiet Wolf\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"https://www.anapioficeandfire.com/api/characters/232\",\"allegiances\":\"House Stark of Winterfell\",\"books\":[\"A Clash of Kings\",\"A Storm of Swords\",\"A Feast for Crows\",\"A Dance with Dragons\",\"The World of Ice and Fire\"],\"povBooks\":\"A Game of Thrones\",\"tvSeries\":[\"Season 1\",\"Season 6\"],\"playedBy\":[\"Sean Bean\",\"Sebastian Croft\",\"Robert Aramayo\"]},{\"url\":\"https://www.anapioficeandfire.com/api/characters/529\",\"id\":529,\"name\":\"Jaime Lannister\",\"gender\":\"Male\",\"culture\":\"Westerlands\",\"born\":\"In 266 AC, at Casterly Rock\",\"died\":\"\",\"alive\":true,\"titles\":[\"Ser\",\"Lord Commander of the Kingsguard\",\"Warden of the East (formerly)\"],\"aliases\":[\"The Kingslayer\",\"The Lion of Lannister\",\"The Young Lion\",\"Cripple\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Lannister of Casterly Rock\",\"books\":[\"A Game of Thrones\",\"A Clash of Kings\"],\"povBooks\":[\"A Storm of Swords\",\"A Feast for Crows\",\"A Dance with Dragons\"],\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\"],\"playedBy\":\"Nikolaj Coster-Waldau\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/576\",\"id\":576,\"name\":\"Jon Connington\",\"gender\":\"Male\",\"culture\":\"Stormlands\",\"born\":\"In or between 263 AC and 265 AC\",\"died\":\"\",\"alive\":true,\"titles\":[\"Lord of Griffin's Roost\",\"Hand of the King\",\"Hand of the True King\"],\"aliases\":\"Griffthe Mad King's Hand\",\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":[\"House Connington of Griffin's Roost\",\"House Targaryen of King's Landing\"],\"books\":[\"A Storm of Swords\",\"A Feast for Crows\",\"The World of Ice and Fire\"],\"povBooks\":\"A Dance with Dragons\",\"tvSeries\":\"\",\"playedBy\":\"\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/583\",\"id\":583,\"name\":\"Jon Snow\",\"gender\":\"Male\",\"culture\":\"Northmen\",\"born\":\"In 283 AC\",\"died\":\"\",\"alive\":true,\"titles\":\"Lord Commander of the Night's Watch\",\"aliases\":[\"Lord Snow\",\"Ned Stark's Bastard\",\"The Snow of Winterfell\",\"The Crow-Come-Over\",\"The 998th Lord Commander of the Night's Watch\",\"The Bastard of Winterfell\",\"The Black Bastard of the Wall\",\"Lord Crow\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Stark of Winterfell\",\"books\":\"A Feast for Crows\",\"povBooks\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Dance with Dragons\"],\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\"],\"playedBy\":\"Kit Harington\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/60\",\"id\":60,\"name\":\"Aeron Greyjoy\",\"gender\":\"Male\",\"culture\":\"Ironborn\",\"born\":\"In or between 269 AC and 273 AC, at Pyke\",\"died\":\"\",\"alive\":true,\"titles\":[\"Priest of the Drowned God\",\"Captain of the Golden Storm (formerly)\"],\"aliases\":[\"The Damphair\",\"Aeron Damphair\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Greyjoy of Pyke\",\"books\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Dance with Dragons\"],\"povBooks\":\"A Feast for Crows\",\"tvSeries\":\"Season 6\",\"playedBy\":\"Michael Feast\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/605\",\"id\":605,\"name\":\"Kevan Lannister\",\"gender\":\"Male\",\"culture\":\"\",\"born\":\"In 244 AC\",\"died\":\"In 300 AC, at King's Landing\",\"alive\":false,\"titles\":[\"Ser\",\"Master of laws\",\"Lord Regent\",\"Protector of the Realm\"],\"aliases\":\"\",\"father\":\"\",\"mother\":\"\",\"spouse\":\"https://www.anapioficeandfire.com/api/characters/327\",\"allegiances\":\"House Lannister of Casterly Rock\",\"books\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Feast for Crows\"],\"povBooks\":\"A Dance with Dragons\",\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 5\",\"Season 6\"],\"playedBy\":\"Ian Gelder\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/743\",\"id\":743,\"name\":\"Melisandre\",\"gender\":\"Female\",\"culture\":\"Asshai\",\"born\":\"At Unknown\",\"died\":\"\",\"alive\":true,\"titles\":\"\",\"aliases\":[\"The Red Priestess\",\"The Red Woman\",\"The King's Red Shadow\",\"Lady Red\",\"Lot Seven\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":[],\"books\":[\"A Clash of Kings\",\"A Storm of Swords\",\"A Feast for Crows\"],\"povBooks\":\"A Dance with Dragons\",\"tvSeries\":[\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\"],\"playedBy\":\"Carice van Houten\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/751\",\"id\":751,\"name\":\"Merrett Frey\",\"gender\":\"Male\",\"culture\":\"Rivermen\",\"born\":\"In 262 AC\",\"died\":\"In 300 AC, at Near Oldstones\",\"alive\":false,\"titles\":\"\",\"aliases\":\"Merrett Muttonhead\",\"father\":\"\",\"mother\":\"\",\"spouse\":\"https://www.anapioficeandfire.com/api/characters/712\",\"allegiances\":\"House Frey of the Crossing\",\"books\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Feast for Crows\",\"A Dance with Dragons\"],\"povBooks\":\"A Storm of Swords\",\"tvSeries\":\"\",\"playedBy\":\"\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/844\",\"id\":844,\"name\":\"Quentyn Martell\",\"gender\":\"Male\",\"culture\":\"Dornish\",\"born\":\"In 281 AC, at Sunspear, Dorne\",\"died\":\"In 300 AC, at Meereen\",\"alive\":false,\"titles\":\"Prince\",\"aliases\":[\"Frog\",\"Prince Frog\",\"The prince who came too late\",\"The Dragonrider\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Nymeros Martell of Sunspear\",\"books\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Feast for Crows\"],\"povBooks\":\"A Dance with Dragons\",\"tvSeries\":\"\",\"playedBy\":\"\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/954\",\"id\":954,\"name\":\"Samwell Tarly\",\"gender\":\"Male\",\"culture\":\"Andal\",\"born\":\"In 283 AC, at Horn Hill\",\"died\":\"\",\"alive\":true,\"titles\":\"\",\"aliases\":[\"Sam\",\"Ser Piggy\",\"Prince Pork-chop\",\"Lady Piggy\",\"Sam the Slayer\",\"Black Sam\",\"Lord of Ham\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"\",\"allegiances\":\"House Tarly of Horn Hill\",\"books\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Dance with Dragons\"],\"povBooks\":[\"A Storm of Swords\",\"A Feast for Crows\"],\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\"],\"playedBy\":\"John Bradley-West\"},{\"url\":\"https://www.anapioficeandfire.com/api/characters/957\",\"id\":957,\"name\":\"Sansa Stark\",\"gender\":\"Female\",\"culture\":\"Northmen\",\"born\":\"In 286 AC, at Winterfell\",\"died\":\"\",\"alive\":true,\"titles\":\"Princess\",\"aliases\":[\"Little bird\",\"Alayne Stone\",\"Jonquil\"],\"father\":\"\",\"mother\":\"\",\"spouse\":\"https://www.anapioficeandfire.com/api/characters/1052\",\"allegiances\":[\"House Baelish of Harrenhal\",\"House Stark of Winterfell\"],\"books\":\"A Dance with Dragons\",\"povBooks\":[\"A Game of Thrones\",\"A Clash of Kings\",\"A Storm of Swords\",\"A Feast for Crows\"],\"tvSeries\":[\"Season 1\",\"Season 2\",\"Season 3\",\"Season 4\",\"Season 5\",\"Season 6\"],\"playedBy\":\"Sophie Turner\"}],\"options\":{\"mode\":\"tree\",\"modes\":[\"code\",\"form\",\"text\",\"tree\",\"view\"]}},\"evals\":[],\"jsHooks\":[]}\n# Who are the GoT characters?\n\ngot_chars %>% \n  map_chr(\"name\") # to retrieve the elements with name\n\n\n [1] \"Theon Greyjoy\"      \"Tyrion Lannister\"   \"Victarion Greyjoy\" \n [4] \"Will\"               \"Areo Hotah\"         \"Chett\"             \n [7] \"Cressen\"            \"Arianne Martell\"    \"Daenerys Targaryen\"\n[10] \"Davos Seaworth\"     \"Arya Stark\"         \"Arys Oakheart\"     \n[13] \"Asha Greyjoy\"       \"Barristan Selmy\"    \"Varamyr\"           \n[16] \"Brandon Stark\"      \"Brienne of Tarth\"   \"Catelyn Stark\"     \n[19] \"Cersei Lannister\"   \"Eddard Stark\"       \"Jaime Lannister\"   \n[22] \"Jon Connington\"     \"Jon Snow\"           \"Aeron Greyjoy\"     \n[25] \"Kevan Lannister\"    \"Melisandre\"         \"Merrett Frey\"      \n[28] \"Quentyn Martell\"    \"Samwell Tarly\"      \"Sansa Stark\"       \n\nLearning points\nI think I am still swimming in this package.. The basic idea of using the map functions is to try it on one element and then map it, but I think I need more practice…\nReference\nhttps://r4ds.had.co.nz/\nhttps://jrnold.github.io/r4ds-exercise-solutions/\nhttps://www.r-bloggers.com/2015/12/how-to-write-the-first-for-loop-in-r/\nhttp://www.rebeccabarter.com/blog/2019-08-19_purrr/\nhttps://jennybc.github.io/purrr-tutorial/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-26T20:53:03+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210526_packages /",
    "title": "Packages I use and books I read",
    "description": "The packages I use often and useful reference books",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-26",
    "categories": [],
    "contents": "\nUseful packages for importing data\ndatapasta\nUseful packages for data exploration\nlistviewer (to view lists)\nskimr\nUseful packages for data wrangling\njanitor: to clean data names\ntidyverse\nlubridate\nstringr\nUseful packages for data visualization\npatchwork https://www.business-science.io/code-tools/2021/05/11/patchwork.html\nggthemes\nggsci\nggrepel\nggstatsplot https://indrajeetpatil.github.io/ggstatsplot/\nggfortify\nggstance\nbbplot (for bbc style graphics, https://bbc.github.io/rcookbook/)\ngridExtra\nplotly\nleaflet\ngganimate (animated ggplots)\ngrafify\nsuperheat (create heatmaps)\nUseful packages for modelling\ninteractions\njtools\nUseful packages for statistical inference\nrstatix (pipe friendly framework for basic statistical tests) https://github.com/kassambara/rstatix\nUseful packages for Design of Experiment\nAlgDesign\nDoE.base\npid (Process Improvement using Data, parato Plot)\nFrF2\nOthers\ndistill\nkableExtra\nDT https://rstudio.github.io/DT/\npacman (to load libraries quickly)\ntidytuesdayR (for practice datasets updated weekly) https://github.com/rfordatascience/tidytuesday\nUseful Reference Books\nR\nR Packages https://r-pkgs.org/\nR packages primer https://kbroman.org/pkg_primer/\nData Visualization\nhttps://informationisbeautiful.net/visualizations/dataviz-books/\nDesign of Experiment\nDesign and Analysis of Experiment with R\nMachine Learning\nFeature engineering and selection - a practical approach\nApplied Predictive Modelling\nChemometrics\nChemometrics with R\nChemometrics in Food Science\nNear-Infrared Spectroscopy: Theory, Spectral Analysis, Instrumentation, and Applications\n\n\n\n",
    "preview": {},
    "last_modified": "2021-07-26T13:52:44+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210526_Tidyverse Chap 16 - Vectors/",
    "title": "Vectors",
    "description": "R4DS 16 - Vectors",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-26",
    "categories": [],
    "contents": "\nR4DS Practice 16: Vectors\nThe codes below are from the practice exercises in https://r4ds.had.co.nz/, and are taken with reference from: https://jrnold.github.io/r4ds-exercise-solutions/\nLet’s begin now\nLoading tidyverse package.\n\n\nlibrary(tidyverse)\n\n\n\nVector Basics\nThere are two types of vectors:\natomic vectors - logical, integer, double, character, complex, raw\nlists, also known as recursive vectors because lists can contain other lists\nNaming vectors, so that they can be used for subsetting:\n\n\n# use the set_names function\npurrr::set_names(1:3, nm = letters[1:3])\n\n\na b c \n1 2 3 \n\ndplyr::filter() only works for tibbles.\nSubsetting\n\n\nx <- c(\"one\", \"two\", \"three\", \"four\", \"five\")\n\n# subset with square brackets\nx[c(3,2,5)]\n\n\n[1] \"three\" \"two\"   \"five\" \n\nx[c(2,2,4,4,5)]\n\n\n[1] \"two\"  \"two\"  \"four\" \"four\" \"five\"\n\n# subsetting with named vectors\n\na <- set_names(1:10, # vectors \n               letters[1:10]) # names\na\n\n\n a  b  c  d  e  f  g  h  i  j \n 1  2  3  4  5  6  7  8  9 10 \n\n# specify what to be subsetted\na[c(\"d\", \"e\", \"j\")]\n\n\n d  e  j \n 4  5 10 \n\nCreate a function that takes a vector as input and returns the last value.\n\n\n# try first\n\nx <- c(seq(from = 1, to = 10, by = 2))\nx\n\n\n[1] 1 3 5 7 9\n\nlength(x) # 5\n\n\n[1] 5\n\nx[[length(x)]] # returns the last value by subsetting it out as single element\n\n\n[1] 9\n\n# put as function\nlast_value <- function(x) {\n  \n  # check for case with no length\n  if(length(x)) {\n    \n    x[[length(x)]]\n  }\n  \n  else {\n    x\n  }\n}\n\n\nlast_value(x)\n\n\n[1] 9\n\nCreate a function that take a vector as input and returns the elements at even numbered positions\n\n\nx\n\n\n[1] 1 3 5 7 9\n\n# put as function\neven_indices <- function(x) {\n  \n  # check for case with no length\n  if(length(x)) {\n    \n    x[seq_along(x) %% 2 == 0]\n  }\n  \n  else {\n    x\n  }\n}\n\n\neven_indices(x)\n\n\n[1] 3 7\n\nLists\nLists can contain other lists. Lists can contain a mixture of objects.\nSubsetting lists\n\n\n# Create a list\n\na <- list(a = 1:3,\n          b = \"a string\", \n          c = pi,\n          d  = list(-1, -5))\n\na\n\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a string\"\n\n$c\n[1] 3.141593\n\n$d\n$d[[1]]\n[1] -1\n\n$d[[2]]\n[1] -5\n\n# subset a list using [ ]\n\na[1:2] # first two items of the list\n\n\n$a\n[1] 1 2 3\n\n$b\n[1] \"a string\"\n\n# subset a single component of the list using [[ ]]\n\nstr(a[[4]])  # goes into the list item 4\n\n\nList of 2\n $ : num -1\n $ : num -5\n\na[[4]][1]  # list item 4, first number\n\n\n[[1]]\n[1] -1\n\n# using $ to extract names elements of the list\n\na$a\n\n\n[1] 1 2 3\n\nLearning points\nThe main takeaway that I had from this chapter is how to name vectors to subset them (using [ ]), and how to use different ways to subset lists (using [ ], [[ ]], $)\nReference\nhttps://r4ds.had.co.nz/\nhttps://jrnold.github.io/r4ds-exercise-solutions/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-26T13:01:28+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210525_cheatsheet/",
    "title": "Cheatsheets",
    "description": "The package to download all cheatsheets",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-25",
    "categories": [],
    "contents": "\nThis article was first read from [https://technistema.com/posts/announcing-the-cheatsheet-package/]\nThe package\nThere is this cheatsheet package that allows you to download all the cheatsheets with one single command.\n\n\nlibrary(cheatsheet)\n\n\n\nSet your working directory and download into the folder\n\n\nget_all_cheatsheets(local_path = \"~/Desktop/cheatsheets\", tidyverse_only = TRUE) # can also set to FALSE\n\n\n\nReference\nhttps://technistema.com/posts/announcing-the-cheatsheet-package/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-25T21:38:32+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210525_Tidyverse Chap 15 - Functions/",
    "title": "Functions",
    "description": "R4DS 15 - Functions",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-25",
    "categories": [],
    "contents": "\nR4DS Practice 15: Functions\nThe codes below are from the practice exercises in https://r4ds.had.co.nz/, and are taken with reference from: https://jrnold.github.io/r4ds-exercise-solutions/\nLet’s begin now\nLoading tidyverse package.\n\n\nlibrary(tidyverse)\nlibrary(tibble)\n\n\n\nIntroduction\nWhy are functions important?\nThey help to automate common tasks, rather than copying and pasting, thus minimising human error.\nWhen should you write a function?\nWhen you need to copy and paste anything more than … twice..\n\n\n# rnorm - random generation for normal distribution\n\ndf <- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf\n\n\n# A tibble: 10 x 4\n        a      b       c       d\n    <dbl>  <dbl>   <dbl>   <dbl>\n 1  2.33  -0.557  0.889  -1.45  \n 2 -0.650  0.272 -1.92   -0.0814\n 3 -0.647  0.299  1.10    0.243 \n 4  0.586  0.373  0.175   0.845 \n 5 -0.953 -1.04  -0.410  -1.86  \n 6 -0.293 -1.52   0.888  -1.29  \n 7  0.654  0.169 -0.766  -0.775 \n 8  0.980 -0.423 -0.280   0.292 \n 9 -1.96   0.162  0.0332  1.08  \n10 -0.182 -0.454 -0.667   0.753 \n\n# Manual coding\n\ndf$a <- (df$a - min(df$a, na.rm = T)/\n           (max(df$a, na.rm = T)) - min(df$a, na.rm = T))\ndf$b <- (df$b - min(df$b, na.rm = T)/\n           (max(df$b, na.rm = T)) - min(df$b, na.rm = T))\ndf$c <- (df$c - min(df$c, na.rm = T)/\n           (max(df$c, na.rm = T)) - min(df$c, na.rm = T))\ndf$d <- (df$d - min(df$d, na.rm = T)/\n           (max(df$d, na.rm = T)) - min(df$d, na.rm = T))\n\n# How to reduce copying, pasting, and manual replacing?\n\n# Identify the number of inputs:\n\n# - 1 variable: a numeric vector\n\nx <- df$a\n(x-min(x, na.rm = T)/(max(x, na.rm = T) - min(x, na.rm = T)))\n\n\n [1] 4.9317569 1.9547736 1.9578164 3.1905751 1.6515770 2.3117706\n [7] 3.2592569 3.5847952 0.6455328 2.4224042\n\nrange <- range(x, na.rm = T)\nrange # good practice to give names to intermediate calculations\n\n\n[1] 0.8419688 5.1281929\n\n# After trying out with a simple input, \n# Now you can turn it into a function:\n\n# a. identify the name of the function\n\n# b. list the inputs: function (input variable)\n\n# c: place the code into the body of the function\n\nrescale01 <- function(x) {\n  range <- range(x, na.rm = T)\n  (x - range[1])/(range[2] - range[1])\n}\n\nrescale01(c(0,5,10))\n\n\n[1] 0.0 0.5 1.0\n\n# What if there are Inf values?\n\nx <- c(1:10, Inf)\nx\n\n\n [1]   1   2   3   4   5   6   7   8   9  10 Inf\n\nrescale01(x) # error: NaN\n\n\n [1]   0   0   0   0   0   0   0   0   0   0 NaN\n\n# Let's fix the function\nrescale01_inf <- function(x) {\n  range <- range(x, na.rm = T, finite = T)\n  (x - range[1])/(range[2] - range[1])\n}\n\nrescale01_inf(x)\n\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556\n [7] 0.6666667 0.7777778 0.8888889 1.0000000       Inf\n\n# What if you want to map -Inf to 0, and Inf to 1?\n\n  range <- range(x, na.rm = T, finite = T)\n  y <- (x - range[1])/(range[2] - range[1])\n  y[y ==-Inf] <- 0\n  y[y ==Inf] <- 1\n  y\n\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556\n [7] 0.6666667 0.7777778 0.8888889 1.0000000 1.0000000\n\n# put into function\nrescale01_inf_b <- function(x) {\n  range <- range(x, na.rm = T, finite = T)\n  y <- (x - range[1])/(range[2] - range[1])\n  y[y==-Inf] <- 0\n  y[y==Inf] <- 1\n  y\n}\n\nrescale01_inf(x)\n\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556\n [7] 0.6666667 0.7777778 0.8888889 1.0000000       Inf\n\nrescale01_inf_b(x)\n\n\n [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556\n [7] 0.6666667 0.7777778 0.8888889 1.0000000 1.0000000\n\nPractice turning the following code snippets into functions\n\n\n# to calculate the proportion of na values\n\nx <- c(0, 1, 2, NA, 4, NA)\n\nmean(is.na(x)) # number of NA as proportion\n\n\n[1] 0.3333333\n\n# write the function\nprop_na <- function(x) {\n  mean(is.na(x))\n  \n}\n\nprop_na(x)\n\n\n[1] 0.3333333\n\n# to standardize the vector so that it sums to 1\nx/sum(x, na.rm = T)\n\n\n[1] 0.0000000 0.1428571 0.2857143        NA 0.5714286        NA\n\n# write the function\n\nsum_to_one <- function(x, na.rm = F){\n  x/sum(x, na.rm = na.rm)\n  \n}\n\nsum_to_one(1:5)\n\n\n[1] 0.06666667 0.13333333 0.20000000 0.26666667 0.33333333\n\nsum_to_one(c(1:5, NA))\n\n\n[1] NA NA NA NA NA NA\n\nsum_to_one(c(1:5, NA), na.rm = T)\n\n\n[1] 0.06666667 0.13333333 0.20000000 0.26666667 0.33333333         NA\n\n# to calculate the coefficient of variation\n\nsd(x, na.rm = T)/mean(x, na.rm = T)\n\n\n[1] 0.9759001\n\ncalc_coefficent_variation <- function(x, na.rm = F){\n  sd(x, na.rm = na.rm)/ mean(x,na.rm = na.rm)\n  \n}\n\ncalc_coefficent_variation(1:5)\n\n\n[1] 0.5270463\n\nCompute the sample variance\n\n\nvariance <- function(x, na.rm = T){\n  \n  n <- length(x)\n  m <- mean(x, na.rm = T)\n  sq_err = (x - m)^2\n  sum(sq_err)/n-1\n\n}\n\nvar(1:10)\n\n\n[1] 9.166667\n\nCompute the skewness\n\n\nskewness <- function(x, na.rm = F) {\n  n <- length(x)\n  m <- mean(x, na.rm = na.rm)\n  v <- var(x, na.rm = na.rm)\n  sum((x-m)^3 / (n-2)) / v^(3/2)\n  \n}\n\nskewness(c(1,2,5,100))\n\n\n[1] 1.494554\n\nWrite a function: both_na(), that takes two vectors of the same length and returns the number of positions that have an NA in both vectors.\n\n\nx <- c(1:10, NA)\nx\n\n\n [1]  1  2  3  4  5  6  7  8  9 10 NA\n\ny <- c(1:10, NA)\ny\n\n\n [1]  1  2  3  4  5  6  7  8  9 10 NA\n\nsum(is.na(x) & is.na(y))\n\n\n[1] 1\n\n# write the function\n\nboth_na <- function(x, y) {\n  sum(is.na(x) & is.na(y))\n}\n\nboth_na(\n  c(NA, 1,2,4),\n  c(NA, NA, 1, 4)\n)\n\n\n[1] 1\n\nLearning points\nFunctions aren’t as daunting as I thought. It can be simplified into a step-by-step manner. First, know what you want to automate from the function Identify the input variables Try out a code Write a function for the code and give it a proper name Even better, compile it into a package for your future use.\nReference\nhttps://r4ds.had.co.nz/\nhttps://jrnold.github.io/r4ds-exercise-solutions/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-25T21:39:29+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210524_Tidyverse Chap 13 - Lubridate/",
    "title": "Date and Time in R",
    "description": "R4DS 13 - Lubridate",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-24",
    "categories": [],
    "contents": "\nR4DS Practice 13: lubridate\nThe codes below are from the practice exercises in https://r4ds.had.co.nz/, and are taken with reference from: https://jrnold.github.io/r4ds-exercise-solutions/\nLet’s begin now\nLoading tidyverse package.\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(nycflights13)\n\n\n\nCreating Date/Times from Strings\nIdentify the order in which the year, month and day appear in your dates, then arrange y, m , d in the same order.\n\n\nymd(\"2021-05-24\")\n\n\n[1] \"2021-05-24\"\n\n# you can also use unquoted numbers\n\nymd(20210524)\n\n\n[1] \"2021-05-24\"\n\n# you can also add time using h, m, s\n\nymd_hms (\"2021-05-24 20:11:09\", tz = \"Singapore\")\n\n\n[1] \"2021-05-24 20:11:09 +08\"\n\n# Use OlsonNames() to display timezone names\n\n\n\nCreating Dates from Individual Components\nSometimes, you have the individual components instead of teh date-time spread across multiple columns.\n\n\nflights %>% \n  select(year, month, day, hour, minute)\n\n\n# A tibble: 336,776 x 5\n    year month   day  hour minute\n   <int> <int> <int> <dbl>  <dbl>\n 1  2013     1     1     5     15\n 2  2013     1     1     5     29\n 3  2013     1     1     5     40\n 4  2013     1     1     5     45\n 5  2013     1     1     6      0\n 6  2013     1     1     5     58\n 7  2013     1     1     6      0\n 8  2013     1     1     6      0\n 9  2013     1     1     6      0\n10  2013     1     1     6      0\n# … with 336,766 more rows\n\n# To create date/time:\n# use make_date()\n# or make_datetime()\n\nflights %>% \n  select(year, month, day, hour, minute) %>% \n  mutate(departure = make_datetime(year, month, day, hour, minute))\n\n\n# A tibble: 336,776 x 6\n    year month   day  hour minute departure          \n   <int> <int> <int> <dbl>  <dbl> <dttm>             \n 1  2013     1     1     5     15 2013-01-01 05:15:00\n 2  2013     1     1     5     29 2013-01-01 05:29:00\n 3  2013     1     1     5     40 2013-01-01 05:40:00\n 4  2013     1     1     5     45 2013-01-01 05:45:00\n 5  2013     1     1     6      0 2013-01-01 06:00:00\n 6  2013     1     1     5     58 2013-01-01 05:58:00\n 7  2013     1     1     6      0 2013-01-01 06:00:00\n 8  2013     1     1     6      0 2013-01-01 06:00:00\n 9  2013     1     1     6      0 2013-01-01 06:00:00\n10  2013     1     1     6      0 2013-01-01 06:00:00\n# … with 336,766 more rows\n\nUse the appropriate lubridate function to parse each of the following dates:\n\n\nd1 <- \"January 1, 2010\"\nmdy(d1)\n\n\n[1] \"2010-01-01\"\n\nd2 <- \"2015-Mar-07\"\nymd(d2)\n\n\n[1] \"2015-03-07\"\n\nd3 <- \"06-Jun-2017\"\ndmy(d3)\n\n\n[1] \"2017-06-06\"\n\nd4 <- c(\"August 19 (2015)\", \"July 1 (2015)\")\nmdy(d4)\n\n\n[1] \"2015-08-19\" \"2015-07-01\"\n\nd5 <- c(\"12/30/14\") # Dec 30, 2014\nmdy(d5)\n\n\n[1] \"2014-12-30\"\n\nReference\nhttps://r4ds.had.co.nz/\nhttps://jrnold.github.io/r4ds-exercise-solutions/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-05-25T19:51:52+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210514_Tidyverse Chap 9 - Tidy Data/",
    "title": "Tidy Data",
    "description": "R4DS 09 - Tidy Data with tidyr",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-17",
    "categories": [],
    "contents": "\nR4DS Practice 09: Tidyr\nThe codes below are from the practice exercises in https://r4ds.had.co.nz/, and are taken with reference from: https://jrnold.github.io/r4ds-exercise-solutions/\nLet’s begin now\nLoading tidyverse package.\n\n\nlibrary(tidyverse)\n\n\n\nCompute the rate for table two.\n\n\ntable2\n\n\n# A tibble: 12 x 4\n   country      year type            count\n   <chr>       <int> <chr>           <int>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\n# Extract the number of TB cases per country per year.\n\ntable2_cases <- table2 %>% \n  filter(type == \"cases\") %>% \n  dplyr::rename(cases = count) %>% \n  arrange(country, year)\n\ntable2_population <- table2 %>% \n  filter(type == \"population\") %>% \n  dplyr::rename(population = count) %>% \n  arrange(country, year)\n\n# create a new dataframe with the population and cases columns and calculate the cases per capita\n\nt2_cases_per_cap <- tibble(\n  year = table2_cases$year,\n  country = table2_cases$country,\n  cases = table2_cases$cases,\n  population = table2_population$population\n) %>% \n  mutate(cases_per_cap = cases/population* 10000) %>% \n  select(country, year, cases_per_cap) %>% \n  mutate(type = \"cases_per_cap\") %>% \n  dplyr::rename(count = cases_per_cap)\n\nt2_cases_per_cap\n\n\n# A tibble: 6 x 4\n  country      year count type         \n  <chr>       <int> <dbl> <chr>        \n1 Afghanistan  1999 0.373 cases_per_cap\n2 Afghanistan  2000 1.29  cases_per_cap\n3 Brazil       1999 2.19  cases_per_cap\n4 Brazil       2000 4.61  cases_per_cap\n5 China        1999 1.67  cases_per_cap\n6 China        2000 1.67  cases_per_cap\n\nbind_rows(table2, t2_cases_per_cap) %>% \n  arrange(country, year, type, count)\n\n\n# A tibble: 18 x 4\n   country      year type            count\n   <chr>       <int> <chr>           <dbl>\n 1 Afghanistan  1999 cases         7.45e+2\n 2 Afghanistan  1999 cases_per_cap 3.73e-1\n 3 Afghanistan  1999 population    2.00e+7\n 4 Afghanistan  2000 cases         2.67e+3\n 5 Afghanistan  2000 cases_per_cap 1.29e+0\n 6 Afghanistan  2000 population    2.06e+7\n 7 Brazil       1999 cases         3.77e+4\n 8 Brazil       1999 cases_per_cap 2.19e+0\n 9 Brazil       1999 population    1.72e+8\n10 Brazil       2000 cases         8.05e+4\n11 Brazil       2000 cases_per_cap 4.61e+0\n12 Brazil       2000 population    1.75e+8\n13 China        1999 cases         2.12e+5\n14 China        1999 cases_per_cap 1.67e+0\n15 China        1999 population    1.27e+9\n16 China        2000 cases         2.14e+5\n17 China        2000 cases_per_cap 1.67e+0\n18 China        2000 population    1.28e+9\n\nRepeat for table4a and table 4b\n\n\ntable4a\n\n\n# A tibble: 3 x 3\n  country     `1999` `2000`\n* <chr>        <int>  <int>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntable4b\n\n\n# A tibble: 3 x 3\n  country         `1999`     `2000`\n* <chr>            <int>      <int>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\ntable4c <- tibble(\n  country = table4a$country,\n  `1999` = table4a$`1999`/table4b$`1999`*10000,\n  `2000` = table4a$`2000`/table4b$`2000`*10000\n  \n)\n\ntable4c\n\n\n# A tibble: 3 x 3\n  country     `1999` `2000`\n  <chr>        <dbl>  <dbl>\n1 Afghanistan  0.373   1.29\n2 Brazil       2.19    4.61\n3 China        1.67    1.67\n\nPlot the number of cases for different years for different countries using table 2.\n\n\ntable2 %>% \n  dplyr::filter(type == \"cases\") %>% \n  ggplot(aes(year, count, group = country)) +\n  geom_line(aes(group = country)) +\n  geom_point(aes(group = country, col = country)) +\n  labs(x = \"year\", y = \"cases\") +\n  scale_x_continuous(breaks = unique(table2$year)) +\n  theme_classic()\n\n\n\n\nPivoting\n\n\ntable4a\n\n\n# A tibble: 3 x 3\n  country     `1999` `2000`\n* <chr>        <int>  <int>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\ntidy4a <- table4a %>% \n  pivot_longer(c(`1999`, `2000`),\n               names_to = \"year\",\n               values_to = \"cases\")\ntidy4a\n\n\n# A tibble: 6 x 3\n  country     year   cases\n  <chr>       <chr>  <int>\n1 Afghanistan 1999     745\n2 Afghanistan 2000    2666\n3 Brazil      1999   37737\n4 Brazil      2000   80488\n5 China       1999  212258\n6 China       2000  213766\n\ntable4b\n\n\n# A tibble: 3 x 3\n  country         `1999`     `2000`\n* <chr>            <int>      <int>\n1 Afghanistan   19987071   20595360\n2 Brazil       172006362  174504898\n3 China       1272915272 1280428583\n\ntidy4b <- table4b %>% \n  pivot_longer(c(`1999`, `2000`),\n               names_to = \"year\",\n               values_to = \"population\"\n              )\ntidy4b\n\n\n# A tibble: 6 x 3\n  country     year  population\n  <chr>       <chr>      <int>\n1 Afghanistan 1999    19987071\n2 Afghanistan 2000    20595360\n3 Brazil      1999   172006362\n4 Brazil      2000   174504898\n5 China       1999  1272915272\n6 China       2000  1280428583\n\nWhy are pivot_longer() and pivot_wider() not perfectly symmetrical?\n\n\nstocks <- tibble(\n  year = c(2015, 2015, 2016, 2016),\n  half = c( 1, 2, 1, 2),\n  return = c(1.88, 0.59, 0.92, 0.17)\n)\n\nstocks\n\n\n# A tibble: 4 x 3\n   year  half return\n  <dbl> <dbl>  <dbl>\n1  2015     1   1.88\n2  2015     2   0.59\n3  2016     1   0.92\n4  2016     2   0.17\n\nstocks %>% \n  pivot_wider(names_from = year,\n              values_from = return) %>% \n  pivot_longer(`2015`: `2016`,\n               names_to = \"year\",\n               values_to = \"return\")\n\n\n# A tibble: 4 x 3\n   half year  return\n  <dbl> <chr>  <dbl>\n1     1 2015    1.88\n2     1 2016    0.92\n3     2 2015    0.59\n4     2 2016    0.17\n\nYear became a character column instead of dbl. When the data frame is converted from wide to long, the column type information is lost.\n\n\nstocks %>% \n  pivot_wider(names_from = year,\n              values_from = return) %>% \n  pivot_longer(`2015`: `2016`,\n               names_to = \"year\",\n               values_to = \"return\",\n               names_transform = list(year = as.numeric))\n\n\n# A tibble: 4 x 3\n   half  year return\n  <dbl> <dbl>  <dbl>\n1     1  2015   1.88\n2     1  2016   0.92\n3     2  2015   0.59\n4     2  2016   0.17\n\nWhy does widening the table below fail? How would you add a new column to uniquely identify each value?\n\n\npeople <- tribble(\n  \n  ~name,    ~key,    ~value,\n  # # # # # # # # # # # # # \n  \"Phillip\", \"age\", 45,\n  \"Phillip\", \"height\", 186,\n  \"Phillip\", \"age\", 50,\n  \"Jessica\", \"age\", 37,\n  \"Jessica\", \"height\",156\n)\n\npeople %>% \n  group_by(name, key) %>% \n  mutate(obs = row_number()) %>% \n  pivot_wider(names_from = \"name\",\n              values_from = \"value\")\n\n\n# A tibble: 3 x 4\n# Groups:   key [2]\n  key      obs Phillip Jessica\n  <chr>  <int>   <dbl>   <dbl>\n1 age        1      45      37\n2 height     1     186     156\n3 age        2      50      NA\n\nTidy the tibble below:\n\n\npreg <- tribble(\n  ~pregnant, ~male, ~female,\n  ###\n  \"yes\", NA, 10,\n  \"no\", 20, 12\n)\n\npreg %>% \n  pivot_longer(cols = c(male, female),\n               names_to = \"sex\",\n               values_to = \"count\",\n               values_drop_na = T)\n\n\n# A tibble: 3 x 3\n  pregnant sex    count\n  <chr>    <chr>  <dbl>\n1 yes      female    10\n2 no       male      20\n3 no       female    12\n\nSeparate\n\n\ntable3\n\n\n# A tibble: 6 x 3\n  country      year rate             \n* <chr>       <int> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\ntable3 %>% \n  separate(rate, into = c(\"cases\", \"population\"),\n           convert = T)\n\n\n# A tibble: 6 x 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nWhat do extra and fill arguments do in separate()?\n\n\n# separate\ntibble(x = c(\"a,b,c\",  \"d,e,f,g\", \"h,i,j\")) %>% \n  separate(x, c(\"one\" , \"two\", \"three\"),\n           extra = \"drop\") # remove extra g\n\n\n# A tibble: 3 x 3\n  one   two   three\n  <chr> <chr> <chr>\n1 a     b     c    \n2 d     e     f    \n3 h     i     j    \n\ntibble(x = c(\"a,b,c\",  \"d,e,f,g\", \"h,i,j\")) %>% \n  separate(x, c(\"one\" , \"two\", \"three\"),\n           extra = \"merge\") # merge g to f\n\n\n# A tibble: 3 x 3\n  one   two   three\n  <chr> <chr> <chr>\n1 a     b     c    \n2 d     e     f,g  \n3 h     i     j    \n\n\n\n# fill\ntibble(x = c(\"a,b,c\", \"d,e\", \"f,g,i\")) %>% \n  separate(x, c(\"one\", \"two\", \"three\"),\n                fill = \"right\") # fill to right\n\n\n# A tibble: 3 x 3\n  one   two   three\n  <chr> <chr> <chr>\n1 a     b     c    \n2 d     e     <NA> \n3 f     g     i    \n\ntibble(x = c(\"a,b,c\", \"d,e\", \"f,g,i\")) %>% \n  separate(x, c(\"one\", \"two\", \"three\"),\n                fill = \"left\") # fill to left\n\n\n# A tibble: 3 x 3\n  one   two   three\n  <chr> <chr> <chr>\n1 a     b     c    \n2 <NA>  d     e    \n3 f     g     i    \n\nMissing Values\n\n\nstocks <- tibble(\n  year = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),\n  qtr = c( 1,2,3,4,2,3,4),\n  return = c(1.88, 0.59, 0.35,NA, 0.92, 0.17, 2.66)\n)\n\nstocks\n\n\n# A tibble: 7 x 3\n   year   qtr return\n  <dbl> <dbl>  <dbl>\n1  2015     1   1.88\n2  2015     2   0.59\n3  2015     3   0.35\n4  2015     4  NA   \n5  2016     2   0.92\n6  2016     3   0.17\n7  2016     4   2.66\n\nstocks %>% \n  pivot_wider(names_from = year,\n              values_from = return,\n              values_fill = 0)\n\n\n# A tibble: 4 x 3\n    qtr `2015` `2016`\n  <dbl>  <dbl>  <dbl>\n1     1   1.88   0   \n2     2   0.59   0.92\n3     3   0.35   0.17\n4     4  NA      2.66\n\n# replace the missing values with 0 using complete()\nstocks %>% \n  complete(year, qtr, fill = list(return = 0))\n\n\n# A tibble: 8 x 3\n   year   qtr return\n  <dbl> <dbl>  <dbl>\n1  2015     1   1.88\n2  2015     2   0.59\n3  2015     3   0.35\n4  2015     4   0   \n5  2016     1   0   \n6  2016     2   0.92\n7  2016     3   0.17\n8  2016     4   2.66\n\nCase study\n\n\n# TB dataset broken down by year, country, age, gender , diagnosis method.\nwho\n\n\n# A tibble: 7,240 x 60\n   country     iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534\n   <chr>       <chr> <chr> <int>       <int>        <int>        <int>\n 1 Afghanistan AF    AFG    1980          NA           NA           NA\n 2 Afghanistan AF    AFG    1981          NA           NA           NA\n 3 Afghanistan AF    AFG    1982          NA           NA           NA\n 4 Afghanistan AF    AFG    1983          NA           NA           NA\n 5 Afghanistan AF    AFG    1984          NA           NA           NA\n 6 Afghanistan AF    AFG    1985          NA           NA           NA\n 7 Afghanistan AF    AFG    1986          NA           NA           NA\n 8 Afghanistan AF    AFG    1987          NA           NA           NA\n 9 Afghanistan AF    AFG    1988          NA           NA           NA\n10 Afghanistan AF    AFG    1989          NA           NA           NA\n# … with 7,230 more rows, and 53 more variables: new_sp_m3544 <int>,\n#   new_sp_m4554 <int>, new_sp_m5564 <int>, new_sp_m65 <int>,\n#   new_sp_f014 <int>, new_sp_f1524 <int>, new_sp_f2534 <int>,\n#   new_sp_f3544 <int>, new_sp_f4554 <int>, new_sp_f5564 <int>,\n#   new_sp_f65 <int>, new_sn_m014 <int>, new_sn_m1524 <int>,\n#   new_sn_m2534 <int>, new_sn_m3544 <int>, new_sn_m4554 <int>,\n#   new_sn_m5564 <int>, new_sn_m65 <int>, new_sn_f014 <int>,\n#   new_sn_f1524 <int>, new_sn_f2534 <int>, new_sn_f3544 <int>,\n#   new_sn_f4554 <int>, new_sn_f5564 <int>, new_sn_f65 <int>,\n#   new_ep_m014 <int>, new_ep_m1524 <int>, new_ep_m2534 <int>,\n#   new_ep_m3544 <int>, new_ep_m4554 <int>, new_ep_m5564 <int>,\n#   new_ep_m65 <int>, new_ep_f014 <int>, new_ep_f1524 <int>,\n#   new_ep_f2534 <int>, new_ep_f3544 <int>, new_ep_f4554 <int>,\n#   new_ep_f5564 <int>, new_ep_f65 <int>, newrel_m014 <int>,\n#   newrel_m1524 <int>, newrel_m2534 <int>, newrel_m3544 <int>,\n#   newrel_m4554 <int>, newrel_m5564 <int>, newrel_m65 <int>,\n#   newrel_f014 <int>, newrel_f1524 <int>, newrel_f2534 <int>,\n#   newrel_f3544 <int>, newrel_f4554 <int>, newrel_f5564 <int>,\n#   newrel_f65 <int>\n\n# make the dataset into a long shape\n\nwho1 <- who %>% \n  pivot_longer(cols = new_sp_m014:newrel_f65,\n               names_to = \"key\",\n               values_to = \"cases\",\n               values_drop_na = T) %>% \n  mutate(names_from = str_replace(key, \"newrel\", \"new_rel\")) %>% \n  separate(key, c(\"new\", \"type\", \"sexage\"), sep = \"_\") %>% \n  select(-new, -iso2, -iso3) %>% \n  separate(sexage, c(\"sex\", \"age\"), sep =1)\n\nglimpse(who1)\n\n\nRows: 76,046\nColumns: 7\n$ country    <chr> \"Afghanistan\", \"Afghanistan\", \"Afghanistan\", \"Afg…\n$ year       <int> 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1997, 1…\n$ type       <chr> \"sp\", \"sp\", \"sp\", \"sp\", \"sp\", \"sp\", \"sp\", \"sp\", \"…\n$ sex        <chr> \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"m\", \"f\", \"f\", \"f\",…\n$ age        <chr> \"014\", \"1524\", \"2534\", \"3544\", \"4554\", \"5564\", \"6…\n$ cases      <int> 0, 10, 6, 3, 5, 2, 0, 5, 38, 36, 14, 8, 0, 1, 30,…\n$ names_from <chr> \"new_sp_m014\", \"new_sp_m1524\", \"new_sp_m2534\", \"n…\n\nReference\nhttps://r4ds.had.co.nz/\nhttps://jrnold.github.io/r4ds-exercise-solutions/\n\n\n\n",
    "preview": "posts/20210514_Tidyverse Chap 9 - Tidy Data/09---Tidy-Data_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-05-17T14:09:02+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210517_Tidyverse Chap 10 - Relational Data/",
    "title": "Relational Data",
    "description": "R4DS 09 - Relational Data with dplyr",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-17",
    "categories": [],
    "contents": "\nR4DS Practice 10: dplyr\nThe codes below are from the practice exercises in https://r4ds.had.co.nz/, and are taken with reference from: https://jrnold.github.io/r4ds-exercise-solutions/\nLet’s begin now\nLoading tidyverse package.\n\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\n\n\nRelational data\nTypically, there are many tables of data, and you must combine them to answer the questions you are interested in. Collectively, mutiple tables of data are called relational data, becase you are interested in the relations and not just the individual datasets.\nnycflights13\nairlines: look up the full carrier name from its abbreviated code\n\n\nairlines\n\n\n# A tibble: 16 x 2\n   carrier name                       \n   <chr>   <chr>                      \n 1 9E      Endeavor Air Inc.          \n 2 AA      American Airlines Inc.     \n 3 AS      Alaska Airlines Inc.       \n 4 B6      JetBlue Airways            \n 5 DL      Delta Air Lines Inc.       \n 6 EV      ExpressJet Airlines Inc.   \n 7 F9      Frontier Airlines Inc.     \n 8 FL      AirTran Airways Corporation\n 9 HA      Hawaiian Airlines Inc.     \n10 MQ      Envoy Air                  \n11 OO      SkyWest Airlines Inc.      \n12 UA      United Air Lines Inc.      \n13 US      US Airways Inc.            \n14 VX      Virgin America             \n15 WN      Southwest Airlines Co.     \n16 YV      Mesa Airlines Inc.         \n\nairports: gives information about each airport, identified by the faa airport code\n\n\nairports\n\n\n# A tibble: 1,458 x 8\n   faa   name                 lat    lon   alt    tz dst   tzone      \n   <chr> <chr>              <dbl>  <dbl> <dbl> <dbl> <chr> <chr>      \n 1 04G   Lansdowne Airport   41.1  -80.6  1044    -5 A     America/Ne…\n 2 06A   Moton Field Munic…  32.5  -85.7   264    -6 A     America/Ch…\n 3 06C   Schaumburg Region…  42.0  -88.1   801    -6 A     America/Ch…\n 4 06N   Randall Airport     41.4  -74.4   523    -5 A     America/Ne…\n 5 09J   Jekyll Island Air…  31.1  -81.4    11    -5 A     America/Ne…\n 6 0A9   Elizabethton Muni…  36.4  -82.2  1593    -5 A     America/Ne…\n 7 0G6   Williams County A…  41.5  -84.5   730    -5 A     America/Ne…\n 8 0G7   Finger Lakes Regi…  42.9  -76.8   492    -5 A     America/Ne…\n 9 0P2   Shoestring Aviati…  39.8  -76.6  1000    -5 U     America/Ne…\n10 0S9   Jefferson County …  48.1 -123.    108    -8 A     America/Lo…\n# … with 1,448 more rows\n\nplanes: gives information about each plane, identified by its tail number\n\n\nplanes\n\n\n# A tibble: 3,322 x 9\n   tailnum  year type    manufacturer model engines seats speed engine\n   <chr>   <int> <chr>   <chr>        <chr>   <int> <int> <int> <chr> \n 1 N10156   2004 Fixed … EMBRAER      EMB-…       2    55    NA Turbo…\n 2 N102UW   1998 Fixed … AIRBUS INDU… A320…       2   182    NA Turbo…\n 3 N103US   1999 Fixed … AIRBUS INDU… A320…       2   182    NA Turbo…\n 4 N104UW   1999 Fixed … AIRBUS INDU… A320…       2   182    NA Turbo…\n 5 N10575   2002 Fixed … EMBRAER      EMB-…       2    55    NA Turbo…\n 6 N105UW   1999 Fixed … AIRBUS INDU… A320…       2   182    NA Turbo…\n 7 N107US   1999 Fixed … AIRBUS INDU… A320…       2   182    NA Turbo…\n 8 N108UW   1999 Fixed … AIRBUS INDU… A320…       2   182    NA Turbo…\n 9 N109UW   1999 Fixed … AIRBUS INDU… A320…       2   182    NA Turbo…\n10 N110UW   1999 Fixed … AIRBUS INDU… A320…       2   182    NA Turbo…\n# … with 3,312 more rows\n\nweather: gives the weather at each NYC airport for each hour\n\n\nweather\n\n\n# A tibble: 26,115 x 15\n   origin  year month   day  hour  temp  dewp humid wind_dir\n   <chr>  <int> <int> <int> <int> <dbl> <dbl> <dbl>    <dbl>\n 1 EWR     2013     1     1     1  39.0  26.1  59.4      270\n 2 EWR     2013     1     1     2  39.0  27.0  61.6      250\n 3 EWR     2013     1     1     3  39.0  28.0  64.4      240\n 4 EWR     2013     1     1     4  39.9  28.0  62.2      250\n 5 EWR     2013     1     1     5  39.0  28.0  64.4      260\n 6 EWR     2013     1     1     6  37.9  28.0  67.2      240\n 7 EWR     2013     1     1     7  39.0  28.0  64.4      240\n 8 EWR     2013     1     1     8  39.9  28.0  62.2      250\n 9 EWR     2013     1     1     9  39.9  28.0  62.2      260\n10 EWR     2013     1     1    10  41    28.0  59.6      260\n# … with 26,105 more rows, and 6 more variables: wind_speed <dbl>,\n#   wind_gust <dbl>, precip <dbl>, pressure <dbl>, visib <dbl>,\n#   time_hour <dttm>\n\nImagine that you want to draw the route each plane flies from its origin to its destination. What variables would you need? What tables would you need to combine?\n\n\n# require the latitude and longitude of the origin and destination airports of each flight. \n\nglimpse(flights)\n\n\nRows: 336,776\nColumns: 19\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 201…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, …\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, …\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838,…\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846,…\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2,…\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV…\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, …\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EW…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FL…\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 1…\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, …\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, …\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 20…\n\nglimpse(airports) # has lat, lon\n\n\nRows: 1,458\nColumns: 8\n$ faa   <chr> \"04G\", \"06A\", \"06C\", \"06N\", \"09J\", \"0A9\", \"0G6\", \"0G7\"…\n$ name  <chr> \"Lansdowne Airport\", \"Moton Field Municipal Airport\", …\n$ lat   <dbl> 41.13047, 32.46057, 41.98934, 41.43191, 31.07447, 36.3…\n$ lon   <dbl> -80.61958, -85.68003, -88.10124, -74.39156, -81.42778,…\n$ alt   <dbl> 1044, 264, 801, 523, 11, 1593, 730, 492, 1000, 108, 40…\n$ tz    <dbl> -5, -6, -6, -5, -5, -5, -5, -5, -5, -8, -5, -6, -5, -5…\n$ dst   <chr> \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"A\", \"U\", \"A\", \"A\",…\n$ tzone <chr> \"America/New_York\", \"America/Chicago\", \"America/Chicag…\n\nflights_latlon <- flights %>% \n  inner_join(select(airports, origin = faa,\n                    origin_lat = lat,\n                    origin_lon = lon),\n             by = \"origin\") %>% \n  inner_join(select(airports, dest = faa,\n                    dest_lat = lat,\n                    dest_lon = lon),\n             by = \"dest\") \n\nglimpse(flights_latlon)\n\n\nRows: 329,174\nColumns: 23\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 201…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ dep_time       <int> 517, 533, 542, 554, 554, 555, 557, 557, 558, …\n$ sched_dep_time <int> 515, 529, 540, 600, 558, 600, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -6, -4, -5, -3, -3, -2, -2, -2, -2, …\n$ arr_time       <int> 830, 850, 923, 812, 740, 913, 709, 838, 753, …\n$ sched_arr_time <int> 819, 830, 850, 837, 728, 854, 723, 846, 745, …\n$ arr_delay      <dbl> 11, 20, 33, -25, 12, 19, -14, -8, 8, -2, -3, …\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"DL\", \"UA\", \"B6\", \"EV\", \"B6…\n$ flight         <int> 1545, 1714, 1141, 461, 1696, 507, 5708, 79, 3…\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N668DN\", \"N394…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"LGA\", \"EWR\", \"EWR\", \"LG…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"ATL\", \"ORD\", \"FLL\", \"IA…\n$ air_time       <dbl> 227, 227, 160, 116, 150, 158, 53, 140, 138, 1…\n$ distance       <dbl> 1400, 1416, 1089, 762, 719, 1065, 229, 944, 7…\n$ hour           <dbl> 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, …\n$ minute         <dbl> 15, 29, 40, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 20…\n$ origin_lat     <dbl> 40.69250, 40.77725, 40.63975, 40.77725, 40.69…\n$ origin_lon     <dbl> -74.16867, -73.87261, -73.77893, -73.87261, -…\n$ dest_lat       <dbl> 29.98443, 29.98443, 25.79325, 33.63672, 41.97…\n$ dest_lon       <dbl> -95.34144, -95.34144, -80.29056, -84.42807, -…\n\n# first 100 flights\n\nflights_latlon %>% \n  slice(1:100) %>% \n  ggplot(aes(\n    x = origin_lon, xend = dest_lon,\n    y = origin_lat, yend = dest_lat\n  )) +\n  borders(\"state\") +\n  geom_segment(arrow = arrow(length = unit(0.1, \"cm\"))) +\n  coord_quickmap() +\n  labs( y = \"Lat\",\n        x = \"Lon\")\n\n\n\n\nKeys\nAdd a surrogate key to flights\n\n\nflights\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nflights %>% \n  mutate(flight_id = row_number()) %>% \n  glimpse()\n\n\nRows: 336,776\nColumns: 20\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 201…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, …\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, …\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838,…\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846,…\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2,…\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV…\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, …\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EW…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FL…\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 1…\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, …\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, …\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 20…\n$ flight_id      <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14…\n\nMutating Joins\nCompute the average delay by destination, then join on the airports data frame so that you can show the spatial distribution of delays.\n\n\navg_dest_delays <- flights %>% \n  group_by(dest) %>% \n  summarise(delay = mean(arr_delay, na.rm = T)) %>% \n  inner_join(airports, by = c(dest = \"faa\")) %>% \n  ggplot(aes(lon, lat, col = delay)) +\n  borders(\"state\") +\n  geom_point() +\n  coord_quickmap() +\n  scale_color_viridis_c() +\n  theme_classic()\n\navg_dest_delays \n\n\n\n\nAdd the location of the origin and destination to flights\n\n\nairport_locations <- airports %>% \n  select(faa, lat, lon)\n\nflights %>% \n  select(year:day, hour, origin, dest) %>% \n  left_join(airport_locations, by = c(\"origin\" = \"faa\")) %>% \n  left_join(airport_locations, by = c(\"dest\" = \"faa\"),\n            suffix = c(\"_origin\", \"_dest\"))\n\n\n# A tibble: 336,776 x 10\n    year month   day  hour origin dest  lat_origin lon_origin lat_dest\n   <int> <int> <int> <dbl> <chr>  <chr>      <dbl>      <dbl>    <dbl>\n 1  2013     1     1     5 EWR    IAH         40.7      -74.2     30.0\n 2  2013     1     1     5 LGA    IAH         40.8      -73.9     30.0\n 3  2013     1     1     5 JFK    MIA         40.6      -73.8     25.8\n 4  2013     1     1     5 JFK    BQN         40.6      -73.8     NA  \n 5  2013     1     1     6 LGA    ATL         40.8      -73.9     33.6\n 6  2013     1     1     5 EWR    ORD         40.7      -74.2     42.0\n 7  2013     1     1     6 EWR    FLL         40.7      -74.2     26.1\n 8  2013     1     1     6 LGA    IAD         40.8      -73.9     38.9\n 9  2013     1     1     6 JFK    MCO         40.6      -73.8     28.4\n10  2013     1     1     6 LGA    ORD         40.8      -73.9     42.0\n# … with 336,766 more rows, and 1 more variable: lon_dest <dbl>\n\nIs there a relationship between the age of a plane and its delays?\n\n\n# merge flights with planes (which contains plane years)\n# calculate the average departure delay for each age of flight\n\nplane_cohorts_dep <- inner_join(flights,\n                            select(planes, tailnum, plane_year = year),\n                            by = \"tailnum\") %>%\n  mutate(age = year - plane_year) %>% \n  filter(!is.na(age)) %>% \n  mutate(age = if_else(age>25, 25L, age)) %>% \n  group_by(age) %>% \n  summarise(dep_delay_mean = mean(dep_delay, na.rm = T),\n            dep_delay_sd = sd(dep_delay, na.rm = T),\n            n_dep_delay = sum(!is.na(dep_delay))) %>% \n  ggplot(aes(x = age, y = dep_delay_mean)) +\n  geom_point() +\n  scale_x_continuous(\"Age of plane(years)\", breaks = seq(0,30, by = 10)) +\n  scale_y_continuous(\"Mean Dep Delays (min)\") +\n  labs(subtitle = \"Departure delay increases with age of plane until 10 years, then it declines and flattens out.\",\n       title = \"Relationship between Departure Delay and Age of Plane\") +\n  theme_classic()\n\n\n\nplane_cohorts_arr <- inner_join(flights,\n                            select(planes, tailnum, plane_year = year),\n                            by = \"tailnum\") %>%\n  mutate(age = year - plane_year) %>% \n  filter(!is.na(age)) %>% \n  mutate(age = if_else(age>25, 25L, age)) %>% \n  group_by(age) %>% \n  summarise(\n            arr_delay_mean = mean(arr_delay, na.rm = T),\n            arr_delay_sd = sd(arr_delay, na.rm = T),\n            n_arr_delay = sum(!is.na(arr_delay))) %>% \n  ggplot(aes(x = age, y = arr_delay_mean)) +\n  geom_point() +\n  scale_x_continuous(\"Age of plane(years)\", breaks = seq(0,30, by = 10)) +\n  scale_y_continuous(\"Mean Arr Delays (min)\") +\n  labs(subtitle = \"Arr delay increases with age of plane until 10 years, then it declines and flattens out.\",\n       title = \"Relationship between Arr Delay and Age of Plane\") +\n  theme_classic()\n\ngridExtra::grid.arrange(plane_cohorts_dep, plane_cohorts_arr, nrow = 2) \n\n\n\n\nWhat weather conditions make it more likely to see a delay?\n\n\nflight_weather <- flights %>% \n  inner_join(weather, by = c(\n    \"origin\" = \"origin\",\n    \"year\" = \"year\",\n    \"month\" = \"month\",\n    \"day\" = \"day\", \n    \"hour\" = \"hour\"\n  ))\n\nflight_weather %>% \n  mutate(visib_cat = cut_interval(visib, n = 10)) %>% \n  group_by(visib_cat) %>% \n  summarise(dep_delay = mean(dep_delay, na.rm = T)) %>% \n  ggplot(aes(x = visib_cat, y = dep_delay)) +\n  geom_point() +\n  labs(title = \"Relationship beween visibility and delay times\",\n       subtitle = \"A decrease in visibility increases delay timings\") +\n  theme_classic()\n\n\n\n\nFiltering Joins\nFilter flights to only show flights with planes that have flown at least 100 flights.\n\n\nflights_gte100 <- flights %>% \n  filter(!is.na(tailnum)) %>% \n  group_by(tailnum) %>% \n  count() %>% \n  filter(n >=100)\n\nflights %>% \n  semi_join(flights_gte100, by = \"tailnum\")\n\n\n# A tibble: 228,390 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      544            545        -1     1004\n 4  2013     1     1      554            558        -4      740\n 5  2013     1     1      555            600        -5      913\n 6  2013     1     1      557            600        -3      709\n 7  2013     1     1      557            600        -3      838\n 8  2013     1     1      558            600        -2      849\n 9  2013     1     1      558            600        -2      853\n10  2013     1     1      558            600        -2      923\n# … with 228,380 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nCombine fueleconomy::vehicles and fueleconomy::common to find only records for the most common models\n\n\nfueleconomy::vehicles %>% \n  semi_join(fueleconomy::common, by = c(\"make\", \"model\"))\n\n\n# A tibble: 14,531 x 12\n      id make  model   year class  trans drive   cyl displ fuel    hwy\n   <dbl> <chr> <chr>  <dbl> <chr>  <chr> <chr> <dbl> <dbl> <chr> <dbl>\n 1  1833 Acura Integ…  1986 Subco… Auto… Fron…     4   1.6 Regu…    28\n 2  1834 Acura Integ…  1986 Subco… Manu… Fron…     4   1.6 Regu…    28\n 3  3037 Acura Integ…  1987 Subco… Auto… Fron…     4   1.6 Regu…    28\n 4  3038 Acura Integ…  1987 Subco… Manu… Fron…     4   1.6 Regu…    28\n 5  4183 Acura Integ…  1988 Subco… Auto… Fron…     4   1.6 Regu…    27\n 6  4184 Acura Integ…  1988 Subco… Manu… Fron…     4   1.6 Regu…    28\n 7  5303 Acura Integ…  1989 Subco… Auto… Fron…     4   1.6 Regu…    27\n 8  5304 Acura Integ…  1989 Subco… Manu… Fron…     4   1.6 Regu…    28\n 9  6442 Acura Integ…  1990 Subco… Auto… Fron…     4   1.8 Regu…    24\n10  6443 Acura Integ…  1990 Subco… Manu… Fron…     4   1.8 Regu…    26\n# … with 14,521 more rows, and 1 more variable: cty <dbl>\n\nReference\nhttps://r4ds.had.co.nz/\nhttps://jrnold.github.io/r4ds-exercise-solutions/\n\n\n\n",
    "preview": "posts/20210517_Tidyverse Chap 10 - Relational Data/10---Relational-Data_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-05-17T15:19:35+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210512_Tidyverse Chap 8 - Import/",
    "title": "Data Import",
    "description": "R4DS 08 - Data Import with readr",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-14",
    "categories": [],
    "contents": "\nR4DS Practice 08: Data Import\nThe codes below are from the practice exercises in https://r4ds.had.co.nz/, and are taken with reference from: https://jrnold.github.io/r4ds-exercise-solutions/\nLet’s begin now\nLoading tidyverse package.\n\n\nlibrary(tidyverse)\n\n\n\nIntroduction\nThis chapter is about importing data (plain text rectangular files) into R.\nA cheatsheet is available at: https://readr.tidyverse.org/\nImporting in csv file:\nIt is important to define the path to the file to read. The data below is a dataset from tidytuesday, and the data was scrapped from fastfoodnutrition.org.\n\n\n# Importing Fast Food Data from\n\ncalories <- read_csv(\"https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-09-04/fastfood_calories.csv\")\n\nglimpse(calories)\n\n\nRows: 515\nColumns: 18\n$ X1          <dbl> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 1…\n$ restaurant  <chr> \"Mcdonalds\", \"Mcdonalds\", \"Mcdonalds\", \"Mcdonald…\n$ item        <chr> \"Artisan Grilled Chicken Sandwich\", \"Single Baco…\n$ calories    <dbl> 380, 840, 1130, 750, 920, 540, 300, 510, 430, 77…\n$ cal_fat     <dbl> 60, 410, 600, 280, 410, 250, 100, 210, 190, 400,…\n$ total_fat   <dbl> 7, 45, 67, 31, 45, 28, 12, 24, 21, 45, 18, 34, 2…\n$ sat_fat     <dbl> 2.0, 17.0, 27.0, 10.0, 12.0, 10.0, 5.0, 4.0, 11.…\n$ trans_fat   <dbl> 0.0, 1.5, 3.0, 0.5, 0.5, 1.0, 0.5, 0.0, 1.0, 2.5…\n$ cholesterol <dbl> 95, 130, 220, 155, 120, 80, 40, 65, 85, 175, 40,…\n$ sodium      <dbl> 1110, 1580, 1920, 1940, 1980, 950, 680, 1040, 10…\n$ total_carb  <dbl> 44, 62, 63, 62, 81, 46, 33, 49, 35, 42, 38, 48, …\n$ fiber       <dbl> 3, 2, 3, 2, 4, 3, 2, 3, 2, 3, 2, 3, 3, 5, 2, 2, …\n$ sugar       <dbl> 11, 18, 18, 18, 18, 9, 7, 6, 7, 10, 5, 11, 11, 1…\n$ protein     <dbl> 37, 46, 70, 55, 46, 25, 15, 25, 25, 51, 15, 32, …\n$ vit_a       <dbl> 4, 6, 10, 6, 6, 10, 10, 0, 20, 20, 2, 10, 10, 10…\n$ vit_c       <dbl> 20, 20, 20, 25, 20, 2, 2, 4, 4, 6, 0, 10, 20, 15…\n$ calcium     <dbl> 20, 20, 50, 20, 20, 15, 10, 2, 15, 20, 15, 35, 3…\n$ salad       <chr> \"Other\", \"Other\", \"Other\", \"Other\", \"Other\", \"Ot…\n\nTo read a file where the fields are separated by “|”, use the read_delim() function, and specify the delimiter.\nFor read_csv(), you can also use trim_ws to trim the whitespace before and after cells, indicate the locale, specify what to do with NA, and show the progress bar if needed.\nTo read the following:\n\n\nx <- \"x, y\\n1, `a, b`\"\nread_csv(x, quote = \"`\")\n\n\n# A tibble: 1 x 2\n      x y    \n  <dbl> <chr>\n1     1 a, b \n\nIdentify what is wrong with each of the following inline CSV files:\n\n\nread_csv(\"a, b\\n1,2,3\\n4,5,6\")\n\n\n# A tibble: 2 x 2\n      a     b\n  <dbl> <dbl>\n1     1     2\n2     4     5\n\n# only 2 columns have headers specified but there should be three column headers\n\n\n\n\n\nread_csv(\"a,b,c\\n1,2\\n1,2,3,4\")\n\n\n# A tibble: 2 x 3\n      a     b     c\n  <dbl> <dbl> <dbl>\n1     1     2    NA\n2     1     2     3\n\n# the number of columns in data do not match number of columns in headers\n\n\n\n\n\nread_csv(\"a,b\\n\\\"1\")\n\n\n# A tibble: 1 x 2\n      a b    \n  <dbl> <chr>\n1     1 <NA> \n\n# the number of quotation marks are not correct\n\n\n\n\n\nread_csv(\"a,b\\n1,2\\na,b\")\n\n\n# A tibble: 2 x 2\n  a     b    \n  <chr> <chr>\n1 1     2    \n2 a     b    \n\n# col a had a value of a?\n\n\n\n\n\nread_csv(\"a;b\\n1;3\") # delimiter is ;, use read_csv2\n\n\n# A tibble: 1 x 1\n  `a;b`\n  <chr>\n1 1;3  \n\nread_csv2(\"a;b\\n1;3\")\n\n\n# A tibble: 1 x 2\n      a     b\n  <dbl> <dbl>\n1     1     3\n\nParsing a vector\nThe important arguments to locale are:\nlocale( date_names = “en”, date_format = “%AD”, time_format = “%AT”, decimal_mark = “.”, grouping_mark = “,”, tz = “UTC”, encoding = “UTF-8”, asciify = FALSE )\nArguments:\ndate_names\nCharacter representations of day and month names. Either the language code as string (passed on to date_names_lang()) or an object created by date_names().\ndate_format, time_format\nDefault date and time formats.\ndecimal_mark, grouping_mark Symbols used to indicate the decimal place, and to chunk larger numbers. Decimal mark can only be , or ..\ntz\nDefault tz. This is used both for input (if the time zone isn’t present in individual strings), and for output (to control the default display). The default is to use “UTC”, a time zone that does not use daylight savings time (DST) and hence is typically most useful for data. The absence of time zones makes it approximately 50x faster to generate UTC times than any other time zone.\nUse \"\" to use the system default time zone, but beware that this will not be reproducible across systems.\nFor a complete list of possible time zones, see OlsonNames(). Americans, note that “EST” is a Canadian time zone that does not have DST. It is not Eastern Standard Time. It’s better to use “US/Eastern”, “US/Central” etc.\nencoding\nDefault encoding. This only affects how the file is read - readr always converts the output to UTF-8.\nasciify Should diacritics be stripped from date names and converted to ASCII? This is useful if you’re dealing with ASCII data where the correct spellings have been lost. Requires the stringi package\nGenerate the correct format string to parse each of the following dates and times:\n\n\nd1 <- \"January 1, 2010\"\n\nparse_date(d1, \"%B %d, %Y\")\n\n\n[1] \"2010-01-01\"\n\n#\nd2 <- \"2015-Mar-07\"\n\nparse_date(d2, \"%Y-%b-%d\")\n\n\n[1] \"2015-03-07\"\n\n#\nd3 <- \"06-Jun-2017\"\n\nparse_date(d3, \"%d-%b-%Y\")\n\n\n[1] \"2017-06-06\"\n\n#\nd4 <- c(\"August 19 (2015)\", \"July 1 (2015)\")\nparse_date(d4, \"%B %d (%Y)\")\n\n\n[1] \"2015-08-19\" \"2015-07-01\"\n\n#\nd5 <- \"12/03/14\" # Dec 30, 2014\nparse_date(d5, \"%m/%d/%y\")\n\n\n[1] \"2014-12-03\"\n\n#\nt1 <- \"1705\"\nparse_time(t1, \"%H%M\")\n\n\n17:05:00\n\n#\nt2 <- \"11:15:10.12 PM\"\nparse_time(t2, \"%H:%M:%OS %p\")\n\n\n23:15:10.12\n\ndatapasta\nI learnt about this package called datapaste, which allows you to copy and paste data from excel, Wikipedia, directy into R without saving as a separate excel file. This website gives a good summary on the functions. In short, you just have to select the text, and paste into R. Lovely!\nI copied the data below from the SFA website using the datapaste package, but I had to modify some of the text arrangement as commar was used as a separator for ’000; and added Y to the years so that they will be recognized as text.\n\n\nlibrary(datapasta)\n\nchicken_consumption <- tribble(\n  ~Item, ~Y2010, ~Y2011, ~Y2012, ~Y2013, ~Y2014, ~Y2015, ~Y2016, ~Y2017, ~Y2018, ~Y2019,\n  ## -- ## \n  \"Chicken Consumption (tonnes)\", 161460, 170315, 173521, 172054, 170926, 179122, 195290, 167588, 193142, 193903\n  \n  )\n\nglimpse(chicken_consumption)\n\n\nRows: 1\nColumns: 11\n$ Item  <chr> \"Chicken Consumption (tonnes)\"\n$ Y2010 <dbl> 161460\n$ Y2011 <dbl> 170315\n$ Y2012 <dbl> 173521\n$ Y2013 <dbl> 172054\n$ Y2014 <dbl> 170926\n$ Y2015 <dbl> 179122\n$ Y2016 <dbl> 195290\n$ Y2017 <dbl> 167588\n$ Y2018 <dbl> 193142\n$ Y2019 <dbl> 193903\n\nchicken_consumption %>% \n  pivot_longer(cols = starts_with(\"Y\"),\n               names_to = \"Year\",\n               values_to = \"Tonnes\") %>% \n  select(Year, Tonnes) %>% \n  mutate(year_number = (parse_number(Year))) %>% \n  ggplot(aes(x = year_number, y = Tonnes)) +\n  geom_line(col = \"brown\") +\n  geom_point(size = 3, col = \"brown\") +\n  labs( title = \"Chicken Consumption(tonnes) in Singapore\",\n        subtitle = \"Chicken consumption had increased in general, except for 2017.\",\n        caption = \"Source: Singapore Food Agency\") +\n  scale_x_continuous(limits = c(2010, 2019), n.breaks = 10) +\n  theme_classic()\n\n\n\n\nWhat could have resulted in the drop in chicken consumption in 2017? It could be due to the avian flu outbreak as reported in Today.\nReference\nhttps://r4ds.had.co.nz/\nhttps://jrnold.github.io/r4ds-exercise-solutions/\nhttps://rpubs.com/LaurynKeller/662242\n\n\n\n",
    "preview": "posts/20210512_Tidyverse Chap 8 - Import/08---Data-Import_2_files/figure-html5/unnamed-chunk-10-1.png",
    "last_modified": "2021-05-14T17:37:06+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210514_Tidyverse Chap 7 - Tibbles/",
    "title": "Tibbles",
    "description": "R4DS 07 - Tibbles with tibbles",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-12",
    "categories": [],
    "contents": "\nR4DS Practice 07: Tibbles\nThe codes below are from the practice exercises in https://r4ds.had.co.nz/, and are taken with reference from: https://jrnold.github.io/r4ds-exercise-solutions/\nLet’s begin now\nLoading tidyverse package.\n\n\nlibrary(tidyverse)\n\n\n\nTo coerce a data frame into a tibble, use the as_tibble() function:\n\n\niris # this is a dataframe\n\n\n    Sepal.Length Sepal.Width Petal.Length Petal.Width    Species\n1            5.1         3.5          1.4         0.2     setosa\n2            4.9         3.0          1.4         0.2     setosa\n3            4.7         3.2          1.3         0.2     setosa\n4            4.6         3.1          1.5         0.2     setosa\n5            5.0         3.6          1.4         0.2     setosa\n6            5.4         3.9          1.7         0.4     setosa\n7            4.6         3.4          1.4         0.3     setosa\n8            5.0         3.4          1.5         0.2     setosa\n9            4.4         2.9          1.4         0.2     setosa\n10           4.9         3.1          1.5         0.1     setosa\n11           5.4         3.7          1.5         0.2     setosa\n12           4.8         3.4          1.6         0.2     setosa\n13           4.8         3.0          1.4         0.1     setosa\n14           4.3         3.0          1.1         0.1     setosa\n15           5.8         4.0          1.2         0.2     setosa\n16           5.7         4.4          1.5         0.4     setosa\n17           5.4         3.9          1.3         0.4     setosa\n18           5.1         3.5          1.4         0.3     setosa\n19           5.7         3.8          1.7         0.3     setosa\n20           5.1         3.8          1.5         0.3     setosa\n21           5.4         3.4          1.7         0.2     setosa\n22           5.1         3.7          1.5         0.4     setosa\n23           4.6         3.6          1.0         0.2     setosa\n24           5.1         3.3          1.7         0.5     setosa\n25           4.8         3.4          1.9         0.2     setosa\n26           5.0         3.0          1.6         0.2     setosa\n27           5.0         3.4          1.6         0.4     setosa\n28           5.2         3.5          1.5         0.2     setosa\n29           5.2         3.4          1.4         0.2     setosa\n30           4.7         3.2          1.6         0.2     setosa\n31           4.8         3.1          1.6         0.2     setosa\n32           5.4         3.4          1.5         0.4     setosa\n33           5.2         4.1          1.5         0.1     setosa\n34           5.5         4.2          1.4         0.2     setosa\n35           4.9         3.1          1.5         0.2     setosa\n36           5.0         3.2          1.2         0.2     setosa\n37           5.5         3.5          1.3         0.2     setosa\n38           4.9         3.6          1.4         0.1     setosa\n39           4.4         3.0          1.3         0.2     setosa\n40           5.1         3.4          1.5         0.2     setosa\n41           5.0         3.5          1.3         0.3     setosa\n42           4.5         2.3          1.3         0.3     setosa\n43           4.4         3.2          1.3         0.2     setosa\n44           5.0         3.5          1.6         0.6     setosa\n45           5.1         3.8          1.9         0.4     setosa\n46           4.8         3.0          1.4         0.3     setosa\n47           5.1         3.8          1.6         0.2     setosa\n48           4.6         3.2          1.4         0.2     setosa\n49           5.3         3.7          1.5         0.2     setosa\n50           5.0         3.3          1.4         0.2     setosa\n51           7.0         3.2          4.7         1.4 versicolor\n52           6.4         3.2          4.5         1.5 versicolor\n53           6.9         3.1          4.9         1.5 versicolor\n54           5.5         2.3          4.0         1.3 versicolor\n55           6.5         2.8          4.6         1.5 versicolor\n56           5.7         2.8          4.5         1.3 versicolor\n57           6.3         3.3          4.7         1.6 versicolor\n58           4.9         2.4          3.3         1.0 versicolor\n59           6.6         2.9          4.6         1.3 versicolor\n60           5.2         2.7          3.9         1.4 versicolor\n61           5.0         2.0          3.5         1.0 versicolor\n62           5.9         3.0          4.2         1.5 versicolor\n63           6.0         2.2          4.0         1.0 versicolor\n64           6.1         2.9          4.7         1.4 versicolor\n65           5.6         2.9          3.6         1.3 versicolor\n66           6.7         3.1          4.4         1.4 versicolor\n67           5.6         3.0          4.5         1.5 versicolor\n68           5.8         2.7          4.1         1.0 versicolor\n69           6.2         2.2          4.5         1.5 versicolor\n70           5.6         2.5          3.9         1.1 versicolor\n71           5.9         3.2          4.8         1.8 versicolor\n72           6.1         2.8          4.0         1.3 versicolor\n73           6.3         2.5          4.9         1.5 versicolor\n74           6.1         2.8          4.7         1.2 versicolor\n75           6.4         2.9          4.3         1.3 versicolor\n76           6.6         3.0          4.4         1.4 versicolor\n77           6.8         2.8          4.8         1.4 versicolor\n78           6.7         3.0          5.0         1.7 versicolor\n79           6.0         2.9          4.5         1.5 versicolor\n80           5.7         2.6          3.5         1.0 versicolor\n81           5.5         2.4          3.8         1.1 versicolor\n82           5.5         2.4          3.7         1.0 versicolor\n83           5.8         2.7          3.9         1.2 versicolor\n84           6.0         2.7          5.1         1.6 versicolor\n85           5.4         3.0          4.5         1.5 versicolor\n86           6.0         3.4          4.5         1.6 versicolor\n87           6.7         3.1          4.7         1.5 versicolor\n88           6.3         2.3          4.4         1.3 versicolor\n89           5.6         3.0          4.1         1.3 versicolor\n90           5.5         2.5          4.0         1.3 versicolor\n91           5.5         2.6          4.4         1.2 versicolor\n92           6.1         3.0          4.6         1.4 versicolor\n93           5.8         2.6          4.0         1.2 versicolor\n94           5.0         2.3          3.3         1.0 versicolor\n95           5.6         2.7          4.2         1.3 versicolor\n96           5.7         3.0          4.2         1.2 versicolor\n97           5.7         2.9          4.2         1.3 versicolor\n98           6.2         2.9          4.3         1.3 versicolor\n99           5.1         2.5          3.0         1.1 versicolor\n100          5.7         2.8          4.1         1.3 versicolor\n101          6.3         3.3          6.0         2.5  virginica\n102          5.8         2.7          5.1         1.9  virginica\n103          7.1         3.0          5.9         2.1  virginica\n104          6.3         2.9          5.6         1.8  virginica\n105          6.5         3.0          5.8         2.2  virginica\n106          7.6         3.0          6.6         2.1  virginica\n107          4.9         2.5          4.5         1.7  virginica\n108          7.3         2.9          6.3         1.8  virginica\n109          6.7         2.5          5.8         1.8  virginica\n110          7.2         3.6          6.1         2.5  virginica\n111          6.5         3.2          5.1         2.0  virginica\n112          6.4         2.7          5.3         1.9  virginica\n113          6.8         3.0          5.5         2.1  virginica\n114          5.7         2.5          5.0         2.0  virginica\n115          5.8         2.8          5.1         2.4  virginica\n116          6.4         3.2          5.3         2.3  virginica\n117          6.5         3.0          5.5         1.8  virginica\n118          7.7         3.8          6.7         2.2  virginica\n119          7.7         2.6          6.9         2.3  virginica\n120          6.0         2.2          5.0         1.5  virginica\n121          6.9         3.2          5.7         2.3  virginica\n122          5.6         2.8          4.9         2.0  virginica\n123          7.7         2.8          6.7         2.0  virginica\n124          6.3         2.7          4.9         1.8  virginica\n125          6.7         3.3          5.7         2.1  virginica\n126          7.2         3.2          6.0         1.8  virginica\n127          6.2         2.8          4.8         1.8  virginica\n128          6.1         3.0          4.9         1.8  virginica\n129          6.4         2.8          5.6         2.1  virginica\n130          7.2         3.0          5.8         1.6  virginica\n131          7.4         2.8          6.1         1.9  virginica\n132          7.9         3.8          6.4         2.0  virginica\n133          6.4         2.8          5.6         2.2  virginica\n134          6.3         2.8          5.1         1.5  virginica\n135          6.1         2.6          5.6         1.4  virginica\n136          7.7         3.0          6.1         2.3  virginica\n137          6.3         3.4          5.6         2.4  virginica\n138          6.4         3.1          5.5         1.8  virginica\n139          6.0         3.0          4.8         1.8  virginica\n140          6.9         3.1          5.4         2.1  virginica\n141          6.7         3.1          5.6         2.4  virginica\n142          6.9         3.1          5.1         2.3  virginica\n143          5.8         2.7          5.1         1.9  virginica\n144          6.8         3.2          5.9         2.3  virginica\n145          6.7         3.3          5.7         2.5  virginica\n146          6.7         3.0          5.2         2.3  virginica\n147          6.3         2.5          5.0         1.9  virginica\n148          6.5         3.0          5.2         2.0  virginica\n149          6.2         3.4          5.4         2.3  virginica\n150          5.9         3.0          5.1         1.8  virginica\n\nas_tibble(iris) # this is a tibble\n\n\n# A tibble: 150 x 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n 1          5.1         3.5          1.4         0.2 setosa \n 2          4.9         3            1.4         0.2 setosa \n 3          4.7         3.2          1.3         0.2 setosa \n 4          4.6         3.1          1.5         0.2 setosa \n 5          5           3.6          1.4         0.2 setosa \n 6          5.4         3.9          1.7         0.4 setosa \n 7          4.6         3.4          1.4         0.3 setosa \n 8          5           3.4          1.5         0.2 setosa \n 9          4.4         2.9          1.4         0.2 setosa \n10          4.9         3.1          1.5         0.1 setosa \n# … with 140 more rows\n\nTo create a tibble:\n\n\ntibble(\n  x = 1:5,\n  y = 1, # automatically filled down\n  z = x ^2 + y\n)\n\n\n# A tibble: 5 x 3\n      x     y     z\n  <int> <dbl> <dbl>\n1     1     1     2\n2     2     1     5\n3     3     1    10\n4     4     1    17\n5     5     1    26\n\nTo create a transposed tibble, use tribble() :\n\n\ntribble(\n  ~x, ~y, ~z,\n  # -- # -- # -- \n  \"a\", 2, 3.6,\n  \"b\", 1, 8.5\n)\n\n\n# A tibble: 2 x 3\n  x         y     z\n  <chr> <dbl> <dbl>\n1 a         2   3.6\n2 b         1   8.5\n\nExercise:\nTo check if an object is a tibble:\n\n\nstr(mtcars) # this is a dataframe\n\n\n'data.frame':   32 obs. of  11 variables:\n $ mpg : num  21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ...\n $ cyl : num  6 6 4 6 8 6 8 4 4 6 ...\n $ disp: num  160 160 108 258 360 ...\n $ hp  : num  110 110 93 110 175 105 245 62 95 123 ...\n $ drat: num  3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ...\n $ wt  : num  2.62 2.88 2.32 3.21 3.44 ...\n $ qsec: num  16.5 17 18.6 19.4 17 ...\n $ vs  : num  0 0 1 1 0 1 0 1 1 1 ...\n $ am  : num  1 1 1 0 0 0 0 0 0 0 ...\n $ gear: num  4 4 4 3 3 3 3 4 4 4 ...\n $ carb: num  4 4 1 1 2 1 4 2 2 4 ...\n\nis_tibble(mtcars)\n\n\n[1] FALSE\n\nis_tibble(diamonds)\n\n\n[1] TRUE\n\nis_tibble(iris)\n\n\n[1] FALSE\n\nCompare and contrast the following operations on a data.frame and equivalent tibble:\n\n\ndf <- data.frame(abc = 1, \n                 xyz = \"a\")\n\ndf\n\n\n  abc xyz\n1   1   a\n\ndf$x\n\n\n[1] \"a\"\n\ndf[ , \"xyz\"]\n\n\n[1] \"a\"\n\ndf[ , c(\"abc\", \"xyz\")]\n\n\n  abc xyz\n1   1   a\n\ntbl <- as_tibble(df)\ntbl\n\n\n# A tibble: 1 x 2\n    abc xyz  \n  <dbl> <chr>\n1     1 a    \n\ntbl[, \"xyz\"]\n\n\n# A tibble: 1 x 1\n  xyz  \n  <chr>\n1 a    \n\ntbl[ , c(\"abc\", \"xyz\")]\n\n\n# A tibble: 1 x 2\n    abc xyz  \n  <dbl> <chr>\n1     1 a    \n\nPractice referring to nonsyntactic names in the following dataframe:\n\n\nannoying <- tibble(\n  `1` = 1:10,\n  `2` = `1` * 2 + rnorm(length(`1`))\n)\n\nannoying\n\n\n# A tibble: 10 x 2\n     `1`   `2`\n   <int> <dbl>\n 1     1  3.59\n 2     2  4.09\n 3     3  6.77\n 4     4  8.48\n 5     5  8.72\n 6     6 13.8 \n 7     7 14.0 \n 8     8 15.0 \n 9     9 17.8 \n10    10 21.1 \n\n# Extract the variable called 1:\n\nannoying$`1`\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n# Plot a scatterplot of 1 vs 2\n\nannoying %>% \n  ggplot(aes(x = `1`,\n         y = `2`)) +\n  geom_point() +\n  theme_classic()\n\n\n\n# Create a new column called 3, which is 2 divided by 1\n\nannoying_new <- annoying %>% \n  mutate(`3` = `2` / `1`) %>% \n  dplyr::rename(one = `1`,  # need to specify dplyr:: if not will have conflict with plyr\n                two = `2`,\n                three = `3`)\n\nglimpse(annoying_new)\n\n\nRows: 10\nColumns: 3\n$ one   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10\n$ two   <dbl> 3.585017, 4.092088, 6.767932, 8.476036, 8.722597, 13.7…\n$ three <dbl> 3.585017, 2.046044, 2.255977, 2.119009, 1.744519, 2.29…\n\nWhat does tibble::enframe() do?\nenframe() converts named atomic vectors or lists to one- or two-column data frames. For a list, the result will be a nested tibble with a column of type list. For unnamed vectors, the natural sequence is used as name column.\n\n\nenframe(1:3)\n\n\n# A tibble: 3 x 2\n   name value\n  <int> <int>\n1     1     1\n2     2     2\n3     3     3\n\nenframe(c(a = 5, b = 7))\n\n\n# A tibble: 2 x 2\n  name  value\n  <chr> <dbl>\n1 a         5\n2 b         7\n\nenframe(list(one = 1, two = 2:3, three = 4:6))\n\n\n# A tibble: 3 x 2\n  name  value    \n  <chr> <list>   \n1 one   <dbl [1]>\n2 two   <int [2]>\n3 three <int [3]>\n\ndeframe(tibble(a = as.list(1:3)))\n\n\n[[1]]\n[1] 1\n\n[[2]]\n[1] 2\n\n[[3]]\n[1] 3\n\nWhat option controls how many additional column names are printed at the foot of a tibble?\n\n\nprint(mtcars)\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4\n                    carb\nMazda RX4              4\nMazda RX4 Wag          4\nDatsun 710             1\nHornet 4 Drive         1\nHornet Sportabout      2\nValiant                1\nDuster 360             4\nMerc 240D              2\nMerc 230               2\nMerc 280               4\nMerc 280C              4\nMerc 450SE             3\nMerc 450SL             3\nMerc 450SLC            3\nCadillac Fleetwood     4\nLincoln Continental    4\nChrysler Imperial      4\nFiat 128               1\nHonda Civic            2\nToyota Corolla         1\nToyota Corona          1\nDodge Challenger       2\nAMC Javelin            2\nCamaro Z28             4\nPontiac Firebird       2\nFiat X1-9              1\nPorsche 914-2          2\nLotus Europa           2\nFord Pantera L         4\nFerrari Dino           6\nMaserati Bora          8\nVolvo 142E             2\n\nprint(mtcars, n_extra = 4)\n\n\n                     mpg cyl  disp  hp drat    wt  qsec vs am gear\nMazda RX4           21.0   6 160.0 110 3.90 2.620 16.46  0  1    4\nMazda RX4 Wag       21.0   6 160.0 110 3.90 2.875 17.02  0  1    4\nDatsun 710          22.8   4 108.0  93 3.85 2.320 18.61  1  1    4\nHornet 4 Drive      21.4   6 258.0 110 3.08 3.215 19.44  1  0    3\nHornet Sportabout   18.7   8 360.0 175 3.15 3.440 17.02  0  0    3\nValiant             18.1   6 225.0 105 2.76 3.460 20.22  1  0    3\nDuster 360          14.3   8 360.0 245 3.21 3.570 15.84  0  0    3\nMerc 240D           24.4   4 146.7  62 3.69 3.190 20.00  1  0    4\nMerc 230            22.8   4 140.8  95 3.92 3.150 22.90  1  0    4\nMerc 280            19.2   6 167.6 123 3.92 3.440 18.30  1  0    4\nMerc 280C           17.8   6 167.6 123 3.92 3.440 18.90  1  0    4\nMerc 450SE          16.4   8 275.8 180 3.07 4.070 17.40  0  0    3\nMerc 450SL          17.3   8 275.8 180 3.07 3.730 17.60  0  0    3\nMerc 450SLC         15.2   8 275.8 180 3.07 3.780 18.00  0  0    3\nCadillac Fleetwood  10.4   8 472.0 205 2.93 5.250 17.98  0  0    3\nLincoln Continental 10.4   8 460.0 215 3.00 5.424 17.82  0  0    3\nChrysler Imperial   14.7   8 440.0 230 3.23 5.345 17.42  0  0    3\nFiat 128            32.4   4  78.7  66 4.08 2.200 19.47  1  1    4\nHonda Civic         30.4   4  75.7  52 4.93 1.615 18.52  1  1    4\nToyota Corolla      33.9   4  71.1  65 4.22 1.835 19.90  1  1    4\nToyota Corona       21.5   4 120.1  97 3.70 2.465 20.01  1  0    3\nDodge Challenger    15.5   8 318.0 150 2.76 3.520 16.87  0  0    3\nAMC Javelin         15.2   8 304.0 150 3.15 3.435 17.30  0  0    3\nCamaro Z28          13.3   8 350.0 245 3.73 3.840 15.41  0  0    3\nPontiac Firebird    19.2   8 400.0 175 3.08 3.845 17.05  0  0    3\nFiat X1-9           27.3   4  79.0  66 4.08 1.935 18.90  1  1    4\nPorsche 914-2       26.0   4 120.3  91 4.43 2.140 16.70  0  1    5\nLotus Europa        30.4   4  95.1 113 3.77 1.513 16.90  1  1    5\nFord Pantera L      15.8   8 351.0 264 4.22 3.170 14.50  0  1    5\nFerrari Dino        19.7   6 145.0 175 3.62 2.770 15.50  0  1    5\nMaserati Bora       15.0   8 301.0 335 3.54 3.570 14.60  0  1    5\nVolvo 142E          21.4   4 121.0 109 4.11 2.780 18.60  1  1    4\n                    carb\nMazda RX4              4\nMazda RX4 Wag          4\nDatsun 710             1\nHornet 4 Drive         1\nHornet Sportabout      2\nValiant                1\nDuster 360             4\nMerc 240D              2\nMerc 230               2\nMerc 280               4\nMerc 280C              4\nMerc 450SE             3\nMerc 450SL             3\nMerc 450SLC            3\nCadillac Fleetwood     4\nLincoln Continental    4\nChrysler Imperial      4\nFiat 128               1\nHonda Civic            2\nToyota Corolla         1\nToyota Corona          1\nDodge Challenger       2\nAMC Javelin            2\nCamaro Z28             4\nPontiac Firebird       2\nFiat X1-9              1\nPorsche 914-2          2\nLotus Europa           2\nFord Pantera L         4\nFerrari Dino           6\nMaserati Bora          8\nVolvo 142E             2\n\nReference\nhttps://r4ds.had.co.nz/\nhttps://jrnold.github.io/r4ds-exercise-solutions/\n\n\n\n",
    "preview": "posts/20210514_Tidyverse Chap 7 - Tibbles/07---Tibbles_files/figure-html5/unnamed-chunk-7-1.png",
    "last_modified": "2021-05-14T15:44:53+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210501_Tidyverse Chap 5 - ggplot2/",
    "title": "Exploratory Data Analysis",
    "description": "R4DS 05 - Exploratory Data Analysis",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-05-01",
    "categories": [],
    "contents": "\nR4DS Practice 05: Exploratory Data Analysis\nThe codes below are from the practice exercises in https://r4ds.had.co.nz/, and are taken with reference from: https://jrnold.github.io/r4ds-exercise-solutions/\nLet’s begin now\nLoading tidyverse package. The main packages used are dplyr and ggplot2.\n\n\nlibrary(tidyverse)\n\n\n\nEDA\nEDA is carried out to generate questions about your data, to search for answers by visualizing, transforming and modelling your data, and to use the insights to further generate new questions.\nVisualizing variation\nVisualizing distributions: Categorical - Use Bar Chart\n\n\ncount_cut <- diamonds %>% \n  count(cut) %>% \n  arrange(desc(n))\n\ncount_cut\n\n\n# A tibble: 5 x 2\n  cut           n\n  <ord>     <int>\n1 Ideal     21551\n2 Premium   13791\n3 Very Good 12082\n4 Good       4906\n5 Fair       1610\n\ndiamonds %>% \n  ggplot(aes(x = cut)) +\n  geom_bar(fill = \"coral\")  +\n  theme_classic()\n\n\n\n\nVisualizing distributions: Numerical - Histogram\n\n\ndiamonds %>% \n  count(cut_width(carat, 0.5))  # cut_width() makes groups of width width.\n\n\n# A tibble: 11 x 2\n   `cut_width(carat, 0.5)`     n\n   <fct>                   <int>\n 1 [-0.25,0.25]              785\n 2 (0.25,0.75]             29498\n 3 (0.75,1.25]             15977\n 4 (1.25,1.75]              5313\n 5 (1.75,2.25]              2002\n 6 (2.25,2.75]               322\n 7 (2.75,3.25]                32\n 8 (3.25,3.75]                 5\n 9 (3.75,4.25]                 4\n10 (4.25,4.75]                 1\n11 (4.75,5.25]                 1\n\ndiamonds %>% \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.5, fill = \"deepskyblue4\", col = \"black\") +\n  labs(title = \"Histogram of carat size distribution in diamonds dataset\",\n       subtitle = \"Most of the diamonds have a carat value between 0.25 and 0.75\") +\n  scale_x_continuous(n.breaks =  20) +\n  theme_classic()\n\n\n\n\nA histogram divides the x-axis into equally spaced bins and then uses the height of each bar to display the number of observations that fall in each bin.\nTo overlay multiple histograms:\n\n\ndiamonds %>% \n  ggplot(aes(x = carat, color = cut)) +\n  geom_freqpoly(binwidth = 0.1) +\n  theme_classic()\n\n\n\n\nTypical Values\n\n\ndiamonds %>% \n  filter(carat<3) %>% \n  ggplot(aes(x = carat)) +\n  geom_histogram(binwidth = 0.01, col = \"coral\") +\n  scale_x_continuous(n.breaks = 20) +\n  labs(title = \"Histogram of diamond sizes < 3 carats.\",\n        subtitle = \"Most diamonds are smaller than 1 carat\") +\n  theme_classic()\n\n\n\n\nUnusual Values\n\n\ndiamonds %>% \n  ggplot(aes(x = y)) +\n  geom_histogram(aes(x = y), binwidth = 0.5, fill = \"deepskyblue3\") +\n  labs(title = \"Histogram for Diamond Carat Sizes\",\n       subtitle = \"Using coord_cartesian() to reveal outlier values that might otherwise not be obvious.\") +\n  scale_x_continuous(n.breaks = 20) +\n  coord_cartesian(ylim = c(0, 20)) +\n  theme_classic()\n\n\n\n\nExercise\nExplore the distribution of each of the x, y, z variables in diamonds dataset.\n\n\nglimpse(diamonds)\n\n\nRows: 53,940\nColumns: 10\n$ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22…\n$ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, Very…\n$ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J…\n$ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, …\n$ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1…\n$ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, …\n$ price   <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 33…\n$ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87…\n$ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78…\n$ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49…\n\ndiamonds %>% \n  select(x,y,z) %>% \n  summary() # summary statistics\n\n\n       x                y                z         \n Min.   : 0.000   Min.   : 0.000   Min.   : 0.000  \n 1st Qu.: 4.710   1st Qu.: 4.720   1st Qu.: 2.910  \n Median : 5.700   Median : 5.710   Median : 3.530  \n Mean   : 5.731   Mean   : 5.735   Mean   : 3.539  \n 3rd Qu.: 6.540   3rd Qu.: 6.540   3rd Qu.: 4.040  \n Max.   :10.740   Max.   :58.900   Max.   :31.800  \n\n# geom_histogram\n\nhist_x <- diamonds %>% \n  select(x) %>% \n  ggplot(aes(x = x)) +\n  geom_histogram(binwidth = 0.01, fill = \"deepskyblue4\") +\n  labs(title = \"Histogram for x\",\n       subtitle = \"x has a multimodal distribution and is of the range of 0 - 10.74\") +\n  scale_x_continuous(n.breaks = 30) +\n  theme_classic()\n\n\nhist_y <- diamonds %>% \n  select(y) %>% \n  ggplot(aes(x = y)) +\n  geom_histogram(binwidth = 0.01, fill = \"deepskyblue4\") +\n  labs(title = \"Histogram for y\",\n       subtitle = \"y has a multimodal distribution and is of the range of 0 - 58.90. There seem to be outliers in this variable.\") +\n  scale_x_continuous(n.breaks = 30) +\n  theme_classic()\n\n(hist_z <- diamonds %>% \n  select(z) %>% \n  ggplot(aes(x = z)) +\n  geom_histogram(binwidth = 0.01, fill = \"deepskyblue4\") +\n  labs(title = \"Histogram for z\",\n       subtitle = \"z has a multimodal distribution and is of the range of 0 - 31.8 There seem to be outliers in this variable.\") +\n  scale_x_continuous(n.breaks = 30) +\n  theme_classic())\n\n\n\ngridExtra::grid.arrange(hist_x, hist_y, hist_z, nrow = 3)\n\n\n\n\nx is likely to be length, y is likely to be breadth and z is likely to be depth.\n\n\ndiamonds %>% \n  ggplot(aes(x, y)) +\n  geom_point() +\n  labs(title = \"Scatterplot of x vs y\") +\n  theme_classic()\n\n\n\ndiamonds %>% \n  ggplot(aes(x, z)) +\n  geom_point() +\n  labs(title = \"Scatterplot of x vs z\") +\n  theme_classic()\n\n\n\n\nThe plots above make it easier to detect outliers in relation to other variables.\nExplore the distribution of price\n\n\ndiamonds %>% \n  select(price) %>% \n  summary()\n\n\n     price      \n Min.   :  326  \n 1st Qu.:  950  \n Median : 2401  \n Mean   : 3933  \n 3rd Qu.: 5324  \n Max.   :18823  \n\ndiamonds %>% \n  select(price) %>% \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 5)\n\n\n\ndiamonds %>% \n  select(price) %>% \n  ggplot(aes(x = price)) +\n  geom_histogram(binwidth = 5) +\n  labs(title = \"Histogram for price distribution of diamonds\",\n       subtitle = \"There is a strange gap between $1000 and $2000\") +\n  coord_cartesian(xlim = c(0, 3000)) +\n  theme_classic()\n\n\n\n\nHow many diamonds are 0.99 carat? How many are 1 carat?\n\n\ndiamonds %>% \n  filter(carat == 0.99) %>% \n  count() # 23\n\n\n# A tibble: 1 x 1\n      n\n  <int>\n1    23\n\ndiamonds %>% \n  filter(carat == 1) %>% \n  count()\n\n\n# A tibble: 1 x 1\n      n\n  <int>\n1  1558\n\n# This could be due to the price differences.\n\n\n\nMissing Values\nIt is recommended to replace outlier values with missing values using mutate() and ifelse().\n\n\ndiamonds_b <- diamonds %>% \n  mutate(y_edited = ifelse(y<3 | y >20, NA, y))\n\ndiamonds_b %>% \n  ggplot(aes(x, y_edited)) +\n  geom_point(na.rm = T)\n\n\n\n\nVisualizing Covariation (behaviour between variables)\nCovariation is the tendency for the values of two or more variables to vary together in a related way. The best way to spot covariation is the visualize the relationship between two or more variables.\nCategorical and Continuous Variable: Boxplot\n\n\ndiamonds %>% \n  ggplot(aes(x = fct_rev(cut), y = price)) +\n  geom_boxplot() +\n  labs(title = \"How price varies for different cuts\",\n       subtitle = \"Ideal cuts cost lower than fair cuts\",\n       x = \"Cut\",\n       y = \"Price\") +\n  theme_classic() +\n  coord_flip()\n\n\n\n\nExercise\nImprove the visualization for departure times of cancelled vs non-cancelled flights\n\n\nlibrary(nycflights13)\n\nglimpse(flights)\n\n\nRows: 336,776\nColumns: 19\n$ year           <int> 2013, 2013, 2013, 2013, 2013, 2013, 2013, 201…\n$ month          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ day            <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ dep_time       <int> 517, 533, 542, 544, 554, 554, 555, 557, 557, …\n$ sched_dep_time <int> 515, 529, 540, 545, 600, 558, 600, 600, 600, …\n$ dep_delay      <dbl> 2, 4, 2, -1, -6, -4, -5, -3, -3, -2, -2, -2, …\n$ arr_time       <int> 830, 850, 923, 1004, 812, 740, 913, 709, 838,…\n$ sched_arr_time <int> 819, 830, 850, 1022, 837, 728, 854, 723, 846,…\n$ arr_delay      <dbl> 11, 20, 33, -18, -25, 12, 19, -14, -8, 8, -2,…\n$ carrier        <chr> \"UA\", \"UA\", \"AA\", \"B6\", \"DL\", \"UA\", \"B6\", \"EV…\n$ flight         <int> 1545, 1714, 1141, 725, 461, 1696, 507, 5708, …\n$ tailnum        <chr> \"N14228\", \"N24211\", \"N619AA\", \"N804JB\", \"N668…\n$ origin         <chr> \"EWR\", \"LGA\", \"JFK\", \"JFK\", \"LGA\", \"EWR\", \"EW…\n$ dest           <chr> \"IAH\", \"IAH\", \"MIA\", \"BQN\", \"ATL\", \"ORD\", \"FL…\n$ air_time       <dbl> 227, 227, 160, 183, 116, 150, 158, 53, 140, 1…\n$ distance       <dbl> 1400, 1416, 1089, 1576, 762, 719, 1065, 229, …\n$ hour           <dbl> 5, 5, 5, 5, 6, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, …\n$ minute         <dbl> 15, 29, 40, 45, 0, 58, 0, 0, 0, 0, 0, 0, 0, 0…\n$ time_hour      <dttm> 2013-01-01 05:00:00, 2013-01-01 05:00:00, 20…\n\nflights %>% \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + sched_min/60\n  ) %>% \n  ggplot(aes(x = cancelled, y = sched_dep_time)) +\n  geom_boxplot() +\n  labs(title = \"Departure times of cancelled vs non-cancelled flights\") +\n  theme_classic()\n\n\n\n\nWhich variable in the diamonds dataset is the most important for predicting the price of a diamond? How is that variable correlated with cut? Why does the combination of those two relationships lead to lower quality diamonds being more expensive?\n\n\nglimpse(diamonds)\n\n\nRows: 53,940\nColumns: 10\n$ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0.22…\n$ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, Very…\n$ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I, J…\n$ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS1, …\n$ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 65.1…\n$ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 61, …\n$ price   <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338, 33…\n$ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3.87…\n$ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3.78…\n$ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2.49…\n\n# To investigate the relationship between price and carat\n\ndiamonds %>% \n  select(carat, price) %>% \n  ggplot(aes(carat, price)) +\n  labs(title = \"Relationship between price and carat of diamonds\",\n       subtitle = \"The larger the diamond, the higher the price of diamonds\") +\n  geom_point() +\n  theme_classic()\n\n\n\n\n\n\n# To investigate the relationship between cut and price\n\ndiamonds %>% \n  select(cut, price) %>% \n  ggplot(aes(x = cut, y = price)) +\n  geom_boxplot(notch = T) +\n  labs(title = \"Relationship between cut and price of diamonds\",\n       subtitle = \"There is a weak negative relationship between cut and price.\") +\n  theme_classic()\n\n\n\n\n\n\n# Relationship between color and price\n\ndiamonds %>% \n  select(color, price) %>% \n  ggplot(aes(x = fct_rev(color), y = price)) +\n  geom_boxplot(notch = T) +\n  labs(title = \"Relationship between color and price of diamonds\",\n       subtitle = \"There is a weak negative relationship between cut and price.\",\n       caption = \"The scale of color goes from D (best) to J (worst)\",\n       x = \"Color\",\n       y = \"Price\") +\n  theme_classic()\n\n\n\n\n\n\n# Relationship between price and clarity\n\ndiamonds %>% \n  select(clarity, price) %>% \n  ggplot(aes(x = clarity, y = price)) +\n  geom_boxplot(notch = T) +\n  labs(title = \"Relationship between clarity and price of diamonds\",\n       subtitle = \"There is a weak negative relationship between clarity and price.\",\n       caption = \"The scale of clarity goes from I1 (worst) to IF (best)\",\n       x = \"Clarity\",\n       y = \"Price\") +\n  theme_classic()\n\n\n\n\nCarat is the best predictor of diamond prices.\n\n\n#Comparing carat and cut\n\ndiamonds %>% \n  select(carat, cut) %>% \n  ggplot(aes(carat, cut)) +\n  geom_boxplot() +\n  labs(title = \"Relationship between cut and carat size\",\n       subtitle = \"Bigger diamonds may not be of the best cut.\") +\n  theme_classic()\n\n\n\n\nOne problem with boxplots is that they were developed in an era of much smaller datasets and tend to display a prohibitively large number of “outlier values”. One approach to remedy this problem is the letter value plot.\n\n\n# install.packages(\"lvplot\")\nlibrary(lvplot)\n\ndiamonds %>% \n  ggplot(aes(cut, price)) +\n  geom_lv(col = \"deepskyblue4\") +\n  theme_classic()\n\n\n\n\nThe letter value plots give more insights on how the data is distributed beyond the quantiles. Larger boxes mean that a larger proportion of the data falls within the boxed area.\nCompare and contrast geom_violin() with a faceted geom_histogram() or a colored geom_freqpoly().\n\n\ndiamonds %>% \n  ggplot(aes(price)) +\n  geom_histogram(col = \"black\", fill = \"deepskyblue4\") +\n  labs(title = \"Histogram showing price distribution of different cuts of diamonds\",\n       subtitle = \"Histograms are good for comparing the overall shape of distributions across categories\") +\n  facet_wrap( ~cut, ncol = 1, scales = \"free_y\") +\n  theme_classic()\n\n\n\ndiamonds %>% \n  ggplot(aes(cut, price)) +\n  geom_violin(fill = \"orange\") +\n  labs(title = \"Violin plot showing distribution of prices for different cuts of diamonds\",\n       subtitle = \"Violin plot shows the full distribution of data, and are useful if the distribution is multimodal.\") +\n  theme_classic() +\n  coord_flip()\n\n\n\n\nTwo Categorical Variables\nTo visualize the covariation between categorical variables, dplyr can be used to compute the count, and then visualize using geom_tile()\n\n\ndiamonds %>% \n  count(color, cut)\n\n\n# A tibble: 35 x 3\n   color cut           n\n   <ord> <ord>     <int>\n 1 D     Fair        163\n 2 D     Good        662\n 3 D     Very Good  1513\n 4 D     Premium    1603\n 5 D     Ideal      2834\n 6 E     Fair        224\n 7 E     Good        933\n 8 E     Very Good  2400\n 9 E     Premium    2337\n10 E     Ideal      3903\n# … with 25 more rows\n\n# visualize\n\ndiamonds %>% \n  count(color, cut) %>%  \n  ggplot(aes(x = color, y = cut)) + # use the variable with longer labels on y axis\n  geom_tile(aes(fill = n)) +\n  scale_fill_gradient(low = \"white\", high = \"deepskyblue4\") # manual \n\n\n\n\nTo show the distribution of cut within color, the proportion of each cut within a color can be calculated.\n\n\ndiamonds %>% \n  count(color, cut) %>% \n  group_by(cut) %>% \n  mutate(proportion = n/sum(n)) %>% \n  ggplot(aes(color, cut)) +\n  geom_tile(aes(fill = proportion)) +\n  scale_fill_gradient(low = \"white\", high = \"deepskyblue4\") # manual \n\n\n\n\nTwo continuous variables\nA scatter plot may be used to visualize the co-variation between two variables.\nIf the dataset is too big, one can bin one continuous variable such that it acts like a categorical variable, and then use boxplot or frequency polygon to visualize.\nReference\nhttps://r4ds.had.co.nz/\nhttps://jrnold.github.io/r4ds-exercise-solutions/\nhttps://towardsdatascience.com/letter-value-plot-the-easy-to-understand-boxplot-for-large-datasets-12d6c1279c97\n\n\n\n",
    "preview": "posts/20210501_Tidyverse Chap 5 - ggplot2/05---EDA_files/figure-html5/unnamed-chunk-2-1.png",
    "last_modified": "2021-05-12T16:24:03+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210424_Tidyverse Chap 1 - Data visualization/",
    "title": "Data transformation",
    "description": "R4DS 03 - Data transformation with dplyr",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-04-28",
    "categories": [],
    "contents": "\nR4DS Practice 03: Data transformation with dplyr\nThe codes below are from R4DS book by Hadley Wickham.\nLoading required packages:\n\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\n\n\nThe nycflights13 dataset contains all 336,776 flights that departed from New York City in 2013. The data comes from US Bureau of Transportation Statistics.\n\n\nhead(flights)\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1     1      517            515         2      830\n2  2013     1     1      533            529         4      850\n3  2013     1     1      542            540         2      923\n4  2013     1     1      544            545        -1     1004\n5  2013     1     1      554            600        -6      812\n6  2013     1     1      554            558        -4      740\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\nLearning filter()\nSelecting all flights on Jan 1st:\n\n\nfilter(flights, month == 1, day == 1) # filter month = 1 (Jan) and day = 1 (1st)\n\n\n# A tibble: 842 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 832 more rows, and 12 more variables: sched_arr_time <int>,\n#   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\n# to filter for flights on xmas day\nflights %>% \n  filter(month == 12, day == 25)\n\n\n# A tibble: 719 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013    12    25      456            500        -4      649\n 2  2013    12    25      524            515         9      805\n 3  2013    12    25      542            540         2      832\n 4  2013    12    25      546            550        -4     1022\n 5  2013    12    25      556            600        -4      730\n 6  2013    12    25      557            600        -3      743\n 7  2013    12    25      557            600        -3      818\n 8  2013    12    25      559            600        -1      855\n 9  2013    12    25      559            600        -1      849\n10  2013    12    25      600            600         0      850\n# … with 709 more rows, and 12 more variables: sched_arr_time <int>,\n#   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\n# to filter for flights departing in nov and dec\nflights %>% \n  filter(month %in% c(11, 12))\n\n\n# A tibble: 55,403 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013    11     1        5           2359         6      352\n 2  2013    11     1       35           2250       105      123\n 3  2013    11     1      455            500        -5      641\n 4  2013    11     1      539            545        -6      856\n 5  2013    11     1      542            545        -3      831\n 6  2013    11     1      549            600       -11      912\n 7  2013    11     1      550            600       -10      705\n 8  2013    11     1      554            600        -6      659\n 9  2013    11     1      554            600        -6      826\n10  2013    11     1      554            600        -6      749\n# … with 55,393 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nExercise questions:\nQ1 - Find all flights that had an arrival delay of two or more hours\n\n\nflights %>% \n  filter(arr_delay >= 120)\n\n\n# A tibble: 10,200 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      811            630       101     1047\n 2  2013     1     1      848           1835       853     1001\n 3  2013     1     1      957            733       144     1056\n 4  2013     1     1     1114            900       134     1447\n 5  2013     1     1     1505           1310       115     1638\n 6  2013     1     1     1525           1340       105     1831\n 7  2013     1     1     1549           1445        64     1912\n 8  2013     1     1     1558           1359       119     1718\n 9  2013     1     1     1732           1630        62     2028\n10  2013     1     1     1803           1620       103     2008\n# … with 10,190 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nQ2 - Flew to Houston (IAH or HOU)\n\n\nflights %>% \n  filter(dest %in% c(\"IAH\", \"HOU\"))\n\n\n# A tibble: 9,313 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      623            627        -4      933\n 4  2013     1     1      728            732        -4     1041\n 5  2013     1     1      739            739         0     1104\n 6  2013     1     1      908            908         0     1228\n 7  2013     1     1     1028           1026         2     1350\n 8  2013     1     1     1044           1045        -1     1352\n 9  2013     1     1     1114            900       134     1447\n10  2013     1     1     1205           1200         5     1503\n# … with 9,303 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nQ3 - Were operated by United, American or Delta\n\n\nflights %>% \n  filter(carrier %in% c(\"UA\", \"AA\", \"DL\"))\n\n\n# A tibble: 139,504 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      554            600        -6      812\n 5  2013     1     1      554            558        -4      740\n 6  2013     1     1      558            600        -2      753\n 7  2013     1     1      558            600        -2      924\n 8  2013     1     1      558            600        -2      923\n 9  2013     1     1      559            600        -1      941\n10  2013     1     1      559            600        -1      854\n# … with 139,494 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nQ4 - Departed in July, August, and September\n\n\nflights %>% \n  filter(month %in% c(7:9))\n\n\n# A tibble: 86,326 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     7     1        1           2029       212      236\n 2  2013     7     1        2           2359         3      344\n 3  2013     7     1       29           2245       104      151\n 4  2013     7     1       43           2130       193      322\n 5  2013     7     1       44           2150       174      300\n 6  2013     7     1       46           2051       235      304\n 7  2013     7     1       48           2001       287      308\n 8  2013     7     1       58           2155       183      335\n 9  2013     7     1      100           2146       194      327\n10  2013     7     1      100           2245       135      337\n# … with 86,316 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nQ5 - Arrived more than two hours late, but didn’t leave late\n\n\nflights %>% \n  filter(arr_delay >= 120, dep_delay <=0)\n\n\n# A tibble: 29 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1    27     1419           1420        -1     1754\n 2  2013    10     7     1350           1350         0     1736\n 3  2013    10     7     1357           1359        -2     1858\n 4  2013    10    16      657            700        -3     1258\n 5  2013    11     1      658            700        -2     1329\n 6  2013     3    18     1844           1847        -3       39\n 7  2013     4    17     1635           1640        -5     2049\n 8  2013     4    18      558            600        -2     1149\n 9  2013     4    18      655            700        -5     1213\n10  2013     5    22     1827           1830        -3     2217\n# … with 19 more rows, and 12 more variables: sched_arr_time <int>,\n#   arr_delay <dbl>, carrier <chr>, flight <int>, tailnum <chr>,\n#   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>,\n#   hour <dbl>, minute <dbl>, time_hour <dttm>\n\nQ6 - Were delayed by at least an hour, but made up over 30 minutes in flight\n\n\nflights %>% \n  filter(dep_delay >=60, dep_delay - arr_delay >30)\n\n\n# A tibble: 1,844 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1     2205           1720       285       46\n 2  2013     1     1     2326           2130       116      131\n 3  2013     1     3     1503           1221       162     1803\n 4  2013     1     3     1839           1700        99     2056\n 5  2013     1     3     1850           1745        65     2148\n 6  2013     1     3     1941           1759       102     2246\n 7  2013     1     3     1950           1845        65     2228\n 8  2013     1     3     2015           1915        60     2135\n 9  2013     1     3     2257           2000       177       45\n10  2013     1     4     1917           1700       137     2135\n# … with 1,834 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nQ7 - Departed between midnight and 6am\n\n\nflights %>% \n  filter(dep_time <= 600 | dep_time == 2400)\n\n\n# A tibble: 9,373 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 9,363 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nUsing the between function to filter flights that departed in summer:\n\n\nflights %>% \n  filter(between(month, 7, 9))\n\n\n# A tibble: 86,326 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     7     1        1           2029       212      236\n 2  2013     7     1        2           2359         3      344\n 3  2013     7     1       29           2245       104      151\n 4  2013     7     1       43           2130       193      322\n 5  2013     7     1       44           2150       174      300\n 6  2013     7     1       46           2051       235      304\n 7  2013     7     1       48           2001       287      308\n 8  2013     7     1       58           2155       183      335\n 9  2013     7     1      100           2146       194      327\n10  2013     7     1      100           2245       135      337\n# … with 86,316 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nHow many flights have a missing dep_time?\n\n\nflights %>% \n  filter(is.na(dep_time))\n\n\n# A tibble: 8,255 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1       NA           1630        NA       NA\n 2  2013     1     1       NA           1935        NA       NA\n 3  2013     1     1       NA           1500        NA       NA\n 4  2013     1     1       NA            600        NA       NA\n 5  2013     1     2       NA           1540        NA       NA\n 6  2013     1     2       NA           1620        NA       NA\n 7  2013     1     2       NA           1355        NA       NA\n 8  2013     1     2       NA           1420        NA       NA\n 9  2013     1     2       NA           1321        NA       NA\n10  2013     1     2       NA           1545        NA       NA\n# … with 8,245 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nLearning arrange()\n\n\nflights %>% \n  arrange(year, month, day)\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1      517            515         2      830\n 2  2013     1     1      533            529         4      850\n 3  2013     1     1      542            540         2      923\n 4  2013     1     1      544            545        -1     1004\n 5  2013     1     1      554            600        -6      812\n 6  2013     1     1      554            558        -4      740\n 7  2013     1     1      555            600        -5      913\n 8  2013     1     1      557            600        -3      709\n 9  2013     1     1      557            600        -3      838\n10  2013     1     1      558            600        -2      753\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nflights %>% \n  arrange(desc(month))\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013    12     1       13           2359        14      446\n 2  2013    12     1       17           2359        18      443\n 3  2013    12     1      453            500        -7      636\n 4  2013    12     1      520            515         5      749\n 5  2013    12     1      536            540        -4      845\n 6  2013    12     1      540            550       -10     1005\n 7  2013    12     1      541            545        -4      734\n 8  2013    12     1      546            545         1      826\n 9  2013    12     1      549            600       -11      648\n10  2013    12     1      550            600       -10      825\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nExercises\nUsing arrange() to sort all the missing values to the start?\n\n\nflights %>% \n  arrange(desc(is.na(dep_time), dep_time))\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     1       NA           1630        NA       NA\n 2  2013     1     1       NA           1935        NA       NA\n 3  2013     1     1       NA           1500        NA       NA\n 4  2013     1     1       NA            600        NA       NA\n 5  2013     1     2       NA           1540        NA       NA\n 6  2013     1     2       NA           1620        NA       NA\n 7  2013     1     2       NA           1355        NA       NA\n 8  2013     1     2       NA           1420        NA       NA\n 9  2013     1     2       NA           1321        NA       NA\n10  2013     1     2       NA           1545        NA       NA\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nSort flights to find the most delayed flights:\n\n\nflights %>% \n  arrange(desc(dep_delay))\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     1     9      641            900      1301     1242\n 2  2013     6    15     1432           1935      1137     1607\n 3  2013     1    10     1121           1635      1126     1239\n 4  2013     9    20     1139           1845      1014     1457\n 5  2013     7    22      845           1600      1005     1044\n 6  2013     4    10     1100           1900       960     1342\n 7  2013     3    17     2321            810       911      135\n 8  2013     6    27      959           1900       899     1236\n 9  2013     7    22     2257            759       898      121\n10  2013    12     5      756           1700       896     1058\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nSort flights that left the earliest\n\n\nflights %>% \n  arrange(dep_delay)\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013    12     7     2040           2123       -43       40\n 2  2013     2     3     2022           2055       -33     2240\n 3  2013    11    10     1408           1440       -32     1549\n 4  2013     1    11     1900           1930       -30     2233\n 5  2013     1    29     1703           1730       -27     1947\n 6  2013     8     9      729            755       -26     1002\n 7  2013    10    23     1907           1932       -25     2143\n 8  2013     3    30     2030           2055       -25     2213\n 9  2013     3     2     1431           1455       -24     1601\n10  2013     5     5      934            958       -24     1225\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nSort flights to find the fastest flights\n\n\nflights %>% \n  arrange(air_time) %>% \n  head()\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1    16     1355           1315        40     1442\n2  2013     4    13      537            527        10      622\n3  2013    12     6      922            851        31     1021\n4  2013     2     3     2153           2129        24     2247\n5  2013     2     5     1303           1315       -12     1342\n6  2013     2    12     2123           2130        -7     2211\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\nWhich flights traveled the shortest?\n\n\nflights %>% \n  arrange(air_time) %>% \n  head()\n\n\n# A tibble: 6 x 19\n   year month   day dep_time sched_dep_time dep_delay arr_time\n  <int> <int> <int>    <int>          <int>     <dbl>    <int>\n1  2013     1    16     1355           1315        40     1442\n2  2013     4    13      537            527        10      622\n3  2013    12     6      922            851        31     1021\n4  2013     2     3     2153           2129        24     2247\n5  2013     2     5     1303           1315       -12     1342\n6  2013     2    12     2123           2130        -7     2211\n# … with 12 more variables: sched_arr_time <int>, arr_delay <dbl>,\n#   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>,\n#   dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#   minute <dbl>, time_hour <dttm>\n\nwhich flights travelled the longest?\n\n\nflights %>% \n  arrange(desc(air_time))\n\n\n# A tibble: 336,776 x 19\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <int>\n 1  2013     3    17     1337           1335         2     1937\n 2  2013     2     6      853            900        -7     1542\n 3  2013     3    15     1001           1000         1     1551\n 4  2013     3    17     1006           1000         6     1607\n 5  2013     3    16     1001           1000         1     1544\n 6  2013     2     5      900            900         0     1555\n 7  2013    11    12      936            930         6     1630\n 8  2013     3    14      958           1000        -2     1542\n 9  2013    11    20     1006           1000         6     1639\n10  2013     3    15     1342           1335         7     1924\n# … with 336,766 more rows, and 12 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\nLearning select()\n\n\n# Selecting columns by name\n\nflights %>% \n  select(year, month, day)\n\n\n# A tibble: 336,776 x 3\n    year month   day\n   <int> <int> <int>\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# … with 336,766 more rows\n\n# Select all columns between year and day\n\nflights %>% \n  select(year:day)\n\n\n# A tibble: 336,776 x 3\n    year month   day\n   <int> <int> <int>\n 1  2013     1     1\n 2  2013     1     1\n 3  2013     1     1\n 4  2013     1     1\n 5  2013     1     1\n 6  2013     1     1\n 7  2013     1     1\n 8  2013     1     1\n 9  2013     1     1\n10  2013     1     1\n# … with 336,766 more rows\n\n# Select all columns except those from year to day\n\nflights %>% \n  select(-(year:day))\n\n\n# A tibble: 336,776 x 16\n   dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay\n      <int>          <int>     <dbl>    <int>          <int>     <dbl>\n 1      517            515         2      830            819        11\n 2      533            529         4      850            830        20\n 3      542            540         2      923            850        33\n 4      544            545        -1     1004           1022       -18\n 5      554            600        -6      812            837       -25\n 6      554            558        -4      740            728        12\n 7      555            600        -5      913            854        19\n 8      557            600        -3      709            723       -14\n 9      557            600        -3      838            846        -8\n10      558            600        -2      753            745         8\n# … with 336,766 more rows, and 10 more variables: carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>\n\n# Select() with everything()\n\nflights %>% \n  select(time_hour, air_time, everything())\n\n\n# A tibble: 336,776 x 19\n   time_hour           air_time  year month   day dep_time\n   <dttm>                 <dbl> <int> <int> <int>    <int>\n 1 2013-01-01 05:00:00      227  2013     1     1      517\n 2 2013-01-01 05:00:00      227  2013     1     1      533\n 3 2013-01-01 05:00:00      160  2013     1     1      542\n 4 2013-01-01 05:00:00      183  2013     1     1      544\n 5 2013-01-01 06:00:00      116  2013     1     1      554\n 6 2013-01-01 05:00:00      150  2013     1     1      554\n 7 2013-01-01 06:00:00      158  2013     1     1      555\n 8 2013-01-01 06:00:00       53  2013     1     1      557\n 9 2013-01-01 06:00:00      140  2013     1     1      557\n10 2013-01-01 06:00:00      138  2013     1     1      558\n# … with 336,766 more rows, and 13 more variables:\n#   sched_dep_time <int>, dep_delay <dbl>, arr_time <int>,\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   distance <dbl>, hour <dbl>, minute <dbl>\n\nVariables may also be defined and called upon using all_of() and any_of()\n\n\nvars <- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\n\nflights %>% \n  select(all_of(vars))\n\n\n# A tibble: 336,776 x 5\n    year month   day dep_delay arr_delay\n   <int> <int> <int>     <dbl>     <dbl>\n 1  2013     1     1         2        11\n 2  2013     1     1         4        20\n 3  2013     1     1         2        33\n 4  2013     1     1        -1       -18\n 5  2013     1     1        -6       -25\n 6  2013     1     1        -4        12\n 7  2013     1     1        -5        19\n 8  2013     1     1        -3       -14\n 9  2013     1     1        -3        -8\n10  2013     1     1        -2         8\n# … with 336,766 more rows\n\nflights %>% \n  select(any_of(vars))\n\n\n# A tibble: 336,776 x 5\n    year month   day dep_delay arr_delay\n   <int> <int> <int>     <dbl>     <dbl>\n 1  2013     1     1         2        11\n 2  2013     1     1         4        20\n 3  2013     1     1         2        33\n 4  2013     1     1        -1       -18\n 5  2013     1     1        -6       -25\n 6  2013     1     1        -4        12\n 7  2013     1     1        -5        19\n 8  2013     1     1        -3       -14\n 9  2013     1     1        -3        -8\n10  2013     1     1        -2         8\n# … with 336,766 more rows\n\nSelecting variables that contain “time”\n\n\nflights %>% \n  select(contains(\"time\"))\n\n\n# A tibble: 336,776 x 6\n   dep_time sched_dep_time arr_time sched_arr_time air_time\n      <int>          <int>    <int>          <int>    <dbl>\n 1      517            515      830            819      227\n 2      533            529      850            830      227\n 3      542            540      923            850      160\n 4      544            545     1004           1022      183\n 5      554            600      812            837      116\n 6      554            558      740            728      150\n 7      555            600      913            854      158\n 8      557            600      709            723       53\n 9      557            600      838            846      140\n10      558            600      753            745      138\n# … with 336,766 more rows, and 1 more variable: time_hour <dttm>\n\nLearning mutate()\nQuestions\nCurrently dep_time and sched_dep_time are convenient to look at but hard to compute with because they are not really continuous numbers. Convert them to a more convenient representation of number of minutes since midnight\n\n\nflights %>% \n  select(dep_time, sched_dep_time) %>% \n  mutate(dep_time_mins = (dep_time %/% 100 * 60 + dep_time %% 100) %% 1440,\n         sched_dep_time_mins = (sched_dep_time %/% 100 * 60 + sched_dep_time %% 100) %% 1440)\n\n\n# A tibble: 336,776 x 4\n   dep_time sched_dep_time dep_time_mins sched_dep_time_mins\n      <int>          <int>         <dbl>               <dbl>\n 1      517            515           317                 315\n 2      533            529           333                 329\n 3      542            540           342                 340\n 4      544            545           344                 345\n 5      554            600           354                 360\n 6      554            558           354                 358\n 7      555            600           355                 360\n 8      557            600           357                 360\n 9      557            600           357                 360\n10      558            600           358                 360\n# … with 336,766 more rows\n\nCompare air_time with arr_time - dep_time.\n\n\nflights %>% \n  mutate((dep_time = dep_time %/% 100*60 + dep_time %/% 100) %% 1440,\n         arr_time = (arr_time %/% 100*60 + arr_time %% 100) %% 1440,\n         air_time_diff = air_time - (arr_time + dep_time)) %>% \n  filter(air_time_diff != 0)\n\n\n# A tibble: 327,344 x 21\n    year month   day dep_time sched_dep_time dep_delay arr_time\n   <int> <int> <int>    <int>          <int>     <dbl>    <dbl>\n 1  2013     1     1      517            515         2      510\n 2  2013     1     1      533            529         4      530\n 3  2013     1     1      542            540         2      563\n 4  2013     1     1      544            545        -1      604\n 5  2013     1     1      554            600        -6      492\n 6  2013     1     1      554            558        -4      460\n 7  2013     1     1      555            600        -5      553\n 8  2013     1     1      557            600        -3      429\n 9  2013     1     1      557            600        -3      518\n10  2013     1     1      558            600        -2      473\n# … with 327,334 more rows, and 14 more variables:\n#   sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#   flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>,\n#   time_hour <dttm>,\n#   (dep_time = dep_time%/%100 * 60 + dep_time%/%100)%%1440 <dbl>,\n#   air_time_diff <dbl>\n\n# check if air_time_diff = 0 \n\n\n\nThere are many flights in which there was a difference. This could be because some flights passed midnight, or crossed time zones.\nLearn summarize(), group_by()\nTo find out mean delay for each month in each year:\n\n\nflights %>% \n  group_by( year, month, day) %>% \n  summarize(delay = mean(dep_delay, na.rm = T))\n\n\n# A tibble: 365 x 4\n# Groups:   year, month [12]\n    year month   day delay\n   <int> <int> <int> <dbl>\n 1  2013     1     1 11.5 \n 2  2013     1     2 13.9 \n 3  2013     1     3 11.0 \n 4  2013     1     4  8.95\n 5  2013     1     5  5.73\n 6  2013     1     6  7.15\n 7  2013     1     7  5.42\n 8  2013     1     8  2.55\n 9  2013     1     9  2.28\n10  2013     1    10  2.84\n# … with 355 more rows\n\nTo explore the relationship between distance and average delay for each location:\n\n\nflights %>% \n  group_by(dest) %>% \n  summarize(count = n(),\n            dist = round(mean(distance, na.rm = T), 2),\n            delay = round(mean(arr_delay, na.rm = T), 2)) %>% \n  filter(count > 20, dest != \"HNL\") %>% \n  ggplot(aes(x = dist, y = delay)) +\n  geom_point(aes(size = count, alpha = 0.4)) +\n  geom_smooth(se = F) +\n  theme_classic() +\n  theme(legend.position = \"none\")\n\n\n\n\nReferences:\nhttps://jrnold.github.io/r4ds-exercise-solutions/ https://r4ds.had.co.nz/\n\n\n\n",
    "preview": "posts/20210424_Tidyverse Chap 1 - Data visualization/01---Data-visualization_files/figure-html5/unnamed-chunk-26-1.png",
    "last_modified": "2021-05-01T22:46:16+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210421_DOE Full Factorial Another Example/",
    "title": "Design of Experiment - Full Factorial",
    "description": "2ˆk Factorial Design - Another Example",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-04-21",
    "categories": [],
    "contents": "\nSource\nPreviously, I worked on a simple example for full factorial design. Let me try to replicate the results on the application of full factorial design from the link: http://pen.ius.edu.ba/index.php/pen/article/viewFile/145/175\nPackages\n\n\nlibrary(pacman)\np_load(tidyverse, AlgDesign, ggthemes, ggfortify, DoE.base, FrF2)\n\n\n\nBackground\nIn this example DOE is applied on injection-molding process with the aim of improving product quality such as excessive flash.\nFactors considered as affecting for flash formation are:\npack pressure (A),\npack time (B),\ninjection speed (C),\nscrew RPM (D),\nwhile clamping pressure, injection pressure and melting temperature were under control. Each factor affecting flash formation is considered at low and high levels.\nGenerating the design matrix\n\n\nrep_1 <- gen.factorial(c(2,2,2, 2), # number of levels for the variables\n              nVars = 4, # number of variables, in this case it is 3\n              varNames = c(\"X1_pack_pressure\", \"X2_table_speed\", \n                           \"X3_inject_speed\", \"X4_screw_rpm\"))\n\nrep_1\n\n\n   X1_pack_pressure X2_table_speed X3_inject_speed X4_screw_rpm\n1                -1             -1              -1           -1\n2                 1             -1              -1           -1\n3                -1              1              -1           -1\n4                 1              1              -1           -1\n5                -1             -1               1           -1\n6                 1             -1               1           -1\n7                -1              1               1           -1\n8                 1              1               1           -1\n9                -1             -1              -1            1\n10                1             -1              -1            1\n11               -1              1              -1            1\n12                1              1              -1            1\n13               -1             -1               1            1\n14                1             -1               1            1\n15               -1              1               1            1\n16                1              1               1            1\n\nThe design matrix may be exported out to excel for keying in of results (Y: outcome = flash size in mm, a measure of flash formation)\nAfter carrying out the experiments in randomized order, the outcome may be keyed into the same excel file and imported back into Rstudio for analysis.\nOtherwise, the results can also be keyed in manually.\n\n\n# Adding in of outcome, creating a new column called \"Y_flash\" \n\nrep_1$Y_flash <- c(0.22,\n                   6.18,\n                   0,\n                   5.91,\n                   6.6,\n                   6.05,\n                   6.76,\n                   8.65,\n                   0.46,\n                   5.06,\n                   0.55,\n                   4.84,\n                   11.55,\n                   9.9,\n                   9.9,\n                   9.9)\n\nrep_1\n\n\n   X1_pack_pressure X2_table_speed X3_inject_speed X4_screw_rpm\n1                -1             -1              -1           -1\n2                 1             -1              -1           -1\n3                -1              1              -1           -1\n4                 1              1              -1           -1\n5                -1             -1               1           -1\n6                 1             -1               1           -1\n7                -1              1               1           -1\n8                 1              1               1           -1\n9                -1             -1              -1            1\n10                1             -1              -1            1\n11               -1              1              -1            1\n12                1              1              -1            1\n13               -1             -1               1            1\n14                1             -1               1            1\n15               -1              1               1            1\n16                1              1               1            1\n   Y_flash\n1     0.22\n2     6.18\n3     0.00\n4     5.91\n5     6.60\n6     6.05\n7     6.76\n8     8.65\n9     0.46\n10    5.06\n11    0.55\n12    4.84\n13   11.55\n14    9.90\n15    9.90\n16    9.90\n\nVisualization\nPreviously, I created the main effect boxplot using ggplot2. The ggplot2 packages gives me more freedom in terms of customization, and also shows the confidence intervals. The plot below would allow me to see more clearly what is the effect of increasing each X variable independently on the outcome.\n\n\nrep_1 %>% \n  pivot_longer(cols = c(starts_with(\"X\")),\n               names_to = \"X_variables\",\n               values_to = \"X_values\") %>% \n  ggplot(aes(x = factor(X_values), y = Y_flash)) +\n  geom_boxplot(aes(x = factor(X_values), y = Y_flash, fill = X_variables)) +\n  scale_fill_few() +\n  geom_point(col = \"darkgrey\", alpha = 0.8) +\n  facet_grid(~ X_variables) +\n  labs(x = \"\",\n       title = \"Plot of the main effects showing the outcome for each factor.\",\n       caption = \"http://pen.ius.edu.ba/index.php/pen/article/viewFile/145/175\") +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n\nAn increase in X1, X3 and X4 will lead to an increase in Y. The greatest increase in Y is due to an increase in X3.\nThere is another package, FrF2, which allows me to create the main effect plot and interactions plot easily. However, the input would be a linear model, so let me build the model first.\nModelling\n\n\nmodel_interactions <- lm(Y_flash ~ (.)^4, data = rep_1) # use (.) to indicate all X variables\n\nsummary(model_interactions) \n\n\n\nCall:\nlm.default(formula = Y_flash ~ (.)^4, data = rep_1)\n\nResiduals:\nALL 16 residuals are 0: no residual degrees of freedom!\n\nCoefficients:\n                                                              Estimate\n(Intercept)                                                   5.783125\nX1_pack_pressure                                              1.278125\nX2_table_speed                                                0.030625\nX3_inject_speed                                               2.880625\nX4_screw_rpm                                                  0.736875\nX1_pack_pressure:X2_table_speed                               0.233125\nX1_pack_pressure:X3_inject_speed                             -1.316875\nX1_pack_pressure:X4_screw_rpm                                -0.373125\nX2_table_speed:X3_inject_speed                                0.108125\nX2_table_speed:X4_screw_rpm                                  -0.253125\nX3_inject_speed:X4_screw_rpm                                  0.911875\nX1_pack_pressure:X2_table_speed:X3_inject_speed               0.278125\nX1_pack_pressure:X2_table_speed:X4_screw_rpm                 -0.065625\nX1_pack_pressure:X3_inject_speed:X4_screw_rpm                -0.000625\nX2_table_speed:X3_inject_speed:X4_screw_rpm                  -0.298125\nX1_pack_pressure:X2_table_speed:X3_inject_speed:X4_screw_rpm -0.033125\n                                                             Std. Error\n(Intercept)                                                          NA\nX1_pack_pressure                                                     NA\nX2_table_speed                                                       NA\nX3_inject_speed                                                      NA\nX4_screw_rpm                                                         NA\nX1_pack_pressure:X2_table_speed                                      NA\nX1_pack_pressure:X3_inject_speed                                     NA\nX1_pack_pressure:X4_screw_rpm                                        NA\nX2_table_speed:X3_inject_speed                                       NA\nX2_table_speed:X4_screw_rpm                                          NA\nX3_inject_speed:X4_screw_rpm                                         NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed                      NA\nX1_pack_pressure:X2_table_speed:X4_screw_rpm                         NA\nX1_pack_pressure:X3_inject_speed:X4_screw_rpm                        NA\nX2_table_speed:X3_inject_speed:X4_screw_rpm                          NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed:X4_screw_rpm         NA\n                                                             t value\n(Intercept)                                                       NA\nX1_pack_pressure                                                  NA\nX2_table_speed                                                    NA\nX3_inject_speed                                                   NA\nX4_screw_rpm                                                      NA\nX1_pack_pressure:X2_table_speed                                   NA\nX1_pack_pressure:X3_inject_speed                                  NA\nX1_pack_pressure:X4_screw_rpm                                     NA\nX2_table_speed:X3_inject_speed                                    NA\nX2_table_speed:X4_screw_rpm                                       NA\nX3_inject_speed:X4_screw_rpm                                      NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed                   NA\nX1_pack_pressure:X2_table_speed:X4_screw_rpm                      NA\nX1_pack_pressure:X3_inject_speed:X4_screw_rpm                     NA\nX2_table_speed:X3_inject_speed:X4_screw_rpm                       NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed:X4_screw_rpm      NA\n                                                             Pr(>|t|)\n(Intercept)                                                        NA\nX1_pack_pressure                                                   NA\nX2_table_speed                                                     NA\nX3_inject_speed                                                    NA\nX4_screw_rpm                                                       NA\nX1_pack_pressure:X2_table_speed                                    NA\nX1_pack_pressure:X3_inject_speed                                   NA\nX1_pack_pressure:X4_screw_rpm                                      NA\nX2_table_speed:X3_inject_speed                                     NA\nX2_table_speed:X4_screw_rpm                                        NA\nX3_inject_speed:X4_screw_rpm                                       NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed                    NA\nX1_pack_pressure:X2_table_speed:X4_screw_rpm                       NA\nX1_pack_pressure:X3_inject_speed:X4_screw_rpm                      NA\nX2_table_speed:X3_inject_speed:X4_screw_rpm                        NA\nX1_pack_pressure:X2_table_speed:X3_inject_speed:X4_screw_rpm       NA\n\nResidual standard error: NaN on 0 degrees of freedom\nMultiple R-squared:      1, Adjusted R-squared:    NaN \nF-statistic:   NaN on 15 and 0 DF,  p-value: NA\n\nThere is a lot of NA in the model summary. That is because there is not enough degree of freedom to calculate the p-value. However, we can still use the estimates to plot the Pareto chart. The Pareto chart shows the absolute values of the estimates, from the largest effect to the smallest effect, so that you can pinpoint the factor that has the greatest effect on the outcome at a look.\nLet’s visualize:\n\n\npid::paretoPlot(model_interactions) +\n  theme(legend.position = \"bottom\")\n\n\n\n\nFrom above, X3 has the greatest effect on outcome, so it is critical that X3 is low so that Y will be minimised.\nThe Pareto chart shows the descending order of factors with an effect on the outcome. How do we tell if the factors are significant?\nAn informal way to find out is to use the half normal probability plot. Factors that are significant deviate from the straight line and will be labellled out.\n\n\nglimpse(rep_1)\n\n\nRows: 16\nColumns: 5\n$ X1_pack_pressure <dbl> -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -…\n$ X2_table_speed   <dbl> -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -…\n$ X3_inject_speed  <dbl> -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1,…\n$ X4_screw_rpm     <dbl> -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1,…\n$ Y_flash          <dbl> 0.22, 6.18, 0.00, 5.91, 6.60, 6.05, 6.76, 8…\n\nDoE.base::halfnormal(model_interactions) # DoE.base package\n\n\n\n\nFrom the plot above, X1, X3, X1X3 and X3X4 showed up as significant terms.\n\n\nFrF2::MEPlot(model_interactions)\n\n\n\n\nX2 is not a significant variable in affecting flash size, and can be removed from the model.\n\n\nmodel_three_factors <- lm(Y_flash ~ X1_pack_pressure * X3_inject_speed * X4_screw_rpm, \n                          data = rep_1)\n\ngvlma::gvlma(model_three_factors) # meets assumptions\n\n\n\nCall:\nlm.default(formula = Y_flash ~ X1_pack_pressure * X3_inject_speed * \n    X4_screw_rpm, data = rep_1)\n\nCoefficients:\n                                  (Intercept)  \n                                     5.783125  \n                             X1_pack_pressure  \n                                     1.278125  \n                              X3_inject_speed  \n                                     2.880625  \n                                 X4_screw_rpm  \n                                     0.736875  \n             X1_pack_pressure:X3_inject_speed  \n                                    -1.316875  \n                X1_pack_pressure:X4_screw_rpm  \n                                    -0.373125  \n                 X3_inject_speed:X4_screw_rpm  \n                                     0.911875  \nX1_pack_pressure:X3_inject_speed:X4_screw_rpm  \n                                    -0.000625  \n\n\nASSESSMENT OF THE LINEAR MODEL ASSUMPTIONS\nUSING THE GLOBAL TEST ON 4 DEGREES-OF-FREEDOM:\nLevel of Significance =  0.05 \n\nCall:\n gvlma::gvlma(x = model_three_factors) \n\n                       Value p-value                Decision\nGlobal Stat        1.630e+00  0.8033 Assumptions acceptable.\nSkewness           4.984e-35  1.0000 Assumptions acceptable.\nKurtosis           1.560e+00  0.2117 Assumptions acceptable.\nLink Function      2.478e-16  1.0000 Assumptions acceptable.\nHeteroscedasticity 7.036e-02  0.7908 Assumptions acceptable.\n\nDoE.base::halfnormal(model_three_factors) # this is an informal way, and we should rely on the linear regression summary to confirm which are the significant factors. \n\n\n\nsummary(model_three_factors)\n\n\n\nCall:\nlm.default(formula = Y_flash ~ X1_pack_pressure * X3_inject_speed * \n    X4_screw_rpm, data = rep_1)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n -1.30  -0.11   0.00   0.11   1.30 \n\nCoefficients:\n                                               Estimate Std. Error\n(Intercept)                                    5.783125   0.194514\nX1_pack_pressure                               1.278125   0.194514\nX3_inject_speed                                2.880625   0.194514\nX4_screw_rpm                                   0.736875   0.194514\nX1_pack_pressure:X3_inject_speed              -1.316875   0.194514\nX1_pack_pressure:X4_screw_rpm                 -0.373125   0.194514\nX3_inject_speed:X4_screw_rpm                   0.911875   0.194514\nX1_pack_pressure:X3_inject_speed:X4_screw_rpm -0.000625   0.194514\n                                              t value Pr(>|t|)    \n(Intercept)                                    29.731 1.78e-09 ***\nX1_pack_pressure                                6.571 0.000175 ***\nX3_inject_speed                                14.809 4.25e-07 ***\nX4_screw_rpm                                    3.788 0.005325 ** \nX1_pack_pressure:X3_inject_speed               -6.770 0.000142 ***\nX1_pack_pressure:X4_screw_rpm                  -1.918 0.091362 .  \nX3_inject_speed:X4_screw_rpm                    4.688 0.001566 ** \nX1_pack_pressure:X3_inject_speed:X4_screw_rpm  -0.003 0.997515    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7781 on 8 degrees of freedom\nMultiple R-squared:  0.9775,    Adjusted R-squared:  0.9579 \nF-statistic: 49.76 on 7 and 8 DF,  p-value: 5.696e-06\n\n\n\npid::paretoPlot(model_three_factors) # X3 has the greatest effect on Y\n\n\n\nFrF2::MEPlot(model_three_factors) # X3 has the greatest effect on outcome\n\n\n\n\nThe three main effects that are positive are X1, X3 and X4; indicating that the molding process should be performed at low level for these three variables to minimize flash size (ie have better product quality).\nLet’s visualize the interactions\n\n\nwith(rep_1, {\ninteraction.plot(x.factor     = X1_pack_pressure,\n                 trace.factor = X3_inject_speed,\n                 response     = Y_flash,\n                 fun = mean,\n                 type=\"b\",\n                 col=c(\"black\",\"red\",\"green\"),  ### Colors for levels of trace var.\n                 pch=c(19, 17, 15),             ### Symbols for levels of trace var.\n                 fixed=TRUE,                    ### Order by factor order in data\n                 leg.bty = \"o\")\n})\n\n\n\nwith(rep_1, {\ninteraction.plot(x.factor     = X3_inject_speed,\n                 trace.factor = X4_screw_rpm,\n                 response     = Y_flash,\n                 fun = mean,\n                 type=\"b\",\n                 col=c(\"black\",\"red\",\"green\"),  ### Colors for levels of trace var.\n                 pch=c(19, 17, 15),             ### Symbols for levels of trace var.\n                 fixed=TRUE,                    ### Order by factor order in data\n                 leg.bty = \"o\")\n})\n\n\n\n\nAs mentioned earlier, the FrF2 package allows me to visualize the main effect and interactions easily,\n\n\nFrF2::IAPlot(model_three_factors)\n\n\n\n\nInteraction effects occur when the effect of one variable depends on the value of another variable. It is important to take into interactions when doing linear regression. From the model summary above, X1X3 and X3X4 are significant interaction terms. This means that the outcome, Y, depends on both X1 and X3, and X3 and X4, and not just individual factors alone. Y will increase when X3 is increased, but will increase even higher if X4 is also increased.\nLearning points\nThere are many packages that can be used for DOE in R. I used FrF2 functions this time to create main effect plots and interaction plots. The workflow is more or less similar to the previous post, and I learnt how half normal probability plots can be used as an informal way to gauge which factors are significant.\nReferences\nhttp://pen.ius.edu.ba/index.php/pen/article/viewFile/145/175\n\n\n\n",
    "preview": "posts/20210421_DOE Full Factorial Another Example/DOE-Full-Factorial---Another-Example_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-04-23T22:47:12+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210414_DOE Full Factorial/",
    "title": "Design of Experiment - Full Factorial",
    "description": "2ˆk Factorial Design - Which factors matter?",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-04-14",
    "categories": [],
    "contents": "\nIntroduction\nDesign of Experiment (DOE) is something I wished I had the chance to learn in university or at my workplace when I was younger, it would have made me a more effective scientist. I took an “Advance Design of Experiment” course with Singapore Quality Institute (SQI) last year, and had a brief introduction to how DOE works. The instructor was using Minitab for his course, and I want to translate it to something that I can do with R.\nExperimental design is important for:\nidentifying the factors which may affect the result of an experiment\ndesigning the experiment such that the effects of uncontrolled factors are minimised\nusing statistical analysis to evaluate the effects of factors involved\n((Miller and Miller 2005))\nIt is important to define clearly the purpose of the experiment before you can choose the type of experimental design.\nScreening experiments: There may be many factors involved, and in such cases, it is better to go for fractional factorial designs rather than full factorial designs.\nCharacterization experiments: The aim of such experiments is to pinpoint the effect of each factor, and to understand if there are any interactions. This is usually done after screening, and there are only 2-3 factors left for consideration. In such cases, the full factorial design may be used.\nOptimization experiments: After identifying the factors and interactions of factors, it is desired to determine the combination of factor levels that will provide the optimum response. For such cases, one would have to look at response surface methodology.\nDOE approach vs One Variable At a Time Approach\nThe advantages of using a DOE approach as compared to a OVAT approach are outlined below:(Marini 2013)\nDOE approach gives a global view of the factors, whereas OVAT approach only looks at the effect of each factor locally.\nDOE approach gives a higher quality of information.\nDOE approach is more effective and requires less number of experiments.\nDOE approach takes into account the interactions among the variables, whereas OVAT does not. Interactions means that the outcome is not solely dependent on one variable, and more than one variable would affect the outcome. For example, the best cooking time for a cookie depends on the oven temperature and size of dough. If we only looked at one variable and neglect the other, we are not studying the outcome “with a global perspective.”\nThe full factorial designs are the simplest possible designs, and as a start, I will practice with it first.\nFull Factorial Design (2ˆk)\nFull factorial design requires 2ˆk number of experiments, where k refers to the number of variables in the study. As such, it is not practical to look at more than three factors as the number of experiments will exponentially increase. This is suitable for characterization of variables, and is usually done after screening of variables.\nWorkflow:\nPlan the experiments: define the factors, ranges, number of replicates\nPerform the experiments in a randomized manner\nAnalyse the data by modelling and visualization\nLoad packages\n\n\nlibrary(pacman)\np_load(tidyverse, AlgDesign, ggfortify, jtools, ggstance, interactions,\n       DoE.base, ggthemes, pid)\n\n\n\nCase study\nLet me use the worked example in https://www.itl.nist.gov/div898/handbook/pri/section3/pri3331.htm\nOutcome: Product uniformity\nFactors: Pressure (X1), Table Speed (X2), Down force (X3) No. of replicates: 2\nTotal number of runs: 2^3 x 2 = 16 runs.\nGenerating the design plan\nAlgDesign package\nThe gen.factorial() function is easy to understand, but does not allow for creating replicates and randomizing the order.\n\n\n# from AlgDesign package\nrep_1 <- gen.factorial(c(2,2,2), # number of levels for the variables\n              nVars = 3, # number of variables, in this case it is 3\n              varNames = c(\"X1_pressure\", \"X2_table_speed\", \"X3_down_force\"))\n\nrep_1\n\n\n  X1_pressure X2_table_speed X3_down_force\n1          -1             -1            -1\n2           1             -1            -1\n3          -1              1            -1\n4           1              1            -1\n5          -1             -1             1\n6           1             -1             1\n7          -1              1             1\n8           1              1             1\n\n# to create two sets of replicates\nrep_both <- rbind(rep_1, rep_1) \n\nrep_both\n\n\n   X1_pressure X2_table_speed X3_down_force\n1           -1             -1            -1\n2            1             -1            -1\n3           -1              1            -1\n4            1              1            -1\n5           -1             -1             1\n6            1             -1             1\n7           -1              1             1\n8            1              1             1\n9           -1             -1            -1\n10           1             -1            -1\n11          -1              1            -1\n12           1              1            -1\n13          -1             -1             1\n14           1             -1             1\n15          -1              1             1\n16           1              1             1\n\n# to randomize the order of experiments, use the function: sample()\n\nglimpse(rep_both)\n\n\nRows: 16\nColumns: 3\n$ X1_pressure    <dbl> -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1,…\n$ X2_table_speed <dbl> -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1,…\n$ X3_down_force  <dbl> -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1…\n\n# randomize all rows in dataframe and rename created column as \"order\"\nrep_both$order <- sample(1:16) \n\nrep_both <- rep_both %>% \n  as_tibble() %>% \n  dplyr::select(order, everything())\n\nglimpse(rep_both)\n\n\nRows: 16\nColumns: 4\n$ order          <int> 6, 16, 13, 4, 1, 2, 3, 10, 7, 14, 8, 5, 15, 1…\n$ X1_pressure    <dbl> -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1,…\n$ X2_table_speed <dbl> -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1,…\n$ X3_down_force  <dbl> -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1…\n\nAdding outcome to the dataframe\n\n\nrep_both$outcome <- c(-3,0,-1,2,-1,2,1,6,\n                      -1,-1,0,3,0,1,1,5)\n\nrep_both\n\n\n# A tibble: 16 x 5\n   order X1_pressure X2_table_speed X3_down_force outcome\n   <int>       <dbl>          <dbl>         <dbl>   <dbl>\n 1     6          -1             -1            -1      -3\n 2    16           1             -1            -1       0\n 3    13          -1              1            -1      -1\n 4     4           1              1            -1       2\n 5     1          -1             -1             1      -1\n 6     2           1             -1             1       2\n 7     3          -1              1             1       1\n 8    10           1              1             1       6\n 9     7          -1             -1            -1      -1\n10    14           1             -1            -1      -1\n11     8          -1              1            -1       0\n12     5           1              1            -1       3\n13    15          -1             -1             1       0\n14    12           1             -1             1       1\n15     9          -1              1             1       1\n16    11           1              1             1       5\n\nVisualization\n\n\nglimpse(rep_both)\n\n\nRows: 16\nColumns: 5\n$ order          <int> 6, 16, 13, 4, 1, 2, 3, 10, 7, 14, 8, 5, 15, 1…\n$ X1_pressure    <dbl> -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1,…\n$ X2_table_speed <dbl> -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1,…\n$ X3_down_force  <dbl> -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1…\n$ outcome        <dbl> -3, 0, -1, 2, -1, 2, 1, 6, -1, -1, 0, 3, 0, 1…\n\n# boxplot for all data\n\nrep_both %>% \n  dplyr::select(-order) %>% \n  pivot_longer(cols = c(starts_with(\"X\")),\n               names_to = \"X_variables\",\n               values_to = \"X_values\") %>% \n  ggplot(aes(x = factor(X_values), y = outcome)) +\n  geom_boxplot(aes(x = factor(X_values), y = outcome, fill = X_variables)) +\n  scale_fill_few() +\n  geom_point(col = \"darkgrey\", alpha = 0.5) +\n  facet_grid(~ X_variables) +\n  labs(x = \"\",\n       title = \"Plot of the main effects showing the outcome for each factor.\",\n       caption = \"Source: http://www.itl.nist.gov/div898/handbook/\") +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n\nModelling - Multiple Linear Regression\n\n\nglimpse(rep_both)\n\n\nRows: 16\nColumns: 5\n$ order          <int> 6, 16, 13, 4, 1, 2, 3, 10, 7, 14, 8, 5, 15, 1…\n$ X1_pressure    <dbl> -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1, 1, -1,…\n$ X2_table_speed <dbl> -1, -1, 1, 1, -1, -1, 1, 1, -1, -1, 1, 1, -1,…\n$ X3_down_force  <dbl> -1, -1, -1, -1, 1, 1, 1, 1, -1, -1, -1, -1, 1…\n$ outcome        <dbl> -3, 0, -1, 2, -1, 2, 1, 6, -1, -1, 0, 3, 0, 1…\n\nmodel <- lm(outcome ~ X1_pressure * X2_table_speed * X3_down_force,\n            data = rep_both)\n\n# checking diagnostic plots for normality using ggfortify\n\nautoplot(model)\n\n\n\n\nTaken from: http://www.sthda.com/english/articles/39-regression-model-diagnostics/161-linear-regression-assumptions-and-diagnostics-in-r-essentials/\nThe diagnostic plots show residuals in four different ways:\nResiduals vs Fitted. Used to check the linear relationship assumptions. A horizontal line, without distinct patterns is an indication for a linear relationship, what is good.\nNormal Q-Q. Used to examine whether the residuals are normally distributed. It’s good if residuals points follow the straight dashed line.\nScale-Location (or Spread-Location). Used to check the homogeneity of variance of the residuals (homoscedasticity). Horizontal line with equally spread points is a good indication of homoscedasticity. This is not the case in our example, where we have a heteroscedasticity problem.\nResiduals vs Leverage. Used to identify influential cases, that is extreme values that might influence the regression results when included or excluded from the analysis.\nInterpretation of model results\n\n\nsummary(model)\n\n\n\nCall:\nlm.default(formula = outcome ~ X1_pressure * X2_table_speed * \n    X3_down_force, data = rep_both)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n  -1.0   -0.5    0.0    0.5    1.0 \n\nCoefficients:\n                                         Estimate Std. Error t value\n(Intercept)                                0.8750     0.1976   4.427\nX1_pressure                                1.3750     0.1976   6.957\nX2_table_speed                             1.2500     0.1976   6.325\nX3_down_force                              1.0000     0.1976   5.060\nX1_pressure:X2_table_speed                 0.5000     0.1976   2.530\nX1_pressure:X3_down_force                  0.2500     0.1976   1.265\nX2_table_speed:X3_down_force               0.1250     0.1976   0.632\nX1_pressure:X2_table_speed:X3_down_force   0.1250     0.1976   0.632\n                                         Pr(>|t|)    \n(Intercept)                              0.002205 ** \nX1_pressure                              0.000118 ***\nX2_table_speed                           0.000227 ***\nX3_down_force                            0.000977 ***\nX1_pressure:X2_table_speed               0.035265 *  \nX1_pressure:X3_down_force                0.241504    \nX2_table_speed:X3_down_force             0.544737    \nX1_pressure:X2_table_speed:X3_down_force 0.544737    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7906 on 8 degrees of freedom\nMultiple R-squared:  0.9388,    Adjusted R-squared:  0.8853 \nF-statistic: 17.54 on 7 and 8 DF,  p-value: 0.0002897\n\nAnother way to look at model information using jtools package\n\n\nsumm(model)\n\n\n\nObservations\n\n\n16\n\n\nDependent variable\n\n\noutcome\n\n\nType\n\n\nOLS linear regression\n\n\nF(7,8)\n\n\n17.54\n\n\nR²\n\n\n0.94\n\n\nAdj. R²\n\n\n0.89\n\n\n\n\nEst.\n\n\nS.E.\n\n\nt val.\n\n\np\n\n\n(Intercept)\n\n\n0.88\n\n\n0.20\n\n\n4.43\n\n\n0.00\n\n\nX1_pressure\n\n\n1.38\n\n\n0.20\n\n\n6.96\n\n\n0.00\n\n\nX2_table_speed\n\n\n1.25\n\n\n0.20\n\n\n6.32\n\n\n0.00\n\n\nX3_down_force\n\n\n1.00\n\n\n0.20\n\n\n5.06\n\n\n0.00\n\n\nX1_pressure:X2_table_speed\n\n\n0.50\n\n\n0.20\n\n\n2.53\n\n\n0.04\n\n\nX1_pressure:X3_down_force\n\n\n0.25\n\n\n0.20\n\n\n1.26\n\n\n0.24\n\n\nX2_table_speed:X3_down_force\n\n\n0.12\n\n\n0.20\n\n\n0.63\n\n\n0.54\n\n\nX1_pressure:X2_table_speed:X3_down_force\n\n\n0.12\n\n\n0.20\n\n\n0.63\n\n\n0.54\n\n\n Standard errors: OLS\n\n\nKey points to take note of:\nThe adjusted R-squared value is 0.8853, meaning 88.5% of the variability in outcome is explained by the X variables.\nAll X variables are significant. This should be the case because for full factorial design, we only want to concentrate on characterization of important variables. If this was a screening exercise, variables that are not significant would not be important variables in the experiment.\nThe estimate shows the effect size. When there is an increase in X1, X2, X3, there will be an increase in Y. When X1 increases by 1 unit, Y will increase by 1.375 unit. When X2 increases by 1 unit, Y will increase by 1.25 unit. When X3 increases by 1 unit, Y also increases by 1 unit.\nThe interaction term between X1_pressure and X2_table speed is also significant.\nA better way to visualize is to look at the main effect plot and interaction plot.\nMain Effect Plot\n\n\nplot_summs(model)\n\n\n\n\nAnother way of looking at model coefficients using the pid package:\n\n\npid::paretoPlot(model)\n\n\n\n\nProbing interactions\n\n\ni_1 <- interact_plot(model,\n              pred = X1_pressure,\n              modx = X2_table_speed)\n\ni_2 <- interact_plot(model,\n              pred = X1_pressure,\n              modx = X3_down_force)\n\ni_3 <- interact_plot(model,\n              pred = X2_table_speed,\n              modx = X3_down_force)\n\ni_1\n\n\n\ni_2\n\n\n\ni_3\n\n\n\n\nIn this case, there is interaction, but not within the selected range for X1_pressure. When there is an increase in X1 and X2, Y will also increase.\nThere is no interaction between X1 and X3, and X2 and X3.\nConcluding remarks\nThis exercise allowed me to go through the workflow for planning and analysing the results of a simple full factorial DOE. There are many packages in the R ecosystem that allows for DOE design planning, results interpretation and visualization, and it was fun trying to explore the different functions in various packages.\nAs a next step, I will try to find a more food science related example to work on, ideally one in which there are interactions between factors.\nReferences\nhttps://www.r-bloggers.com/2009/12/design-of-experiments-%E2%80%93-full-factorial-designs/\nhttps://www.itl.nist.gov/div898/handbook/pri/section3/pri3331.htm\nhttps://www.itl.nist.gov/div898/handbook/pri/section4/pri471.htm\n\n\n\nMarini, Federico, ed. 2013. Chemometrics in Food Chemistry. First edition. Data Handling in Science and Technology, volume 28. Amsterdam ; Boston: Elsevier.\n\n\nMiller, James N., and Jane Charlotte Miller. 2005. Statistics and Chemometrics for Analytical Chemistry. Pearson/Prentice Hall.\n\n\n\n\n",
    "preview": "posts/20210414_DOE Full Factorial/DOE-Full-Factorial_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-04-15T21:04:07+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210405_Outliers/",
    "title": "Analytical Chemistry - Comparing Mean, Variance and Detecting Outliers",
    "description": "Using Statistical Tests in Analytical Chemistry",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-04-05",
    "categories": [],
    "contents": "\nBackground\nI was looking at the titration results for our existing titration method today. Our Quality colleagues wanted to minimise variability in lab measurements for a certain analyte, and there was a newly developed method with improved sample preparation steps developed by the lab in another site. We decided to carry out a study locally to check on the variability for the existing method, and also to compare with the improved method.\nMy two interns gave me a series of 5-6 readings done by each of them, and in total I was looking at 11 readings. Some questions I had were:\nWas there any difference in the readings done by the two interns?\nWere the readings different from the true value?\nOne of the readings look quite low, was that an outlier?\nAt a later stage, I would want to compare the existing method with the new method. What metrics should I be looking at?\nHow am I to find out the answers in a statistically sound manner?\nLoading packages\n\n\nlibrary(pacman)\np_load(tidyverse, outliers, ggthemes, ggstatsplot, ggpubr, car)\n\n\n\nData:\nAs I can’t put the company data up online, let me use the various worked examples I found on: http://dpuadweb.depauw.edu/harvey_web/eTextProject/pdfFiles/Chapter14.pdf.\nAll data were sourced from the link above.\nComparing Sulfanilamide analysis results done by four different analysts\nThe data below shows the determination of the percentage purity of a sulfanilamide preparation by four analysts:\n\n\na <- c(94.09, 94.64, 95.08, 94.54, 95.38, 93.62)\nb <- c(99.55, 98.24, 101.1, 100.4, 100.1, NA)\nc <- c(95.14, 94.62, 95.28, 94.59, 94.24, NA)\nd <- c(93.88, 94.23, 96.05, 93.89, 94.95, 95.49)\n\nresults <- cbind(a,b,c,d) %>% \n  as_tibble()\n\nglimpse(results)\n\n\nRows: 6\nColumns: 4\n$ a <dbl> 94.09, 94.64, 95.08, 94.54, 95.38, 93.62\n$ b <dbl> 99.55, 98.24, 101.10, 100.40, 100.10, NA\n$ c <dbl> 95.14, 94.62, 95.28, 94.59, 94.24, NA\n$ d <dbl> 93.88, 94.23, 96.05, 93.89, 94.95, 95.49\n\nLet’s reshape the data:\n\n\nreshaped_data <- results %>% \n  pivot_longer(everything(),\n               names_to = \"analyst\",\n               values_to = \"readings\") %>% \n  mutate(analyst = factor(analyst)) # factor instead of character\n\nglimpse(reshaped_data)\n\n\nRows: 24\nColumns: 2\n$ analyst  <fct> a, b, c, d, a, b, c, d, a, b, c, d, a, b, c, d, a, …\n$ readings <dbl> 94.09, 99.55, 95.14, 93.88, 94.64, 98.24, 94.62, 94…\n\nLet’s look at the mean and standard deviation for results for each analyst:\n\n\nreshaped_data %>% \n  group_by(analyst) %>% \n  summarise(mean = round(mean(readings, na.rm = T), 3),\n            sd = round(sd(readings, na.rm = T), 3)) # na.rm for working with missing data\n\n\n# A tibble: 4 x 3\n  analyst  mean    sd\n  <fct>   <dbl> <dbl>\n1 a        94.6 0.641\n2 b        99.9 1.07 \n3 c        94.8 0.428\n4 d        94.7 0.899\n\nreshaped_data %>% \n  group_by(analyst) %>% \n  summarise(mean = round(mean(readings, na.rm = T), 3)) %>% \n  ggplot(aes(x = analyst, y = mean, label = mean)) +\n  geom_col(fill = \"deepskyblue4\") +\n  geom_text(vjust = -0.2) +\n  labs(title = \"% Purity of a Sulfanilamide Preparation by Four Analysts\",\n       x = \"Analyst\",\n       y = \"Mean of replicates\",\n       caption = \"Source: http://dpuadweb.depauw.edu/harvey_web/eTextProject/pdfFiles/Chapter14.pdf\") +\n  ggthemes::theme_few()\n\n\n\n\nLet’s test if the results differ among the analysts:\n\n\nglimpse(reshaped_data)\n\n\nRows: 24\nColumns: 2\n$ analyst  <fct> a, b, c, d, a, b, c, d, a, b, c, d, a, b, c, d, a, …\n$ readings <dbl> 94.09, 99.55, 95.14, 93.88, 94.64, 98.24, 94.62, 94…\n\nm1 <- aov(readings ~analyst, data = reshaped_data)\nsummary(m1)\n\n\n            Df Sum Sq Mean Sq F value   Pr(>F)    \nanalyst      3 104.20   34.73   54.66 3.05e-09 ***\nResiduals   18  11.44    0.64                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n2 observations deleted due to missingness\n\nFrom the p value of 3.05 x 10^-9, there is a significant difference between the mean values for the analysts. To find out which pair is statistically different, we use the Tukey’s Honest Significant Difference method.\n\n\nTukeyHSD(m1, which = \"analyst\", ordered = F)\n\n\n  Tukey multiple comparisons of means\n    95% family-wise confidence level\n\nFit: aov(formula = readings ~ analyst, data = reshaped_data)\n\n$analyst\n           diff       lwr       upr     p adj\nb-a  5.31966667  3.955487  6.683846 0.0000000\nc-a  0.21566667 -1.148513  1.579846 0.9693980\nd-a  0.19000000 -1.110694  1.490694 0.9755572\nc-b -5.10400000 -6.528839 -3.679161 0.0000000\nd-b -5.12966667 -6.493846 -3.765487 0.0000000\nd-c -0.02566667 -1.389846  1.338513 0.9999438\n\nLet’s show the results in a visual manner:\n\n\nplot(TukeyHSD(m1, which = \"analyst\", cex.axist = 0.5))\n\n\n\n\nThe plot above shows that the readings for Analyst B is significantly higher than the other analysts.\nI learnt that there is a package in R that can carry out modelling and visualization in 1 step, which is the ggstatsplot package.\n\n\nggbetweenstats(\n  data = reshaped_data,\n  x = analyst,\n  y = readings,\n  plot.type = \"box\", \n  type = \"p\", # parametric, non-parametric, robust or bayes\n  title = \"% Purity of a Sulfanilamide Preparation by Four Analysts\",\n  ggtheme = theme_few()\n)\n\n\n\n\nDifferent statistical tests were used for this package (Games-Howell instead of Tukey HSD). Games-Howell assumes uneven variance for the data. The plot above also shows the individual data points, which is a good practice.\nComparing acid-base titration results done by two analysts\nWhat if I wanted to compare the results of two analysts? In that case, t-test should be used in place of ANOVA.\n\n\ntitration_data <- tribble(\n  ~person_a, ~person_b,\n  86.82,      81.01,\n  87.04,      86.15,\n  86.93,      81.73,\n  87.01,      83.19,\n  86.20,      80.27,\n  87.00,      83.94\n)\n\ntitration_data\n\n\n# A tibble: 6 x 2\n  person_a person_b\n     <dbl>    <dbl>\n1     86.8     81.0\n2     87.0     86.2\n3     86.9     81.7\n4     87.0     83.2\n5     86.2     80.3\n6     87       83.9\n\n# reshape the data\n\ntitration_reshaped <- titration_data %>% \n  pivot_longer(cols = everything(),\n               names_to = \"analyst\",\n               values_to = \"readings\") %>% \n  mutate(analyst = factor(analyst))\n\ntitration_reshaped\n\n\n# A tibble: 12 x 2\n   analyst  readings\n   <fct>       <dbl>\n 1 person_a     86.8\n 2 person_b     81.0\n 3 person_a     87.0\n 4 person_b     86.2\n 5 person_a     86.9\n 6 person_b     81.7\n 7 person_a     87.0\n 8 person_b     83.2\n 9 person_a     86.2\n10 person_b     80.3\n11 person_a     87  \n12 person_b     83.9\n\ntitration_reshaped %>% \n  group_by(analyst) %>% \n  summarise(mean = mean(readings),\n           sd = sd(readings))\n\n\n# A tibble: 2 x 3\n  analyst   mean    sd\n  <fct>    <dbl> <dbl>\n1 person_a  86.8 0.320\n2 person_b  82.7 2.16 \n\n# to compare the means of results by the two analysts:\n\nt.test(readings ~ analyst, data = titration_reshaped)\n\n\n\n    Welch Two Sample t-test\n\ndata:  readings by analyst\nt = 4.6147, df = 5.219, p-value = 0.005177\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n 1.852919 6.383748\nsample estimates:\nmean in group person_a mean in group person_b \n              86.83333               82.71500 \n\nThe mean titration readings obtained by Person B is significantly higher than that of Person A.\n\n\nggbetweenstats(\n  data = titration_reshaped,\n  x = analyst, \n  y = readings,\n  title = \"Comparison of titration results for determining the % w/w of sodium bicarbonate in soda ash.\"\n)\n\n\n\n\nComparing Variance\nWhat if I want to compare the variance? This is useful if I want to check if method B reduces the variability of the measure.\n\n\n# Load data that compares the mass of a coin\n\nmtd_a <- c(3.080, 3.094, 3.107, 3.056, 3.112, 3.174, 3.198)\nmtd_b <- c(3.052, 3.141, 3.083, 3.083, 3.048, NA, NA)\n\ncoin_data <- cbind(mtd_a, mtd_b) %>% \n  as_tibble() %>% \n  pivot_longer(cols = everything(),\n               names_to = \"method\",\n               values_to = \"mass_g\")\n\nglimpse(coin_data)\n\n\nRows: 14\nColumns: 2\n$ method <chr> \"mtd_a\", \"mtd_b\", \"mtd_a\", \"mtd_b\", \"mtd_a\", \"mtd_b\",…\n$ mass_g <dbl> 3.080, 3.052, 3.094, 3.141, 3.107, 3.083, 3.056, 3.08…\n\n# computing the mean, standard deviation and variance:\n\ncoin_data %>% \n  group_by(method) %>% \n  summarise(mean = mean(mass_g, na.rm = T),\n            sd = sd(mass_g, na.rm = T),\n            var = var(mass_g, na.rm = T))\n\n\n# A tibble: 2 x 4\n  method  mean     sd     var\n  <chr>  <dbl>  <dbl>   <dbl>\n1 mtd_a   3.12 0.0509 0.00259\n2 mtd_b   3.08 0.0372 0.00138\n\n# use F-test to compare the variance (assuming normal distribution)\n\nvar.test(mass_g ~ method, data = coin_data, alternative = \"two.sided\")\n\n\n\n    F test to compare two variances\n\ndata:  mass_g by method\nF = 1.8726, num df = 6, denom df = 4, p-value = 0.5661\nalternative hypothesis: true ratio of variances is not equal to 1\n95 percent confidence interval:\n  0.2036028 11.6609726\nsample estimates:\nratio of variances \n          1.872598 \n\nshapiro.test(coin_data$mass_g) # p>0.05\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  coin_data$mass_g\nW = 0.90871, p-value = 0.2054\n\nbartlett.test(mass_g ~ method, data = coin_data) # for more than two groups\n\n\n\n    Bartlett test of homogeneity of variances\n\ndata:  mass_g by method\nBartlett's K-squared = 0.4039, df = 1, p-value = 0.5251\n\n# if distribution is not normally distributed, the Levene test may be used:\n\ncar::leveneTest(mass_g ~ method, data = coin_data)  # same as above\n\n\nLevene's Test for Homogeneity of Variance (center = median)\n      Df F value Pr(>F)\ngroup  1  0.4028 0.5399\n      10               \n\nAt 95% confidence interval, there is no evidence to suggest that there is a difference in precision between the two methods.\nComparing replicate readings against true, known value\nThis example is also from the same chapter cited below, in the Reference section.\nA sample is known to have 98.76% sodium bicarbonate. Five replicate measurements were taken, and we want to find out if the analysis is giving inaccurate results.\n\n\nbicarb <- c(98.71, 98.59, 98.62, 98.44, 98.58)\nmean(bicarb)\n\n\n[1] 98.588\n\nsd(bicarb)\n\n\n[1] 0.09731393\n\n# visualize\nshapiro.test(bicarb) # follows normal distribution\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  bicarb\nW = 0.94407, p-value = 0.6949\n\nggqqplot(bicarb) # Q-Q plot\n\n\n\n# boxplot\nggboxplot(bicarb,\n                  ylab = \"readings\")\n\n\n\nt.test(bicarb, mu = 98.76, alternative = \"two.sided\")\n\n\n\n    One Sample t-test\n\ndata:  bicarb\nt = -3.9522, df = 4, p-value = 0.01679\nalternative hypothesis: true mean is not equal to 98.76\n95 percent confidence interval:\n 98.46717 98.70883\nsample estimates:\nmean of x \n   98.588 \n\nThe data suggests that the experimental data is significantly different from the known value, and that there is indeed a source of error when conducting the experiments.\nOutlier Tests\nWhen there are data-points that appear not to be consistent with the other data points, how do you determine if it is an outlier?\nOne way is by visualizing using the box-plot, and an outlier is defined as a point outside of the inter-quartile range (1.5 x IQR).\nThere are some significance tests that can be used to identify outliers, which include the Dixon’s Q-Test (not recommended by ISO), the Grubb’s Test (can be carried out using the outliers package) and the Chauvenet’s Criterion. I haven’t found a package which I can check for outliers using Chauvenet’s Criterion, and will only use the Grubb’s test below.\nGrubb’s test\nThe Grubbs test is a test used to detect outliers (assuming normally distributed data).\nUsing penny weight dataset shown below, it is of interest to test if a penny with a mass of 2.514g is an outlier datapoint.\n\n\npennies <- c(3.067, 3.049, 2.514, 3.048, 3.079, 3.094, 3.109, 3.102)\n\n# Boxplot\n\nshapiro.test(pennies) # not normally distributed (p<0.05), use grubb's test with caveat\n\n\n\n    Shapiro-Wilk normality test\n\ndata:  pennies\nW = 0.52689, p-value = 2.172e-05\n\n# Grubbs test\ngrubbs.test(pennies, type = 10, two.sided = T)\n\n\n\n    Grubbs test for one outlier\n\ndata:  pennies\nG = 2.45880, U = 0.01295, p-value = 5.456e-06\nalternative hypothesis: lowest value 2.514 is an outlier\n\nThe null hypothesis is that there is no outlier, and the alternative hypothesis is that there is an outlier. In this case, as p<0.05, 2.514 is indeed an outlier datapoint.\nLet us check again using a box-plot visualization:\n\n\nggboxplot(pennies) +\n  labs(title = \"Using the 1.5IQR rule, 2.514g penny is an outlier datapoint.\")\n\n\n\n\nLearning Points\nAll my questions I had at the beginning were answered in statistically sound manner. I got to revise the t-test, ANOVA, learnt how to carry out modelling and visualization in one step. I also deepened my analytical chemistry results interpretation by knowing whether to compare the mean or to comare the variance when assessing usability of newly developed methods.\nI learnt a new way to detect for outliers, which is the Grubb’s test. However, I am on the lookout for ways to test for outlier using the Chauvenet’s Criterion.\nReferences\nhttp://dpuadweb.depauw.edu/harvey_web/eTextProject/pdfFiles/Chapter14.pdf\nhttp://www.sthda.com/english/wiki/one-sample-t-test-in-r\nhttp://www.sthda.com/english/wiki/f-test-compare-two-variances-in-r\n\n\n\n",
    "preview": "posts/20210405_Outliers/Outliers_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-04-07T22:13:44+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210328_NIR Meat Data (PLS-regression)/",
    "title": "Meat NIR data",
    "description": "PLS regression on Meat NIR data",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-03-18",
    "categories": [],
    "contents": "\nBackground\nThis example provides the workflow for analysing NIR data spectra to predict the fat, water and protein content of meat.\nWorkflow\nExploratory Data Analysis\nPCA for dimension reduction/exploratory to determine the effective dimension of the dataset.\nSplit dataset into TRAINING and TESTING dataset. Split training dataset into cross validation dataset.\nData processing for training dataset, with the understanding the PLS regression will be used for modeling. Data should be centered and scaled, especially if the predictors are on scales of different magnitude.\nBuild model: In this case, PLS-regression will be used, due to high correlation among X variables; and also because the number of X variables is greater than the number of samples.\nAssess model for cross-validation dataset, and tune to select optimal number of PC to be retained. Resampling techniques include k-fold cross validation, Leave One Out (LOO) cross validation and bootstrapping. For large datasets, 10-fold CV would suffice. For small datasets, repeated 10-fold CV is recommended. For very small datasets (n<50), LOO is recommended.\nAssess model on TESTING dataset\nVisualization of model performance - number of components to be retained and model coefficients, observed vs predicted values (indicator of accuracy)\nFor models with more than 1 Y variable, the PLS1 approach is taken: each Y outcome has its own tuned model.\nReporting model coefficients\nAssessing variable importance, using variable importance scores. As a rule of thumb, VIP values exceeding 1 are considered to contain predictive information for the outcome, Y.\nPLS regression - Theory\nOne of the many advantages of PLS is that it can handle many noisy, collinear (correlated) and missing variables, and can also simultaneously model several response variables Y.\nThe default algorithm is the NIPALS algorithm, which seeks to find the latent (or hidden) relationships among the X variables, which are highly correlated with the response (Y outcome).\nLike PCA, PLS finds linear combinations of the predictors. These linear combinations are commonly called components, or latent variables. PLS linear combinations of X variables are chosen to maximally summarise covariance with the Y outcomes. This means that PLS finds components that maximally summarise the variation of the predictors (X) while simultaneously requiring these components to have maximum correlation with the response (Y). PLS is a supervised dimension reduction procedure, as compared to Principal Component Regression (PCR), which is an unsupervised procedure. PLS will identify the optimal predictor space dimension reduction for the purpose of regression with the outcome, Y. For PCA, the variance in X is captured as much as possible, but there may be cases in which variation in X is not related to variation in Y, or there may be high noise present, and in turn is not related to Y. Hence, Y cannot be predicted correctly since there is no relationship.\nThe NIPALS algorithm is suitable for large datasets (when number of samples, n is much greater than number of predictors, X). For scenarios where there are more predictors than samples, the algorithm by Rannar, which is a kernel based algorithm, is computationally more efficient.\nThere are two versions of PLS - PLS 1 and PLS 2. PLS 1 is used when there is only one Y outcome variable, and PLS 2 may be used when there are more than one Y outcome variables. In PLS 1, a model is build and tuned specific for a Y outcome variabe. In PLS 2, the model is tuned for more than one Y outcomes. It is preferred to tune a model specific for each Y outcome rather in practice.\nIR\nIR spectroscopy is used to determine the chemical makeup of a substance. The theory of IR spectroscopy holds that unique molecualr structures absorb IR frequencies differently. In practice, a spectrometer fires a series of IR frequencies into a sample material, and the devide measures the absorbance of the sample at each individual frequency. This series of measurements creates a spectrum profile which can then be used to determine the chemical makeup of the sample material.\nData\nA Tecator Infratec Food and Feed Analyzer instrument was used to analyse 215 samples of meat across 100 frequencies (800-1050 nm). In addition to an IR profile, the percent content of water, fat and protein for each sample was determined using analytical chemistry.\nThe objective is the establish a predictive relationship between IR spectrum and fat content, to predict a sample’s fat content with IR.\nPackages required\n\n\n# Load packages: \n\nlibrary(pacman)\n\np_load(pls, AppliedPredictiveModeling, tidyverse, modeldata, tidymodels,  janitor, skimr, caret, ggthemes, ggrepel, psych, GGally)\n\n\n\nImport data\nThese data are recorded on a Tecator Infratec Food and Feed Analyzer working in the wavelength range 850 - 1050 nm by the Near Infrared Transmission (NIT) principle. Each sample contains finely chopped pure meat with different moisture, fat and protein contents.\nFor each meat sample the data consists of a 100 channel spectrum of absorbances and the contents of moisture (water), fat and protein. The absorbance is -log10 of the transmittance measured by the spectrometer. The three contents, measured in percent, are determined by analytic chemistry.\n\n\ndata(tecator)\n\n\n\nThe matrix absorp contains the 100 absorbance values for the 215 samples The matrix endpoints contains the percent of moisture, fat, and protein in columns 1, 2, 3 respectively.\n\n\nwavelengths = seq(800,1050, length.out = 100)\n\nmatplot(wavelengths, t(absorp), type = \"l\", lty = 1,\n        ylab = \"Absorbance (-log 10 of transmittance)\")\n\n\n\n\nPCA - the base R way\n\n\nnir.prcomp <- prcomp(absorp, rank. = 8)\n\nsummary(nir.prcomp)\n\n\nImportance of first k=8 (out of 100) components:\n                          PC1     PC2     PC3     PC4     PC5     PC6\nStandard deviation     5.1115 0.48840 0.28009 0.17374 0.03903 0.02581\nProportion of Variance 0.9868 0.00901 0.00296 0.00114 0.00006 0.00003\nCumulative Proportion  0.9868 0.99580 0.99876 0.99990 0.99996 0.99999\n                           PC7     PC8\nStandard deviation     0.01433 0.01041\nProportion of Variance 0.00001 0.00000\nCumulative Proportion  0.99999 1.00000\n\nOne can see that the first component explains 98.7% of variance in X.\nScree plot\n\n\nplot(nir.prcomp, main = \"NIR Meat PCA scree plot\",\n     xlab = \"No. of PC\")\n\n\n\n\nOne can choose number of PC = 2\nLoadings\n\n\nnir.loadings <- nir.prcomp$rotation[, 1:2]\n\n# find max loading for PC 1\nnir.loadings %>% \n  as_tibble() %>% \n  rowid_to_column() %>% \n  pivot_longer(cols = starts_with(\"PC\"),\n               names_to = \"PC\",\n               values_to = \"loadings\") %>% \n  dplyr::filter(PC == \"PC1\") %>% \n  mutate(abs_loading = abs(loadings)) %>% \n  arrange(desc(abs_loading)) %>% \n  head(n = 10)\n\n\n# A tibble: 10 x 4\n   rowid PC    loadings abs_loading\n   <int> <chr>    <dbl>       <dbl>\n 1    42 PC1      0.106       0.106\n 2    41 PC1      0.106       0.106\n 3    43 PC1      0.106       0.106\n 4    67 PC1      0.106       0.106\n 5    68 PC1      0.106       0.106\n 6    69 PC1      0.106       0.106\n 7    66 PC1      0.106       0.106\n 8    70 PC1      0.106       0.106\n 9    84 PC1      0.106       0.106\n10    71 PC1      0.106       0.106\n\n# find max loading for PC 2\nnir.loadings %>% \n  as_tibble() %>% \n  rowid_to_column() %>% \n  pivot_longer(cols = starts_with(\"PC\"),\n               names_to = \"PC\",\n               values_to = \"loadings\") %>% \n  dplyr::filter(PC == \"PC2\") %>% \n  mutate(abs_loading = abs(loadings)) %>% \n  arrange(desc(abs_loading)) %>% \n  head(n = 10)\n\n\n# A tibble: 10 x 4\n   rowid PC    loadings abs_loading\n   <int> <chr>    <dbl>       <dbl>\n 1    14 PC2      0.129       0.129\n 2    15 PC2      0.129       0.129\n 3    13 PC2      0.129       0.129\n 4    16 PC2      0.129       0.129\n 5    12 PC2      0.128       0.128\n 6    17 PC2      0.128       0.128\n 7    11 PC2      0.128       0.128\n 8    18 PC2      0.127       0.127\n 9    10 PC2      0.127       0.127\n10     9 PC2      0.126       0.126\n\n# check which wavelengths\n\nwavelengths[42]\n\n\n[1] 903.5354\n\nwavelengths[68]\n\n\n[1] 969.1919\n\nwavelengths[14]\n\n\n[1] 832.8283\n\nwavelengths[17]\n\n\n[1] 840.404\n\n# Plot\noffset <- c(0, 0.009)\nplot(nir.loadings[, 1:2], type = \"l\",\n     xlim = range(nir.loadings[, 1]) + offset,\n     xlab = \"PC 1 (98.7%)\", ylab = \"PC 2 (0.009%)\")\n\npoints(nir.loadings[c(42,68, 14, 17), 1:2])\n\ntext(nir.loadings[c(42,68, 14, 17), 1:2], pos = 4,\n     labels = paste(c(903,969,832,840), \"nm\"))\n\n\n\n\nTidymodels way for PCA\nI personally prefer this method because the plots can be customised, rather than using the default base R graphics.\n\n\n# Y\nendpoints.tibble <- endpoints %>% as_tibble(.name_repair = \"unique\")\n\n# set names for Y\nnames(endpoints.tibble) <- c(\"water\", \"fat\", \"protein\")\n\n\n# X\nabsorp.tibble <- absorp %>% as_tibble(.name_repair = \"unique\") %>% \n  clean_names()\n\n# combine both datasets\ndata_meat_x <- absorp.tibble\ndata_meat_y <- endpoints.tibble\n\n# combine both datasets\ndata_meat <- cbind(endpoints.tibble, absorp.tibble) %>% \n  mutate(id = seq(1:215))\n\n\n\nChecking assumptions for PCA\n\n\n# Checking assumptions ----\n\ndata_meat_x %>% \n  cor() %>% \n  KMO() # Overal MSA = 0.97, greater than 0.70\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = .)\nOverall MSA =  0.97\nMSA for each item = \n  x1   x2   x3   x4   x5   x6   x7   x8   x9  x10  x11  x12  x13  x14 \n0.99 0.98 0.97 0.97 0.96 0.96 0.96 0.98 0.98 0.98 0.97 0.96 0.96 0.96 \n x15  x16  x17  x18  x19  x20  x21  x22  x23  x24  x25  x26  x27  x28 \n0.97 0.98 0.98 0.99 0.98 0.96 0.96 0.96 0.96 0.96 0.97 0.99 0.98 0.98 \n x29  x30  x31  x32  x33  x34  x35  x36  x37  x38  x39  x40  x41  x42 \n0.97 0.97 0.97 0.97 0.97 0.97 0.98 0.98 0.98 0.98 0.98 0.98 0.97 0.96 \n x43  x44  x45  x46  x47  x48  x49  x50  x51  x52  x53  x54  x55  x56 \n0.96 0.97 0.97 0.98 0.98 0.98 0.98 0.97 0.97 0.97 0.96 0.96 0.96 0.96 \n x57  x58  x59  x60  x61  x62  x63  x64  x65  x66  x67  x68  x69  x70 \n0.97 0.97 0.98 0.98 0.98 0.98 0.98 0.98 0.97 0.96 0.96 0.96 0.97 0.97 \n x71  x72  x73  x74  x75  x76  x77  x78  x79  x80  x81  x82  x83  x84 \n0.97 0.98 0.98 0.97 0.98 0.98 0.97 0.98 0.98 0.98 0.98 0.98 0.97 0.97 \n x85  x86  x87  x88  x89  x90  x91  x92  x93  x94  x95  x96  x97  x98 \n0.97 0.97 0.97 0.98 0.97 0.96 0.96 0.97 0.97 0.98 0.98 0.98 0.98 0.98 \n x99 x100 \n0.97 0.97 \n\ndata_meat_x %>% \n  cor() %>% \n  cortest.bartlett(., n = 215) # p < 0.05\n\n\n$chisq\n[1] Inf\n\n$p.value\n[1] 0\n\n$df\n[1] 4950\n\n# all assumptions met, ok to do PCA\n\n\n\nModel\n\n\n# tidymodels PCA -----\n\n# recipe\nmeat_pca_recipe <- recipe(~ ., data = data_meat) %>% \n  update_role(id, new_role = \"id\") %>% \n  update_role(water, fat, protein, new_role = \"outcome\") %>% \n  step_normalize(all_predictors()) %>% \n  step_pca(all_predictors(), id = \"pca\")\n\nmeat_pca_recipe\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n        id          1\n   outcome          3\n predictor        100\n\nOperations:\n\nCentering and scaling for all_predictors()\nNo PCA components were extracted.\n\n# prep\n\nmeat_pca_prep <- prep(meat_pca_recipe)\n\n# loadings\ntidy_pca_loadings <- meat_pca_prep %>% \n  tidy(id = \"pca\")\n\ntidy_pca_loadings\n\n\n# A tibble: 10,000 x 4\n   terms  value component id   \n   <chr>  <dbl> <chr>     <chr>\n 1 x1    0.0997 PC1       pca  \n 2 x2    0.0997 PC1       pca  \n 3 x3    0.0997 PC1       pca  \n 4 x4    0.0997 PC1       pca  \n 5 x5    0.0997 PC1       pca  \n 6 x6    0.0996 PC1       pca  \n 7 x7    0.0996 PC1       pca  \n 8 x8    0.0997 PC1       pca  \n 9 x9    0.0997 PC1       pca  \n10 x10   0.0997 PC1       pca  \n# … with 9,990 more rows\n\n# bake\nmeat_pca_bake <- bake(meat_pca_prep, data_meat)\nmeat_pca_bake # PCA LOADING VECTORS\n\n\n# A tibble: 215 x 9\n   water   fat protein    id    PC1    PC2     PC3      PC4     PC5\n   <dbl> <dbl>   <dbl> <int>  <dbl>  <dbl>   <dbl>    <dbl>   <dbl>\n 1  60.5  22.5    16.7     1 -4.30  -0.404 -0.137  -0.0939   0.105 \n 2  46    40.1    13.5     2  0.998  0.486 -1.20   -0.0169   0.0303\n 3  71     8.4    20.5     3 -7.14   1.41   0.155   0.244   -0.0815\n 4  72.8   5.9    20.7     4 -1.81   1.15   0.544   0.00581  0.128 \n 5  58.3  25.5    15.5     5  0.988 -1.19  -0.0591 -0.0248   0.101 \n 6  44    42.7    13.7     6  5.56   0.226 -1.24   -0.0877   0.0470\n 7  44    42.7    13.7     7  4.96   0.590 -1.16    0.129   -0.129 \n 8  69.3  10.6    19.3     8 -6.31  -0.625  0.166  -0.144    0.0222\n 9  61.4  19.9    17.7     9 11.6   -0.354  0.185  -0.269    0.0274\n10  61.4  19.9    17.7    10 14.6   -0.274  0.459  -0.0457  -0.111 \n# … with 205 more rows\n\n# Check number of PC: ------\n\n# Check number of PCs by eigenvalues\n\nmeat_pca_prep$steps[[2]]$res$sdev %>% as_tibble() %>% \n  filter(value>1) # only 1 PC\n\n\n# A tibble: 1 x 1\n  value\n  <dbl>\n1  9.93\n\n# check using scree plots\n\nproportion_scree_plot <- meat_pca_prep %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms == \"percent variance\") %>% \n  ggplot(aes(component, value, label = value)) +\n  geom_point(size = 2) +\n  geom_line(size = 1) +\n  geom_text(aes(label = round(value, 2)), vjust = -0.2, size = 3) +\n  labs(title = \"% Variance explained\",\n       y = \"% total variance\",\n       x = \"PC\",\n       subtitle = \"98.63 % of variance is explained in PC 1\",\n       caption = \"Source: Tecator Dataset {caret}\") +\n  theme_few() +\n  theme(axis.title = element_text(face = \"bold\", size = 12),\n        axis.text = element_text(size = 10),\n        plot.title = element_text(size = 14, face = \"bold\"))\n\nproportion_scree_plot\n\n\n\n\nPLS regression\nChange dataset into a tibble format\nAlternatively, another ready-to-use format of date may be imported.\n\n\ndata(meats) # from modeldata package\nmeats\n\n\n# A tibble: 215 x 103\n   x_001 x_002 x_003 x_004 x_005 x_006 x_007 x_008 x_009 x_010 x_011\n   <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n 1  2.62  2.62  2.62  2.62  2.62  2.62  2.62  2.62  2.63  2.63  2.63\n 2  2.83  2.84  2.84  2.85  2.85  2.86  2.86  2.87  2.87  2.88  2.88\n 3  2.58  2.58  2.59  2.59  2.59  2.59  2.59  2.60  2.60  2.60  2.60\n 4  2.82  2.82  2.83  2.83  2.83  2.83  2.83  2.84  2.84  2.84  2.84\n 5  2.79  2.79  2.79  2.79  2.80  2.80  2.80  2.80  2.81  2.81  2.81\n 6  3.01  3.02  3.02  3.03  3.03  3.04  3.04  3.05  3.06  3.06  3.07\n 7  2.99  2.99  3.00  3.01  3.01  3.02  3.02  3.03  3.04  3.04  3.05\n 8  2.53  2.53  2.53  2.53  2.53  2.53  2.53  2.53  2.54  2.54  2.54\n 9  3.27  3.28  3.29  3.29  3.30  3.31  3.31  3.32  3.33  3.33  3.34\n10  3.40  3.41  3.41  3.42  3.43  3.43  3.44  3.45  3.46  3.47  3.48\n# … with 205 more rows, and 92 more variables: x_012 <dbl>,\n#   x_013 <dbl>, x_014 <dbl>, x_015 <dbl>, x_016 <dbl>, x_017 <dbl>,\n#   x_018 <dbl>, x_019 <dbl>, x_020 <dbl>, x_021 <dbl>, x_022 <dbl>,\n#   x_023 <dbl>, x_024 <dbl>, x_025 <dbl>, x_026 <dbl>, x_027 <dbl>,\n#   x_028 <dbl>, x_029 <dbl>, x_030 <dbl>, x_031 <dbl>, x_032 <dbl>,\n#   x_033 <dbl>, x_034 <dbl>, x_035 <dbl>, x_036 <dbl>, x_037 <dbl>,\n#   x_038 <dbl>, x_039 <dbl>, x_040 <dbl>, x_041 <dbl>, x_042 <dbl>,\n#   x_043 <dbl>, x_044 <dbl>, x_045 <dbl>, x_046 <dbl>, x_047 <dbl>,\n#   x_048 <dbl>, x_049 <dbl>, x_050 <dbl>, x_051 <dbl>, x_052 <dbl>,\n#   x_053 <dbl>, x_054 <dbl>, x_055 <dbl>, x_056 <dbl>, x_057 <dbl>,\n#   x_058 <dbl>, x_059 <dbl>, x_060 <dbl>, x_061 <dbl>, x_062 <dbl>,\n#   x_063 <dbl>, x_064 <dbl>, x_065 <dbl>, x_066 <dbl>, x_067 <dbl>,\n#   x_068 <dbl>, x_069 <dbl>, x_070 <dbl>, x_071 <dbl>, x_072 <dbl>,\n#   x_073 <dbl>, x_074 <dbl>, x_075 <dbl>, x_076 <dbl>, x_077 <dbl>,\n#   x_078 <dbl>, x_079 <dbl>, x_080 <dbl>, x_081 <dbl>, x_082 <dbl>,\n#   x_083 <dbl>, x_084 <dbl>, x_085 <dbl>, x_086 <dbl>, x_087 <dbl>,\n#   x_088 <dbl>, x_089 <dbl>, x_090 <dbl>, x_091 <dbl>, x_092 <dbl>,\n#   x_093 <dbl>, x_094 <dbl>, x_095 <dbl>, x_096 <dbl>, x_097 <dbl>,\n#   x_098 <dbl>, x_099 <dbl>, x_100 <dbl>, water <dbl>, fat <dbl>,\n#   protein <dbl>\n\nSplit into training and testing dataset\nNote: preprocessing of data, should be done only for TRAINING and not the whole dataset to ensure independence of training and testing dataset.\n\n\nset.seed(20210318)\n\nmeat_split <- initial_split(meats, prop = 0.80)\n\nmeat_training <- meat_split %>% training()\n\nmeat_testing <- meat_split %>% testing()\n\n\n\nEDA\nJust to check the data to see preprocessing steps required.\n\n\nskim(meat_training)\n\n\nTable 1: Data summary\nName\nmeat_training\nNumber of rows\n173\nNumber of columns\n103\n_______________________\n\nColumn type frequency:\n\nnumeric\n103\n________________________\n\nGroup variables\nNone\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nx_001\n0\n1\n2.80\n0.40\n2.07\n2.53\n2.74\n2.99\n4.22\n▃▇▃▁▁\nx_002\n0\n1\n2.80\n0.40\n2.07\n2.53\n2.74\n3.00\n4.23\n▃▇▃▁▁\nx_003\n0\n1\n2.81\n0.40\n2.07\n2.53\n2.74\n3.01\n4.24\n▃▇▃▁▁\nx_004\n0\n1\n2.81\n0.41\n2.06\n2.53\n2.74\n3.01\n4.25\n▃▇▃▁▁\nx_005\n0\n1\n2.81\n0.41\n2.06\n2.53\n2.74\n3.02\n4.26\n▃▇▃▁▁\nx_006\n0\n1\n2.81\n0.41\n2.06\n2.52\n2.75\n3.03\n4.27\n▃▇▃▁▁\nx_007\n0\n1\n2.82\n0.41\n2.06\n2.53\n2.75\n3.03\n4.28\n▃▇▃▁▁\nx_008\n0\n1\n2.82\n0.42\n2.06\n2.53\n2.75\n3.04\n4.29\n▃▇▃▂▁\nx_009\n0\n1\n2.82\n0.42\n2.06\n2.53\n2.76\n3.05\n4.30\n▃▇▃▂▁\nx_010\n0\n1\n2.83\n0.42\n2.06\n2.53\n2.76\n3.05\n4.31\n▃▇▃▂▁\nx_011\n0\n1\n2.83\n0.42\n2.06\n2.53\n2.77\n3.06\n4.33\n▃▇▃▂▁\nx_012\n0\n1\n2.84\n0.43\n2.06\n2.54\n2.77\n3.07\n4.34\n▃▇▃▂▁\nx_013\n0\n1\n2.84\n0.43\n2.06\n2.54\n2.78\n3.07\n4.35\n▃▇▃▂▁\nx_014\n0\n1\n2.85\n0.43\n2.06\n2.54\n2.78\n3.08\n4.37\n▃▇▃▂▁\nx_015\n0\n1\n2.85\n0.43\n2.07\n2.55\n2.78\n3.09\n4.38\n▃▇▃▂▁\nx_016\n0\n1\n2.86\n0.44\n2.07\n2.55\n2.79\n3.10\n4.40\n▃▇▃▁▁\nx_017\n0\n1\n2.86\n0.44\n2.07\n2.55\n2.79\n3.11\n4.42\n▅▇▃▁▁\nx_018\n0\n1\n2.87\n0.44\n2.07\n2.56\n2.80\n3.12\n4.43\n▅▇▃▁▁\nx_019\n0\n1\n2.88\n0.45\n2.07\n2.56\n2.80\n3.13\n4.45\n▅▇▃▁▁\nx_020\n0\n1\n2.89\n0.45\n2.07\n2.57\n2.81\n3.14\n4.47\n▅▇▅▁▁\nx_021\n0\n1\n2.90\n0.45\n2.07\n2.57\n2.81\n3.15\n4.49\n▅▇▅▁▁\nx_022\n0\n1\n2.90\n0.46\n2.08\n2.57\n2.82\n3.16\n4.51\n▅▇▅▁▁\nx_023\n0\n1\n2.91\n0.46\n2.08\n2.58\n2.82\n3.17\n4.53\n▅▇▅▁▁\nx_024\n0\n1\n2.92\n0.46\n2.08\n2.59\n2.83\n3.17\n4.55\n▅▇▅▁▁\nx_025\n0\n1\n2.93\n0.47\n2.08\n2.59\n2.84\n3.18\n4.57\n▅▇▅▁▁\nx_026\n0\n1\n2.93\n0.47\n2.09\n2.60\n2.85\n3.19\n4.59\n▅▇▅▁▁\nx_027\n0\n1\n2.94\n0.47\n2.09\n2.60\n2.86\n3.20\n4.61\n▅▇▅▁▁\nx_028\n0\n1\n2.95\n0.48\n2.09\n2.61\n2.87\n3.21\n4.63\n▅▇▅▁▁\nx_029\n0\n1\n2.96\n0.48\n2.09\n2.61\n2.88\n3.22\n4.65\n▅▇▅▁▁\nx_030\n0\n1\n2.97\n0.48\n2.09\n2.62\n2.88\n3.24\n4.68\n▅▇▅▂▁\nx_031\n0\n1\n2.98\n0.49\n2.10\n2.62\n2.90\n3.26\n4.70\n▅▇▅▂▁\nx_032\n0\n1\n2.99\n0.49\n2.10\n2.63\n2.91\n3.28\n4.73\n▅▇▅▂▁\nx_033\n0\n1\n3.00\n0.50\n2.10\n2.64\n2.91\n3.31\n4.76\n▅▇▅▂▁\nx_034\n0\n1\n3.01\n0.50\n2.10\n2.65\n2.93\n3.33\n4.78\n▅▇▅▂▁\nx_035\n0\n1\n3.03\n0.51\n2.11\n2.66\n2.94\n3.36\n4.81\n▅▇▅▂▁\nx_036\n0\n1\n3.04\n0.51\n2.11\n2.68\n2.94\n3.39\n4.84\n▅▇▅▂▁\nx_037\n0\n1\n3.05\n0.52\n2.12\n2.69\n2.95\n3.42\n4.87\n▅▇▅▂▁\nx_038\n0\n1\n3.07\n0.52\n2.13\n2.70\n2.97\n3.45\n4.90\n▅▇▅▂▁\nx_039\n0\n1\n3.09\n0.52\n2.14\n2.72\n2.99\n3.47\n4.93\n▅▇▅▂▁\nx_040\n0\n1\n3.11\n0.53\n2.15\n2.73\n3.01\n3.48\n4.96\n▅▇▅▂▁\nx_041\n0\n1\n3.12\n0.53\n2.17\n2.74\n3.03\n3.50\n4.98\n▅▇▅▂▁\nx_042\n0\n1\n3.14\n0.53\n2.18\n2.76\n3.04\n3.52\n5.00\n▅▇▅▂▁\nx_043\n0\n1\n3.15\n0.53\n2.20\n2.78\n3.05\n3.53\n5.01\n▅▇▅▂▁\nx_044\n0\n1\n3.16\n0.53\n2.22\n2.79\n3.06\n3.52\n5.02\n▅▇▅▂▁\nx_045\n0\n1\n3.17\n0.52\n2.24\n2.80\n3.08\n3.50\n5.02\n▅▇▅▂▁\nx_046\n0\n1\n3.19\n0.52\n2.27\n2.82\n3.10\n3.49\n5.02\n▅▇▅▁▁\nx_047\n0\n1\n3.20\n0.51\n2.29\n2.84\n3.10\n3.50\n5.03\n▅▇▅▁▁\nx_048\n0\n1\n3.22\n0.51\n2.32\n2.86\n3.12\n3.50\n5.04\n▅▇▅▁▁\nx_049\n0\n1\n3.25\n0.51\n2.35\n2.89\n3.15\n3.52\n5.06\n▅▇▅▁▁\nx_050\n0\n1\n3.28\n0.51\n2.39\n2.93\n3.19\n3.55\n5.09\n▅▇▅▁▁\nx_051\n0\n1\n3.32\n0.51\n2.43\n2.97\n3.22\n3.58\n5.13\n▅▇▅▁▁\nx_052\n0\n1\n3.36\n0.51\n2.48\n3.01\n3.27\n3.61\n5.17\n▅▇▃▂▁\nx_053\n0\n1\n3.41\n0.51\n2.53\n3.05\n3.32\n3.65\n5.23\n▅▇▃▁▁\nx_054\n0\n1\n3.45\n0.52\n2.57\n3.09\n3.37\n3.70\n5.28\n▅▇▃▁▁\nx_055\n0\n1\n3.50\n0.52\n2.61\n3.13\n3.41\n3.74\n5.33\n▅▇▃▁▁\nx_056\n0\n1\n3.53\n0.52\n2.65\n3.16\n3.45\n3.78\n5.37\n▅▇▃▁▁\nx_057\n0\n1\n3.56\n0.53\n2.67\n3.19\n3.48\n3.81\n5.41\n▅▇▃▁▁\nx_058\n0\n1\n3.58\n0.53\n2.69\n3.21\n3.51\n3.83\n5.43\n▅▇▃▁▁\nx_059\n0\n1\n3.59\n0.53\n2.71\n3.22\n3.53\n3.84\n5.44\n▅▇▃▁▁\nx_060\n0\n1\n3.60\n0.53\n2.72\n3.23\n3.54\n3.85\n5.45\n▅▇▃▁▁\nx_061\n0\n1\n3.61\n0.53\n2.72\n3.24\n3.55\n3.86\n5.46\n▅▇▃▁▁\nx_062\n0\n1\n3.61\n0.53\n2.73\n3.25\n3.55\n3.86\n5.47\n▆▇▃▁▁\nx_063\n0\n1\n3.61\n0.53\n2.73\n3.25\n3.56\n3.87\n5.47\n▆▇▃▁▁\nx_064\n0\n1\n3.62\n0.53\n2.73\n3.24\n3.56\n3.87\n5.47\n▆▇▃▁▁\nx_065\n0\n1\n3.61\n0.53\n2.73\n3.24\n3.55\n3.87\n5.47\n▆▇▃▁▁\nx_066\n0\n1\n3.61\n0.53\n2.73\n3.24\n3.55\n3.86\n5.47\n▆▇▃▁▁\nx_067\n0\n1\n3.60\n0.53\n2.73\n3.23\n3.54\n3.85\n5.47\n▆▇▃▁▁\nx_068\n0\n1\n3.60\n0.53\n2.72\n3.22\n3.53\n3.83\n5.46\n▆▇▃▁▁\nx_069\n0\n1\n3.59\n0.53\n2.72\n3.20\n3.52\n3.82\n5.45\n▆▇▃▁▁\nx_070\n0\n1\n3.58\n0.53\n2.71\n3.19\n3.51\n3.82\n5.44\n▆▇▃▁▁\nx_071\n0\n1\n3.56\n0.53\n2.70\n3.18\n3.50\n3.81\n5.43\n▆▇▃▁▁\nx_072\n0\n1\n3.55\n0.53\n2.69\n3.17\n3.49\n3.80\n5.42\n▆▇▃▁▁\nx_073\n0\n1\n3.54\n0.53\n2.68\n3.16\n3.48\n3.79\n5.40\n▆▇▃▁▁\nx_074\n0\n1\n3.52\n0.53\n2.66\n3.14\n3.46\n3.78\n5.39\n▆▇▃▁▁\nx_075\n0\n1\n3.51\n0.53\n2.65\n3.13\n3.45\n3.77\n5.37\n▆▇▃▁▁\nx_076\n0\n1\n3.49\n0.53\n2.63\n3.11\n3.43\n3.75\n5.36\n▆▇▃▁▁\nx_077\n0\n1\n3.47\n0.53\n2.62\n3.09\n3.40\n3.74\n5.34\n▆▇▃▁▁\nx_078\n0\n1\n3.45\n0.53\n2.60\n3.07\n3.38\n3.73\n5.32\n▆▇▃▁▁\nx_079\n0\n1\n3.43\n0.53\n2.59\n3.05\n3.37\n3.72\n5.30\n▆▇▃▁▁\nx_080\n0\n1\n3.42\n0.53\n2.57\n3.03\n3.35\n3.71\n5.29\n▆▇▃▁▁\nx_081\n0\n1\n3.40\n0.53\n2.55\n3.01\n3.33\n3.68\n5.27\n▇▇▃▁▁\nx_082\n0\n1\n3.38\n0.53\n2.53\n2.99\n3.30\n3.66\n5.25\n▇▇▅▁▁\nx_083\n0\n1\n3.36\n0.53\n2.51\n2.97\n3.28\n3.65\n5.23\n▇▇▅▁▁\nx_084\n0\n1\n3.34\n0.53\n2.50\n2.96\n3.26\n3.63\n5.21\n▇▇▅▂▁\nx_085\n0\n1\n3.32\n0.53\n2.48\n2.94\n3.23\n3.62\n5.19\n▇▇▅▂▁\nx_086\n0\n1\n3.30\n0.53\n2.46\n2.92\n3.21\n3.60\n5.17\n▇▇▅▂▁\nx_087\n0\n1\n3.28\n0.53\n2.44\n2.90\n3.18\n3.57\n5.15\n▇▇▅▂▁\nx_088\n0\n1\n3.25\n0.53\n2.42\n2.87\n3.16\n3.55\n5.13\n▇▇▅▂▁\nx_089\n0\n1\n3.23\n0.53\n2.40\n2.85\n3.13\n3.53\n5.10\n▇▇▅▂▁\nx_090\n0\n1\n3.21\n0.53\n2.38\n2.83\n3.11\n3.51\n5.08\n▇▇▅▂▁\nx_091\n0\n1\n3.19\n0.52\n2.36\n2.81\n3.09\n3.49\n5.05\n▇▇▅▂▁\nx_092\n0\n1\n3.17\n0.52\n2.34\n2.78\n3.08\n3.48\n5.03\n▇▇▅▂▁\nx_093\n0\n1\n3.15\n0.52\n2.32\n2.77\n3.06\n3.46\n5.01\n▇▇▅▂▁\nx_094\n0\n1\n3.13\n0.52\n2.30\n2.75\n3.04\n3.45\n4.99\n▇▇▅▂▁\nx_095\n0\n1\n3.11\n0.52\n2.28\n2.73\n3.01\n3.43\n4.97\n▇▇▅▂▁\nx_096\n0\n1\n3.09\n0.52\n2.26\n2.70\n2.99\n3.42\n4.95\n▇▇▅▂▁\nx_097\n0\n1\n3.07\n0.52\n2.24\n2.68\n2.97\n3.41\n4.92\n▇▇▅▂▁\nx_098\n0\n1\n3.05\n0.52\n2.22\n2.67\n2.95\n3.39\n4.90\n▇▇▅▂▁\nx_099\n0\n1\n3.04\n0.52\n2.21\n2.65\n2.94\n3.37\n4.88\n▇▇▅▂▁\nx_100\n0\n1\n3.02\n0.52\n2.19\n2.63\n2.92\n3.35\n4.85\n▇▇▅▂▁\nwater\n0\n1\n63.32\n9.99\n40.20\n55.50\n65.90\n72.00\n76.60\n▂▂▃▅▇\nfat\n0\n1\n18.07\n12.83\n0.90\n7.20\n14.30\n28.10\n48.20\n▇▅▃▂▂\nprotein\n0\n1\n17.69\n3.08\n11.00\n15.30\n18.90\n20.20\n21.80\n▂▃▃▇▇\n\nChecking the correlation between the Y outcomes:\n\n\nmeat_training %>% \n  select(water:protein) %>% \n  ggcorr(label = T, label_alpha = T, label_round = 2)\n\n\n\n\nFrom the exploratory data analysis, we can see that:\nnumber of samples << number of X variables (fat dataset)\nthere are no missing values, so there is no need for missing values imputation\nY variables are skewed, would need to normalize data\nX variables are highly correlated, and it is recommended to only carry out means-centering and not scaling (Ron Wehren’s book for Chemometrics)\nY variables are also highly correlated with each other\nPLS2 Modelling\nThis section below is taken from: https://www.tidymodels.org/learn/models/pls/, with some changes to the preprocessing steps.\nThis step includes preprocessing, specifying pls model to be used and model tuning.\nFor spectral data, autoscaling (normalization) is not usually recommended. When every spectral variable is set to the same standard deviation, the noise is blown up to the same size as the signal that contains the actual information. In such cases, only means-centering is required.\nPreprocessing\n\n\n# Preprocessing only for TRAINING dataset\n\n# recipe\nmeat_reciped_pls2 <- recipe(water + fat + protein ~., data = meat_training) %>% \n  update_role(water, fat, protein, new_role = \"outcome\") %>% \n  step_center(all_predictors())\n  \n# set folds for cross-validation\n\n# repeated 10-fold cross validation for tuning model to select optimal number of components, since this is a small dataset\n\nset.seed(202103182)\n\nfolds_pls2 <- vfold_cv(meat_training, repeats = 10)\n\nfolds_pls2 <- \n  folds_pls2 %>%\n  mutate(recipes = map(splits, prepper, recipe = meat_reciped_pls2))\n\n\n\nTuning of model using repeated Cross-validation dataset\nTo fit the model, we need to:\nformat the X and Y into 2 different matrices, one for X and one for Y. THis is the format which the pls package requires.\nEstimate the Y outcomes\n\n\nget_var_explained <- function(recipe, ...) {\n  \n  # Extract the predictors and outcomes into their own matrices\n  y_mat <- bake(recipe, new_data = NULL, composition = \"matrix\", all_outcomes())\n  \n  x_mat <- bake(recipe, new_data = NULL, composition = \"matrix\", all_predictors())\n  \n  # The pls package prefers the data in a data frame where the outcome\n  # and predictors are in _matrices_. To make sure this is formatted\n  # properly, use the `I()` function to inhibit `data.frame()` from making\n  # all the individual columns. `pls_format` should have two columns.\n  pls_format <- data.frame(\n    endpoints = I(y_mat),\n    measurements = I(x_mat)\n  )\n  # Fit the model\n  mod <- plsr(endpoints ~ measurements, data = pls_format)\n  \n  # Get the proportion of the predictor variance that is explained\n  # by the model for different number of components. \n  xve <- explvar(mod)/100 \n\n  # To do the same for the outcome, it is more complex. This code \n  # was extracted from pls:::summary.mvr. \n  explained <- \n    drop(pls::R2(mod, estimate = \"train\", intercept = FALSE)$val) %>% \n    # transpose so that components are in rows\n    t() %>% \n    as_tibble() %>%\n    # Add the predictor proportions\n    mutate(predictors = cumsum(xve) %>% as.vector(),\n           components = seq_along(xve)) %>%\n    # Put into a tidy format that is tall\n    pivot_longer(\n      cols = c(-components),\n      names_to = \"source\",\n      values_to = \"proportion\"\n    )\n}\n\n# Compute this dataframe for each resample, and save the results in different columns:\n\nfolds_pls2 <- \n  folds_pls2 %>%\n  dplyr::mutate(var = map(recipes, get_var_explained),\n         var = unname(var))\n\n\n\nScree plot\nExtract variance data:\n\n\nvariance_data <- \n  bind_rows(folds_pls2[[\"var\"]]) %>% # select var col in folds dataset\n  filter(components <= 20) %>% # limit components to from 1 to 20\n  group_by(components, source) %>%\n  summarize(proportion = mean(proportion))\n\nggplot(variance_data, aes(x = components, y = proportion, \n                          col = source, label = round(proportion,2))) + \n  geom_line() + \n  geom_point() +\n  geom_text(nudge_y = 0.05) +\n  labs(title = \"Variance captured for different number of components\", \n       subtitle = \"Predictors (X) variance is captured very well by a single componenet.\\nTo estimate all Y outcomes, one may need 11-13 components.\",\n       x = \"Number of components\",\n       y = \"Mean Cumulative Variance\",\n       caption = \"Source: Code is adapted from https://www.tidymodels.org/learn/models/pls/\") +\n  facet_wrap(source ~., ncol = 2, scales = \"fixed\") +\n  scale_x_continuous(breaks = c(1:20), n.breaks = 19) +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n\nPLS 1 modelling - Water\nSimilar to earlier, to evaluate the PLS model, 10 repeats of the 10-folds cross validation will be used (100 holdout samples) to evaluate the overall model performance (RMSE, MAE, r-sq). However, individual models will be built for water, protein and fat.\nSteps involved:\nsplit dataset (already carried out)\nrecipe: specify pre-processing steps\nfit model\nput into workflow\n\n\n# folds for repeated cross validation\nset.seed(20210325)\nfolds_pls1 <- vfold_cv(meat_training, v = 10, repeats = 10)\n\n# recipe\nmeat_reciped_water_pls1 <- recipe(water ~., data = meats) %>% \n  update_role(fat, protein, new_role = \"other_y\") %>% \n  step_center(all_predictors())\n\nmeat_reciped_water_pls1\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   other_y          2\n   outcome          1\n predictor        100\n\nOperations:\n\nCentering for all_predictors()\n\n# fit model\n\npls_water_model <- plsmod::pls(num_comp = tune()) %>% \n  set_mode(\"regression\") %>% # can be either classification or regression\n  set_engine(\"mixOmics\") # to specify which package to use\n\n\n# put into workflow\n\npls_water_workflow <- workflow() %>% \n  add_recipe(meat_reciped_water_pls1) %>% \n  add_model(pls_water_model)\n\n# create grid\n\npls_1_grid <- expand.grid(num_comp = seq(from = 1, to = 20, by = 1))\n\n\ntuned_pls1_water_results <- pls_water_workflow %>% \n  tune_grid(\n    resamples = folds_pls1,\n    grid = pls_1_grid,\n    metrics = metric_set(rmse, rsq, mae)\n  )\n\n# model results\n\nwater_model_results <- tuned_pls1_water_results %>% \n  collect_metrics()\n\nwater_model_results\n\n\n# A tibble: 60 x 7\n   num_comp .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1        1 mae     standard   7.40    100  0.113  Preprocessor1_Mode…\n 2        1 rmse    standard   8.77    100  0.137  Preprocessor1_Mode…\n 3        1 rsq     standard   0.252   100  0.0162 Preprocessor1_Mode…\n 4        2 mae     standard   4.78    100  0.0874 Preprocessor1_Mode…\n 5        2 rmse    standard   6.06    100  0.110  Preprocessor1_Mode…\n 6        2 rsq     standard   0.657   100  0.0121 Preprocessor1_Mode…\n 7        3 mae     standard   3.31    100  0.0635 Preprocessor1_Mode…\n 8        3 rmse    standard   4.10    100  0.0857 Preprocessor1_Mode…\n 9        3 rsq     standard   0.823   100  0.0102 Preprocessor1_Mode…\n10        4 mae     standard   2.78    100  0.0439 Preprocessor1_Mode…\n# … with 50 more rows\n\n# best model\n\ntuned_pls1_water_results %>% \n  select_best(metric = \"rmse\") # num_comp = 18\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       18 Preprocessor1_Model18\n\ntuned_pls1_water_results %>% \n  select_best(metric = \"rsq\") # num_comp = 18\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       18 Preprocessor1_Model18\n\ntuned_pls1_water_results %>% \n  select_best(metric = \"mae\") # num_comp = 18\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       18 Preprocessor1_Model18\n\n# visualize\ntuned_pls1_water_results %>% \n  collect_metrics() %>% \n  ggplot(aes(num_comp, mean, col = .metric)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(n.breaks = 20) +\n  labs(x = \"Number of components\",\n       y = \"Indicator\",\n       title = \"Plot of MAE, RMSE and R-SQ vs No. of components for TRAINING dataset, with 10-fold repeated cross validation\",\n       subtitle = \"Predicting Water Content: Optimal number of components is 18.\") +\n facet_grid(.metric ~.) +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n# Update model and workflow:\n\nupdated_pls_water_model <-   plsmod::pls(num_comp = 18) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"mixOmics\")\n\nupdated_pls_water_workflow <- pls_water_workflow %>% \n  update_model(updated_pls_water_model)\n\npls_water_fit <- updated_pls_water_workflow %>% \n  fit(data = meat_training)\n\n# Check the most important X variables for the updated Water Model:\n\ntidy_pls_water <- pls_water_fit %>% \n  pull_workflow_fit() %>% \n  tidy()\n\n# Variable importance\n\ntidy_pls_water %>% \n  filter(term != \"Y\",\n         component == c(1:18)) %>%\n  group_by(component) %>% \n  slice_max(abs(value), n = 20) %>% \n  ungroup() %>% \n  ggplot(aes(value, fct_reorder(term, value), fill = factor(component))) +\n  geom_col(show.legend = F) +\n  facet_wrap( ~ component, scales = \"free_y\") +\n  labs( y = NULL) +\n  theme_few()\n\n\n\n# Assess\n\npls_water_fit %>% \n  predict(new_data = meat_training) %>% \n  mutate(truth = meat_training$water) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TRAINING dataset\",\n       x = \"Actual Water Content\",\n       y = \"Predicted Water Content\") +\n  theme_few()\n\n\n\n# for TEST dataset\n  \npls_water_fit %>% \n  predict(new_data = meat_testing) %>% \n  mutate(truth = meat_testing$water) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TEST dataset\",\n       x = \"Actual Water Content\",\n       y = \"Predicted Water Content\") +\n  theme_few()\n\n\n\n# assessing model on test data\nupdated_pls_water_workflow %>% \n  last_fit(meat_split) %>% \n  collect_metrics()\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       2.32  Preprocessor1_Model1\n2 rsq     standard       0.944 Preprocessor1_Model1\n\nFrom above, it seems that the prediction of moisture content is more accurate for higher water content samples.\nTo predict water content for future data:\npls_water_fit %>% predict(trial_data), where trial_data has the NIR spectrum.\nPLS 1 modelling for fat\nRepeat the workflow above for fat:\n\n\nmeat_reciped_fat_pls1 <- recipe(fat ~., data = meats) %>% \n  update_role(water, protein, new_role = \"other_y\") %>% \n  step_center(all_predictors())\n\n\npls_fat_model <- plsmod::pls(num_comp = tune()) %>% \n  set_mode(\"regression\") %>% # can be either classification or regression\n  set_engine(\"mixOmics\")\n\npls_fat_workflow <- workflow() %>% \n  add_recipe(meat_reciped_fat_pls1) %>% \n  add_model(pls_fat_model)\n\ntuned_pls1_fat_results <- pls_fat_workflow %>% \n  tune_grid(\n    resamples = folds_pls1,\n    grid = pls_1_grid,\n    metrics = metric_set(rmse, rsq, mae)\n  )\n\n# model results\n\nfat_model_results <- tuned_pls1_fat_results %>% \n  collect_metrics()\n\nfat_model_results\n\n\n# A tibble: 60 x 7\n   num_comp .metric .estimator   mean     n std_err .config           \n      <dbl> <chr>   <chr>       <dbl> <int>   <dbl> <chr>             \n 1        1 mae     standard    9.50    100  0.149  Preprocessor1_Mod…\n 2        1 rmse    standard   11.5     100  0.178  Preprocessor1_Mod…\n 3        1 rsq     standard    0.223   100  0.0155 Preprocessor1_Mod…\n 4        2 mae     standard    6.32    100  0.120  Preprocessor1_Mod…\n 5        2 rmse    standard    8.21    100  0.143  Preprocessor1_Mod…\n 6        2 rsq     standard    0.617   100  0.0126 Preprocessor1_Mod…\n 7        3 mae     standard    4.24    100  0.0914 Preprocessor1_Mod…\n 8        3 rmse    standard    5.43    100  0.133  Preprocessor1_Mod…\n 9        3 rsq     standard    0.809   100  0.0123 Preprocessor1_Mod…\n10        4 mae     standard    3.57    100  0.0576 Preprocessor1_Mod…\n# … with 50 more rows\n\n# best model\n\ntuned_pls1_fat_results %>% \n  select_best(metric = \"rmse\") # num_comp = 19\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       19 Preprocessor1_Model19\n\ntuned_pls1_fat_results %>% \n  select_best(metric = \"rsq\") # num_comp = 18\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       18 Preprocessor1_Model18\n\ntuned_pls1_fat_results %>% \n  select_best(metric = \"mae\") # num_comp = 19\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       19 Preprocessor1_Model19\n\n# visualize\ntuned_pls1_fat_results %>% \n  collect_metrics() %>% \n  ggplot(aes(num_comp, mean, col = .metric)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(n.breaks = 20) +\n  labs(x = \"Number of components\",\n       y = \"Indicator\",\n       title = \"Plot of MAE, RMSE and R-SQ vs No. of components for TRAINING dataset, with 10-fold repeated cross validation\",\n       subtitle = \"Predicting Fat Content: Optimal number of components is 18.\") +\n facet_grid(.metric ~.) +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n# Update model and workflow:\n\nupdated_pls_fat_model <-   plsmod::pls(num_comp = 18) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"mixOmics\")\n\nupdated_pls_fat_workflow <- pls_fat_workflow %>% \n  update_model(updated_pls_fat_model)\n\npls_fat_fit <- updated_pls_fat_workflow %>% \n  fit(data = meat_training)\n\n# Check the most important X variables for the updated Water Model:\n\ntidy_pls_fat <- pls_fat_fit %>% \n  pull_workflow_fit() %>% \n  tidy()\n\n# Variable importance\n\ntidy_pls_fat %>% \n  filter(term != \"Y\",\n         component == c(1:18)) %>%\n  group_by(component) %>% \n  slice_max(abs(value), n = 20) %>% \n  ungroup() %>% \n  ggplot(aes(value, fct_reorder(term, value), fill = factor(component))) +\n  geom_col(show.legend = F) +\n  facet_wrap( ~ component, scales = \"free_y\") +\n  labs( y = NULL) +\n  theme_few()\n\n\n\n# Assess\n\npls_fat_fit %>% \n  predict(new_data = meat_training) %>% \n  mutate(truth = meat_training$fat) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TRAINING dataset\",\n       x = \"Actual Fat Content\",\n       y = \"Predicted Fat Content\") +\n  theme_few()\n\n\n\n# for TEST dataset\n  \npls_fat_fit %>% \n  predict(new_data = meat_testing) %>% \n  mutate(truth = meat_testing$fat) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TEST dataset\",\n       x = \"Actual Fat Content\",\n       y = \"Predicted Fat Content\") +\n  theme_few()\n\n\n\n# assessing model on test data\nupdated_pls_fat_workflow %>% \n  last_fit(meat_split) %>% \n  collect_metrics()\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       2.66  Preprocessor1_Model1\n2 rsq     standard       0.959 Preprocessor1_Model1\n\nFat content prediction is less accurate for samples with higher fat content.\nTo predict fat content for future data:\npls_fat_fit %>% predict(trial_data), where trial_data has the NIR spectrum.\nPLS 1 modelling for protein\n\n\nmeat_reciped_protein_pls1 <- recipe(protein ~., data = meats) %>%\n  update_role(water, fat, new_role = \"other_y\") %>% \n  step_center(all_predictors())\n\n\npls_protein_model <- plsmod::pls(num_comp = tune()) %>% \n  set_mode(\"regression\") %>% # can be either classification or regression\n  set_engine(\"mixOmics\") \n\npls_protein_workflow <- workflow() %>% \n  add_recipe(meat_reciped_protein_pls1) %>% \n  add_model(pls_protein_model)\n\ntuned_pls1_protein_results <- pls_protein_workflow %>% \n  tune_grid(\n    resamples = folds_pls1,\n    grid = pls_1_grid,\n    metrics = metric_set(rmse, rsq, mae)\n  )\n\n# model results\n\nprotein_model_results <- tuned_pls1_protein_results %>% \n  collect_metrics()\n\nprotein_model_results\n\n\n# A tibble: 60 x 7\n   num_comp .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1        1 mae     standard   2.54    100  0.0378 Preprocessor1_Mode…\n 2        1 rmse    standard   2.97    100  0.0395 Preprocessor1_Mode…\n 3        1 rsq     standard   0.126   100  0.0136 Preprocessor1_Mode…\n 4        2 mae     standard   1.92    100  0.0345 Preprocessor1_Mode…\n 5        2 rmse    standard   2.38    100  0.0366 Preprocessor1_Mode…\n 6        2 rsq     standard   0.406   100  0.0164 Preprocessor1_Mode…\n 7        3 mae     standard   1.37    100  0.0333 Preprocessor1_Mode…\n 8        3 rmse    standard   1.84    100  0.0450 Preprocessor1_Mode…\n 9        3 rsq     standard   0.635   100  0.0170 Preprocessor1_Mode…\n10        4 mae     standard   1.24    100  0.0255 Preprocessor1_Mode…\n# … with 50 more rows\n\n# best model\n\ntuned_pls1_protein_results %>% \n  select_best(metric = \"rmse\") # num_comp = 14\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       14 Preprocessor1_Model14\n\ntuned_pls1_protein_results %>% \n  select_best(metric = \"rsq\") # num_comp = 14\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       15 Preprocessor1_Model15\n\ntuned_pls1_protein_results %>% \n  select_best(metric = \"mae\") # num_comp = 15\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1       15 Preprocessor1_Model15\n\n# visualize\ntuned_pls1_protein_results %>% \n  collect_metrics() %>% \n  ggplot(aes(num_comp, mean, col = .metric)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(n.breaks = 20) +\n  labs(x = \"Number of components\",\n       y = \"Indicator\",\n       title = \"Plot of MAE, RMSE and R-SQ vs No. of components for TRAINING dataset, with 10-fold repeated cross validation\",\n       subtitle = \"Predicting protein Content: Optimal number of components is 14.\") +\n facet_grid(.metric ~.) +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n# Update model and workflow:\n\nupdated_pls_protein_model <-   plsmod::pls(num_comp = 14) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"mixOmics\")\n\nupdated_pls_protein_workflow <- pls_protein_workflow %>% \n  update_model(updated_pls_protein_model)\n\npls_protein_fit <- updated_pls_protein_workflow %>% \n  fit(data = meat_training)\n\n# Check the most important X variables for the updated Water Model:\n\ntidy_pls_protein <- pls_protein_fit %>% \n  pull_workflow_fit() %>% \n  tidy()\n\n# Variable importance\n\ntidy_pls_protein %>% \n  filter(term != \"Y\",\n         component == c(1:14)) %>%\n  group_by(component) %>% \n  slice_max(abs(value), n = 20) %>% \n  ungroup() %>% \n  ggplot(aes(value, fct_reorder(term, value), fill = factor(component))) +\n  geom_col(show.legend = F) +\n  facet_wrap( ~ component, scales = \"free_y\") +\n  labs( y = NULL) +\n  theme_few()\n\n\n\n# Assess\n\npls_protein_fit %>% \n  predict(new_data = meat_training) %>% \n  mutate(truth = meat_training$protein) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TRAINING dataset\",\n       x = \"Actual protein Content\",\n       y = \"Predicted protein Content\") +\n  theme_few()\n\n\n\n# for TEST dataset\n  \npls_protein_fit %>% \n  predict(new_data = meat_testing) %>% \n  mutate(truth = meat_testing$protein) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TEST dataset\",\n       x = \"Actual protein Content\",\n       y = \"Predicted protein Content\") +\n  theme_few()\n\n\n\n# assessing model on test data\nupdated_pls_protein_workflow %>% \n  last_fit(meat_split) %>% \n  collect_metrics()\n\n\n# A tibble: 2 x 4\n  .metric .estimator .estimate .config             \n  <chr>   <chr>          <dbl> <chr>               \n1 rmse    standard       0.463 Preprocessor1_Model1\n2 rsq     standard       0.980 Preprocessor1_Model1\n\nTo predict protein content for future data:\npls_protein_fit %>% predict(trial_data), where trial_data has the NIR spectrum.\nLearning pointers\nThrough this exercise, I learnt:\nhow to carry out PCA and PLS regression on NIR data. However, I am not very sure whether to carry out both means-centering and scaling on NIR data or not?\nhow to efficiently visualize the number of components for PLS regression by following the steps listed on https://www.tidymodels.org/learn/models/pls/\nthe difference between PLS 1 and PLS 2, although I think tidymodels cannot handle PLS 2 yet.\nInitially I tried to carry out PLS regression by visual inspection for optimal number of components so that there will be a tradeoff between over-fitting and bias, but I find that the accuracy is really compromised, so I will stick to the optimal number of components by looking at the three different metrics and choosing from there.\nI think this is a good practice for real-life datasets, but I would probably need to practice more on other datasets to get the hang of PLS regression. It would also be good practice to work on comparison of different models on test dataset.\nReferences\nhttps://mixomicsteam.github.io/Bookdown/pls.html https://www.tidymodels.org/learn/models/pls/ https://www.tmwr.org/resampling.html https://rsample.tidymodels.org/articles/Working_with_rsets.html https://conf20-intro-ml.netlify.app/slides/07-cv.html#1 https://www.sciencedirect.com/science/article/pii/S1878535214000343 https://stackoverflow.com/questions/64582463/how-do-i-specify-a-pls-model-in-tidy-models https://github.com/tidymodels/workflows/issues/37\n\n\n\n",
    "preview": "posts/20210328_NIR Meat Data (PLS-regression)/Meat-NIR-PLS-regression_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-28T23:46:45+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210311_notes on resampling/",
    "title": "Notes - Resampling",
    "description": "Types of resampling techniqes and considerations on which to choose",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-03-11",
    "categories": [],
    "contents": "\nBackground\nWhen I was trying out PLS regression on the gasoline data (a small dataset of 60 samples with NIR spectra measured over more than 400 wavelengths), I was a little stuck at the resampling part. Most examples online used k-fold cross validation as the resampling method, but I was also wondering about bootstrapping, because I remembered that my Prof mentioned it would also help in model performance to prevent overfitting.\nI think there was a need for me to get my concepts right for:\nOverfitting - why it is bad –> so that new data can be predicted accurately, not just having an accurate model for old training data.\nResampling, when do you need to do it? –> to improve model tuning, and give good predictions of how well the model will perform on future samples.\nHow to split the data: 80/20? 70/30? 75/25? –> if you don’t have a lot of data, suggest to use 80/20 so that more datapoints can be used to training.\nWhat are the various resampling techniques available, and how to choose which one to use?\nI got the information below from the book Applied Predictive Modelling http://appliedpredictivemodeling.com/, Chapter 4.\nResampling techniques\nk-fold cross-validation\nSamples are randomly divided into k sets of the same size. A model is fit using all the samples except the first subset. The first subset is then used to assess model performance. The whole process is repeated k times, using other subsets. Model performance may be assessed by comparing error rates or r-squared values.\nk-fold cross-validation usually has a high variance (low bias) compared to other methods. However, for large training sets, the variance-bias issue should be negligible.\nk is usually fixed as 5 or 10. The bias is smaller for k = 10.\nThis method is recommended for tuning model parameters to get the best indicator of performance for small sample sizes as the bias-variance properties are good, and does not come with high computational costs (unlike leave-one-out method).\nBootstrapping\nA bootstrap sample is a random sample of the data taken with replacement. Some samples may be selected multiple times, while there may be others that are not selected at all. In general, bootstrap error rates tend to have less uncertainty than k-fold.\nThis method may be preferred if the aim is to choose between models, as bootstrapping technique has low variance (high bias).\nChoosing models\nThere is a spectrum of interpretability and flexibility for models. Choose variaous models that occur at different parts of the spectrum, for example, a simple and inflexible but easy to interpret linear model, as compared to partial least squares which is lower down the interpretability but higher up the flexibility, as compared to random forests which are hard to interpret but very flexible.\nReflections\nI am really glad to have this Applied Predictive Modelling book with me. It clarified a lot on the concepts part, but most of the code is given in the older coding styles. Looking forward to try on the worked examples using tidymodels framework!\nReferences\nhttps://stackoverflow.com/questions/64254295/predictor-importance-for-pls-model-trained-with-tidymodels\nhttps://rpubs.com/RandallThompson15/HW10_624\nhttps://www.tidyverse.org/blog/2020/04/parsnip-adjacent/\nApplied Predictive Modelling, Max Kuhn and Kjell Johnson, Chapter 4. http://appliedpredictivemodeling.com/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-03-11T21:41:14+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210309_tidy models plsr gasoline/",
    "title": "Tidy Models - Regression",
    "description": "Predicting numerical outcomes using partial least squares on gasoline dataset",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-03-09",
    "categories": [],
    "contents": "\nOverview\nThe partial least square (PLS) regression method may be used to predict one or more numerical Y outcome variable when the X variables are highly correlated. In the chemistry field, it is very useful for relating spectra to chemical properties.\nThe problem with using ordinary least square linear regression for such datasets is that when X variables are highly correlated, it is difficult to interpret coefficients of different X meaningfully, since they are all correlated.\nPLS technique is also useful when the number of samples is lesser than the number of variables. It maximises the covariance of both X and Y to calculate scores (how much a particular object contributes to a latent variable) and loadings (variable coefficients used to define a component). For principal component regression (PCR), only X variables are taken into account. If the variability of X is not related to variability of Y, then PCR will have difficulty identifying a predictive relationship, when one might exist. It helps to uncover latent structures in the highly correlated X variables, so as to predict Y.\nIn a way, PLS is a supervised dimension reduction. It finds components that maximally summarise the variation of the predictors while simultaneiously requiring these components to have maximum correlation with the response.\nThe workflow below is for predicting one Y variable, using multiple X variables, on the gasoline NIR dataset. The outcome variable of interest is the octane number.\nGeneral workflow:\nExploratory data analysis: Check if there are missing values, correlation of variables, identify the pre-processing steps required. How many variables are there? How are they distributed? Is it suitable for PLS?\nThe data may also be visualized before modelling.\nSplit the dataset into training and testing dataset. The training dataset is used to build and tune the models, and the testing dataset is used to evaluate the model. Define a 10-fold cross validation dataset, from the training dataset. Resampling is useful if the sample size is small, to have a better bias-variance tradeoff (minimise over-fitting of model) for better predictive ability.\nPreprocess the training dataset. Data would have to be imputed if there are missing values, and also normalised since PLS is using variance to understand dissimilarity in the X variables.\nTrain the model using the training dataset. In this case, the pls model is used. For PLS model, there is one tuning parameter, which is the number of components. The tune package will be used for tuning, and the data will be the 10-fold cross validation dataset.\nAssess the training model using root mean square error (RMSE) and mean absolute error (MAE), as well as r-sq for accuracy.\nDetermine which variables are important in the model\nPredict new data\nPackages required\n\n\nlibrary(pacman)\np_load(modeldata, pls, tidyverse, tidymodels, corrplot, \n       skimr, plsmod, ggthemes)\n\n\n\nImport data\nThis dataset is from the pls package, and has octane number (Y outcome variable), as well as NIR spectra of 60 gasoline samples.\n\n\ndata(gasoline)\n\n\n\nVisualize\n\n\ndata_plot <- cbind(gasoline, as.data.frame((unclass(gasoline$NIR)))) %>% \n  dplyr::select(-NIR) %>% \n  as_tibble() %>% \n  rowid_to_column() %>% \n  janitor::clean_names() %>% \n  pivot_longer(cols = starts_with(\"x\"),\n               names_to = \"wavelength\",\n               values_to = \"reflectance\") %>% \n  mutate(wavelength_number = parse_number(wavelength))\n\ndata_plot %>% \n  ggplot(aes(x = wavelength_number, y = reflectance, col = factor(rowid))) +\n  geom_line(show.legend = F) +\n  labs(y = \"log(1/R)\",\n       x = \"nm\",\n       title = \"Plot of NIR spectra for 60 gasoline samples\") +\n  coord_cartesian(xlim = c(900, 1800)) +\n  theme_classic()\n\n\n\n\nModelling\nTo use the dataset for modelling using the tidymodels framework, convert the data into a tidy tibble structure. The pls package requires that the data be provided as a matrix format, but for the tidymodels framework, the data should be in a tibble format for ease in creating plots and modelling.\n\n\ngasoline_tidy <-  cbind(gasoline, as.data.frame((unclass(gasoline$NIR)))) %>% \n  dplyr::select(-NIR) %>% \n  as_tibble() %>% \n  janitor::clean_names() \n\nglimpse(gasoline_tidy) # 60 rows, 402 columns\n\n\nRows: 60\nColumns: 402\n$ octane   <dbl> 85.30, 85.25, 88.45, 83.40, 87.90, 85.50, 88.90, 88…\n$ x900_nm  <dbl> -0.050193, -0.044227, -0.046867, -0.046705, -0.0508…\n$ x902_nm  <dbl> -0.045903, -0.039602, -0.041260, -0.042240, -0.0451…\n$ x904_nm  <dbl> -0.042187, -0.035673, -0.036979, -0.038561, -0.0410…\n$ x906_nm  <dbl> -0.037177, -0.030911, -0.031458, -0.034513, -0.0363…\n$ x908_nm  <dbl> -0.033348, -0.026675, -0.026520, -0.030206, -0.0327…\n$ x910_nm  <dbl> -0.031207, -0.023871, -0.023346, -0.027680, -0.0314…\n$ x912_nm  <dbl> -0.030036, -0.022571, -0.021392, -0.026042, -0.0314…\n$ x914_nm  <dbl> -0.031298, -0.025410, -0.024993, -0.028280, -0.0346…\n$ x916_nm  <dbl> -0.034217, -0.028960, -0.029309, -0.030920, -0.0377…\n$ x918_nm  <dbl> -0.036012, -0.032740, -0.033920, -0.034012, -0.0407…\n$ x920_nm  <dbl> -0.039792, -0.036683, -0.038539, -0.037082, -0.0440…\n$ x922_nm  <dbl> -0.043037, -0.040169, -0.042571, -0.040444, -0.0473…\n$ x924_nm  <dbl> -0.047313, -0.044899, -0.047511, -0.044858, -0.0514…\n$ x926_nm  <dbl> -0.048103, -0.046266, -0.048487, -0.046544, -0.0520…\n$ x928_nm  <dbl> -0.050627, -0.048627, -0.050455, -0.048978, -0.0540…\n$ x930_nm  <dbl> -0.053830, -0.052014, -0.053913, -0.052483, -0.0571…\n$ x932_nm  <dbl> -0.054604, -0.053635, -0.055195, -0.054078, -0.0583…\n$ x934_nm  <dbl> -0.056676, -0.055454, -0.056713, -0.055723, -0.0599…\n$ x936_nm  <dbl> -0.058428, -0.056777, -0.057447, -0.056999, -0.0606…\n$ x938_nm  <dbl> -0.060644, -0.059331, -0.059620, -0.059409, -0.0628…\n$ x940_nm  <dbl> -0.061712, -0.060518, -0.060656, -0.060543, -0.0637…\n$ x942_nm  <dbl> -0.063148, -0.062084, -0.061767, -0.061868, -0.0650…\n$ x944_nm  <dbl> -0.064480, -0.063331, -0.062859, -0.062899, -0.0657…\n$ x946_nm  <dbl> -0.065987, -0.065059, -0.064668, -0.064401, -0.0672…\n$ x948_nm  <dbl> -0.066894, -0.065536, -0.065566, -0.064814, -0.0679…\n$ x950_nm  <dbl> -0.068201, -0.066877, -0.066890, -0.065989, -0.0692…\n$ x952_nm  <dbl> -0.069434, -0.067899, -0.068397, -0.067180, -0.0707…\n$ x954_nm  <dbl> -0.069594, -0.067662, -0.068439, -0.066987, -0.0707…\n$ x956_nm  <dbl> -0.071336, -0.069050, -0.070171, -0.068829, -0.0724…\n$ x958_nm  <dbl> -0.071208, -0.068497, -0.070077, -0.069063, -0.0722…\n$ x960_nm  <dbl> -0.071454, -0.068434, -0.070639, -0.069995, -0.0726…\n$ x962_nm  <dbl> -0.071469, -0.067953, -0.070578, -0.070687, -0.0726…\n$ x964_nm  <dbl> -0.071631, -0.067061, -0.070137, -0.070725, -0.0722…\n$ x966_nm  <dbl> -0.070886, -0.066073, -0.069803, -0.070661, -0.0718…\n$ x968_nm  <dbl> -0.071010, -0.065564, -0.069738, -0.071043, -0.0718…\n$ x970_nm  <dbl> -0.070609, -0.065335, -0.070031, -0.071324, -0.0718…\n$ x972_nm  <dbl> -0.070464, -0.064051, -0.069496, -0.070894, -0.0714…\n$ x974_nm  <dbl> -0.070433, -0.063666, -0.069789, -0.071157, -0.0717…\n$ x976_nm  <dbl> -0.069957, -0.062513, -0.069002, -0.070798, -0.0711…\n$ x978_nm  <dbl> -0.070208, -0.061955, -0.069288, -0.071120, -0.0713…\n$ x980_nm  <dbl> -0.070523, -0.060906, -0.069398, -0.071266, -0.0715…\n$ x982_nm  <dbl> -0.070252, -0.059413, -0.068613, -0.070540, -0.0707…\n$ x984_nm  <dbl> -0.069360, -0.058242, -0.067675, -0.069737, -0.0698…\n$ x986_nm  <dbl> -0.068338, -0.056855, -0.066685, -0.068637, -0.0687…\n$ x988_nm  <dbl> -0.066991, -0.055982, -0.065196, -0.067211, -0.0673…\n$ x990_nm  <dbl> -0.065362, -0.055107, -0.063317, -0.065586, -0.0655…\n$ x992_nm  <dbl> -0.063971, -0.054763, -0.061604, -0.064064, -0.0641…\n$ x994_nm  <dbl> -0.062210, -0.054189, -0.059695, -0.062089, -0.0624…\n$ x996_nm  <dbl> -0.060678, -0.054361, -0.058403, -0.060639, -0.0612…\n$ x998_nm  <dbl> -0.059275, -0.054014, -0.056868, -0.059018, -0.0598…\n$ x1000_nm <dbl> -0.059126, -0.054822, -0.056419, -0.058185, -0.0595…\n$ x1002_nm <dbl> -0.058903, -0.055303, -0.056021, -0.057415, -0.0592…\n$ x1004_nm <dbl> -0.058488, -0.055101, -0.055192, -0.056311, -0.0583…\n$ x1006_nm <dbl> -0.057291, -0.054473, -0.053844, -0.054969, -0.0570…\n$ x1008_nm <dbl> -0.055408, -0.053119, -0.051961, -0.052664, -0.0552…\n$ x1010_nm <dbl> -0.053862, -0.051626, -0.050363, -0.050925, -0.0537…\n$ x1012_nm <dbl> -0.051421, -0.048967, -0.047896, -0.048186, -0.0516…\n$ x1014_nm <dbl> -0.050485, -0.047722, -0.046456, -0.046844, -0.0508…\n$ x1016_nm <dbl> -0.048349, -0.045717, -0.044870, -0.045207, -0.0492…\n$ x1018_nm <dbl> -0.047586, -0.044365, -0.044068, -0.044741, -0.0484…\n$ x1020_nm <dbl> -0.046898, -0.043336, -0.043379, -0.044397, -0.0481…\n$ x1022_nm <dbl> -0.047726, -0.043581, -0.044227, -0.044697, -0.0487…\n$ x1024_nm <dbl> -0.048686, -0.044036, -0.045469, -0.045614, -0.0498…\n$ x1026_nm <dbl> -0.049626, -0.044856, -0.046493, -0.046452, -0.0506…\n$ x1028_nm <dbl> -0.050714, -0.046041, -0.048213, -0.047669, -0.0520…\n$ x1030_nm <dbl> -0.051541, -0.046858, -0.049097, -0.048569, -0.0528…\n$ x1032_nm <dbl> -0.052976, -0.048652, -0.050967, -0.050055, -0.0546…\n$ x1034_nm <dbl> -0.054019, -0.049853, -0.052127, -0.051178, -0.0555…\n$ x1036_nm <dbl> -0.055507, -0.051562, -0.053886, -0.052781, -0.0569…\n$ x1038_nm <dbl> -0.056386, -0.052526, -0.054432, -0.053594, -0.0576…\n$ x1040_nm <dbl> -0.056779, -0.053738, -0.055172, -0.054460, -0.0580…\n$ x1042_nm <dbl> -0.056876, -0.054069, -0.055257, -0.054997, -0.0582…\n$ x1044_nm <dbl> -0.056439, -0.053925, -0.054866, -0.055201, -0.0578…\n$ x1046_nm <dbl> -0.056708, -0.054880, -0.055286, -0.055934, -0.0585…\n$ x1048_nm <dbl> -0.056432, -0.054857, -0.055256, -0.055994, -0.0583…\n$ x1050_nm <dbl> -0.056974, -0.055695, -0.055922, -0.056841, -0.0590…\n$ x1052_nm <dbl> -0.057091, -0.055656, -0.055649, -0.056687, -0.0586…\n$ x1054_nm <dbl> -0.057541, -0.056352, -0.056044, -0.057192, -0.0590…\n$ x1056_nm <dbl> -0.057746, -0.056501, -0.056164, -0.057521, -0.0589…\n$ x1058_nm <dbl> -0.058387, -0.057275, -0.056653, -0.058097, -0.0592…\n$ x1060_nm <dbl> -0.058997, -0.057927, -0.057349, -0.058708, -0.0597…\n$ x1062_nm <dbl> -0.058975, -0.058044, -0.057620, -0.058940, -0.0601…\n$ x1064_nm <dbl> -0.059624, -0.058311, -0.058109, -0.059339, -0.0607…\n$ x1066_nm <dbl> -0.059737, -0.058253, -0.057978, -0.059002, -0.0607…\n$ x1068_nm <dbl> -0.060552, -0.058841, -0.058726, -0.059353, -0.0617…\n$ x1070_nm <dbl> -0.060416, -0.058138, -0.058442, -0.058568, -0.0615…\n$ x1072_nm <dbl> -0.061099, -0.058603, -0.058857, -0.058309, -0.0623…\n$ x1074_nm <dbl> -0.060784, -0.058309, -0.058779, -0.057284, -0.0621…\n$ x1076_nm <dbl> -0.061292, -0.058444, -0.059351, -0.057273, -0.0625…\n$ x1078_nm <dbl> -0.061811, -0.058993, -0.060286, -0.057433, -0.0631…\n$ x1080_nm <dbl> -0.061852, -0.059121, -0.060681, -0.057911, -0.0634…\n$ x1082_nm <dbl> -0.062380, -0.059445, -0.061533, -0.059303, -0.0640…\n$ x1084_nm <dbl> -0.062816, -0.059475, -0.061879, -0.060453, -0.0642…\n$ x1086_nm <dbl> -0.063620, -0.060274, -0.062840, -0.062042, -0.0651…\n$ x1088_nm <dbl> -0.063730, -0.060256, -0.063150, -0.062812, -0.0654…\n$ x1090_nm <dbl> -0.064291, -0.060515, -0.063667, -0.063620, -0.0658…\n$ x1092_nm <dbl> -0.064209, -0.060839, -0.063744, -0.063369, -0.0657…\n$ x1094_nm <dbl> -0.064162, -0.060784, -0.063969, -0.063106, -0.0659…\n$ x1096_nm <dbl> -0.063681, -0.060401, -0.063588, -0.062210, -0.0654…\n$ x1098_nm <dbl> -0.062757, -0.059631, -0.062893, -0.061071, -0.0646…\n$ x1100_nm <dbl> -0.061833, -0.058887, -0.062346, -0.060476, -0.0637…\n$ x1102_nm <dbl> -0.060653, -0.057861, -0.060954, -0.059613, -0.0623…\n$ x1104_nm <dbl> -0.059872, -0.057174, -0.060277, -0.059053, -0.0614…\n$ x1106_nm <dbl> -0.058221, -0.055943, -0.058933, -0.057879, -0.0596…\n$ x1108_nm <dbl> -0.057000, -0.054586, -0.057725, -0.056993, -0.0583…\n$ x1110_nm <dbl> -0.054962, -0.052730, -0.056403, -0.055674, -0.0566…\n$ x1112_nm <dbl> -0.052726, -0.050650, -0.054639, -0.053633, -0.0544…\n$ x1114_nm <dbl> -0.050188, -0.047988, -0.052604, -0.051368, -0.0521…\n$ x1116_nm <dbl> -0.047313, -0.045184, -0.050002, -0.048753, -0.0490…\n$ x1118_nm <dbl> -0.044383, -0.042775, -0.047952, -0.046565, -0.0458…\n$ x1120_nm <dbl> -0.040658, -0.039567, -0.044838, -0.043878, -0.0415…\n$ x1122_nm <dbl> -0.036981, -0.036483, -0.041595, -0.041106, -0.0366…\n$ x1124_nm <dbl> -0.030389, -0.031762, -0.036552, -0.036145, -0.0297…\n$ x1126_nm <dbl> -0.022454, -0.026061, -0.030222, -0.029643, -0.0205…\n$ x1128_nm <dbl> -0.011760, -0.017961, -0.021030, -0.020369, -0.0080…\n$ x1130_nm <dbl> 0.000941, -0.008352, -0.010311, -0.009442, 0.006771…\n$ x1132_nm <dbl> 0.018067, 0.004104, 0.003633, 0.005123, 0.026072, 0…\n$ x1134_nm <dbl> 0.039563, 0.020129, 0.021227, 0.023279, 0.050595, 0…\n$ x1136_nm <dbl> 0.065490, 0.040174, 0.043017, 0.045674, 0.080282, 0…\n$ x1138_nm <dbl> 0.093330, 0.061190, 0.065780, 0.069009, 0.111046, 0…\n$ x1140_nm <dbl> 0.119886, 0.082808, 0.088740, 0.092430, 0.140255, 0…\n$ x1142_nm <dbl> 0.143335, 0.103309, 0.110123, 0.113654, 0.162922, 0…\n$ x1144_nm <dbl> 0.161862, 0.121239, 0.128801, 0.131457, 0.178850, 0…\n$ x1146_nm <dbl> 0.178692, 0.139799, 0.147827, 0.148927, 0.192453, 0…\n$ x1148_nm <dbl> 0.190440, 0.154684, 0.163220, 0.162499, 0.200921, 0…\n$ x1150_nm <dbl> 0.194380, 0.163895, 0.171823, 0.169830, 0.200736, 0…\n$ x1152_nm <dbl> 0.187846, 0.163702, 0.170286, 0.168042, 0.189637, 0…\n$ x1154_nm <dbl> 0.173436, 0.155861, 0.160042, 0.159366, 0.170796, 0…\n$ x1156_nm <dbl> 0.158487, 0.146830, 0.147558, 0.149124, 0.152340, 0…\n$ x1158_nm <dbl> 0.146299, 0.139954, 0.137904, 0.141988, 0.138688, 0…\n$ x1160_nm <dbl> 0.143034, 0.140520, 0.136222, 0.142149, 0.133949, 0…\n$ x1162_nm <dbl> 0.146884, 0.147677, 0.142161, 0.148574, 0.137171, 0…\n$ x1164_nm <dbl> 0.156093, 0.159739, 0.153683, 0.159490, 0.146438, 0…\n$ x1166_nm <dbl> 0.167857, 0.174511, 0.168846, 0.173304, 0.159196, 0…\n$ x1168_nm <dbl> 0.179468, 0.189393, 0.184297, 0.186928, 0.172705, 0…\n$ x1170_nm <dbl> 0.192422, 0.204829, 0.201057, 0.201306, 0.187194, 0…\n$ x1172_nm <dbl> 0.205774, 0.222001, 0.219339, 0.217320, 0.202939, 0…\n$ x1174_nm <dbl> 0.222009, 0.240784, 0.238474, 0.235025, 0.221091, 0…\n$ x1176_nm <dbl> 0.238851, 0.259705, 0.257398, 0.253360, 0.239527, 0…\n$ x1178_nm <dbl> 0.257624, 0.279918, 0.277868, 0.273140, 0.260289, 0…\n$ x1180_nm <dbl> 0.282144, 0.305844, 0.303828, 0.298527, 0.286929, 0…\n$ x1182_nm <dbl> 0.316637, 0.342649, 0.340386, 0.334143, 0.324194, 0…\n$ x1184_nm <dbl> 0.360689, 0.390617, 0.388889, 0.379790, 0.370256, 0…\n$ x1186_nm <dbl> 0.410005, 0.445219, 0.444171, 0.431122, 0.418339, 0…\n$ x1188_nm <dbl> 0.454409, 0.497316, 0.497294, 0.478623, 0.458811, 0…\n$ x1190_nm <dbl> 0.485965, 0.535230, 0.535431, 0.512772, 0.480722, 0…\n$ x1192_nm <dbl> 0.497682, 0.552865, 0.552290, 0.529281, 0.483581, 0…\n$ x1194_nm <dbl> 0.489535, 0.545858, 0.541118, 0.523347, 0.465097, 0…\n$ x1196_nm <dbl> 0.466098, 0.516133, 0.504049, 0.497873, 0.432696, 0…\n$ x1198_nm <dbl> 0.432583, 0.474097, 0.453667, 0.462310, 0.392652, 0…\n$ x1200_nm <dbl> 0.394805, 0.429533, 0.400454, 0.423781, 0.351462, 0…\n$ x1202_nm <dbl> 0.359732, 0.388781, 0.353163, 0.387603, 0.315084, 0…\n$ x1204_nm <dbl> 0.331966, 0.357836, 0.318767, 0.359001, 0.288042, 0…\n$ x1206_nm <dbl> 0.309186, 0.334257, 0.293785, 0.336412, 0.267363, 0…\n$ x1208_nm <dbl> 0.292567, 0.316834, 0.276645, 0.319126, 0.251579, 0…\n$ x1210_nm <dbl> 0.275393, 0.300208, 0.260872, 0.301999, 0.235251, 0…\n$ x1212_nm <dbl> 0.255452, 0.280041, 0.242538, 0.282530, 0.216596, 0…\n$ x1214_nm <dbl> 0.235247, 0.259029, 0.224122, 0.261341, 0.198460, 0…\n$ x1216_nm <dbl> 0.213967, 0.237029, 0.206128, 0.239522, 0.181406, 0…\n$ x1218_nm <dbl> 0.192011, 0.215364, 0.188049, 0.218119, 0.165114, 0…\n$ x1220_nm <dbl> 0.170424, 0.192902, 0.169174, 0.195463, 0.148187, 0…\n$ x1222_nm <dbl> 0.150943, 0.171982, 0.152376, 0.174744, 0.133288, 0…\n$ x1224_nm <dbl> 0.132428, 0.151256, 0.136216, 0.154083, 0.118446, 0…\n$ x1226_nm <dbl> 0.115921, 0.133448, 0.121756, 0.135921, 0.105215, 0…\n$ x1228_nm <dbl> 0.099535, 0.115749, 0.106574, 0.118325, 0.090738, 0…\n$ x1230_nm <dbl> 0.084481, 0.099265, 0.090796, 0.101493, 0.076775, 0…\n$ x1232_nm <dbl> 0.071254, 0.084244, 0.076286, 0.086168, 0.063483, 0…\n$ x1234_nm <dbl> 0.058049, 0.070886, 0.062592, 0.072287, 0.050650, 0…\n$ x1236_nm <dbl> 0.047446, 0.060095, 0.051793, 0.061218, 0.040134, 0…\n$ x1238_nm <dbl> 0.036880, 0.049378, 0.041423, 0.050044, 0.030024, 0…\n$ x1240_nm <dbl> 0.029082, 0.040630, 0.033518, 0.040826, 0.021975, 0…\n$ x1242_nm <dbl> 0.021789, 0.032988, 0.026652, 0.032597, 0.015352, 0…\n$ x1244_nm <dbl> 0.016038, 0.026985, 0.021066, 0.026030, 0.010021, 0…\n$ x1246_nm <dbl> 0.011251, 0.021400, 0.016164, 0.020097, 0.005572, 0…\n$ x1248_nm <dbl> 0.006186, 0.016041, 0.011051, 0.014526, 0.001400, 0…\n$ x1250_nm <dbl> 0.003116, 0.012094, 0.007728, 0.010441, -0.001737, …\n$ x1252_nm <dbl> -0.000998, 0.007511, 0.003719, 0.005578, -0.005442,…\n$ x1254_nm <dbl> -0.004703, 0.003694, -0.000068, 0.001684, -0.008756…\n$ x1256_nm <dbl> -0.009270, -0.001214, -0.004753, -0.002925, -0.0128…\n$ x1258_nm <dbl> -0.012907, -0.005412, -0.008519, -0.007083, -0.0162…\n$ x1260_nm <dbl> -0.017234, -0.010057, -0.012552, -0.011555, -0.0201…\n$ x1262_nm <dbl> -0.020832, -0.014084, -0.016265, -0.015370, -0.0235…\n$ x1264_nm <dbl> -0.024051, -0.017764, -0.019766, -0.018619, -0.0270…\n$ x1266_nm <dbl> -0.027297, -0.021488, -0.023338, -0.021719, -0.0301…\n$ x1268_nm <dbl> -0.029792, -0.024172, -0.025883, -0.023654, -0.0323…\n$ x1270_nm <dbl> -0.032400, -0.027224, -0.029186, -0.025991, -0.0353…\n$ x1272_nm <dbl> -0.033746, -0.029156, -0.031037, -0.027169, -0.0369…\n$ x1274_nm <dbl> -0.035721, -0.031021, -0.033466, -0.028784, -0.0383…\n$ x1276_nm <dbl> -0.036701, -0.032030, -0.034469, -0.029554, -0.0394…\n$ x1278_nm <dbl> -0.037363, -0.032565, -0.035675, -0.030701, -0.0404…\n$ x1280_nm <dbl> -0.037377, -0.032049, -0.035766, -0.031122, -0.0405…\n$ x1282_nm <dbl> -0.037489, -0.031394, -0.036086, -0.031775, -0.0405…\n$ x1284_nm <dbl> -0.037695, -0.031097, -0.036513, -0.032641, -0.0406…\n$ x1286_nm <dbl> -0.036906, -0.029836, -0.036003, -0.032724, -0.0402…\n$ x1288_nm <dbl> -0.037094, -0.029678, -0.036171, -0.033748, -0.0403…\n$ x1290_nm <dbl> -0.036752, -0.028968, -0.035911, -0.034132, -0.0398…\n$ x1292_nm <dbl> -0.037211, -0.029269, -0.036362, -0.035013, -0.0398…\n$ x1294_nm <dbl> -0.037308, -0.029262, -0.036318, -0.035620, -0.0397…\n$ x1296_nm <dbl> -0.037673, -0.029877, -0.036997, -0.036407, -0.0400…\n$ x1298_nm <dbl> -0.037877, -0.030030, -0.037092, -0.036595, -0.0400…\n$ x1300_nm <dbl> -0.038212, -0.030565, -0.037218, -0.036742, -0.0400…\n$ x1302_nm <dbl> -0.038426, -0.031404, -0.037411, -0.037076, -0.0401…\n$ x1304_nm <dbl> -0.038534, -0.032320, -0.037429, -0.037230, -0.0403…\n$ x1306_nm <dbl> -0.039103, -0.033693, -0.038010, -0.037877, -0.0411…\n$ x1308_nm <dbl> -0.038933, -0.034155, -0.037523, -0.037530, -0.0406…\n$ x1310_nm <dbl> -0.039611, -0.035496, -0.038127, -0.038428, -0.0415…\n$ x1312_nm <dbl> -0.039168, -0.035629, -0.037544, -0.037948, -0.0407…\n$ x1314_nm <dbl> -0.038598, -0.035932, -0.037255, -0.037560, -0.0402…\n$ x1316_nm <dbl> -0.037733, -0.035289, -0.036148, -0.036566, -0.0390…\n$ x1318_nm <dbl> -0.037027, -0.034887, -0.035291, -0.035762, -0.0379…\n$ x1320_nm <dbl> -0.035891, -0.034467, -0.034674, -0.035156, -0.0374…\n$ x1322_nm <dbl> -0.034290, -0.033427, -0.033358, -0.033697, -0.0361…\n$ x1324_nm <dbl> -0.033224, -0.032634, -0.032758, -0.032653, -0.0353…\n$ x1326_nm <dbl> -0.031475, -0.030815, -0.031228, -0.030570, -0.0334…\n$ x1328_nm <dbl> -0.030071, -0.029493, -0.030097, -0.028914, -0.0324…\n$ x1330_nm <dbl> -0.028086, -0.027298, -0.027881, -0.026203, -0.0302…\n$ x1332_nm <dbl> -0.026779, -0.026081, -0.026697, -0.024685, -0.0290…\n$ x1334_nm <dbl> -0.024450, -0.024199, -0.024809, -0.022470, -0.0267…\n$ x1336_nm <dbl> -0.022709, -0.022546, -0.022950, -0.020930, -0.0245…\n$ x1338_nm <dbl> -0.020088, -0.020154, -0.020248, -0.018910, -0.0215…\n$ x1340_nm <dbl> -0.016814, -0.017032, -0.016845, -0.016121, -0.0179…\n$ x1342_nm <dbl> -0.013638, -0.013717, -0.013423, -0.013279, -0.0139…\n$ x1344_nm <dbl> -0.008755, -0.008264, -0.007559, -0.008334, -0.0080…\n$ x1346_nm <dbl> -0.003402, -0.002064, -0.000991, -0.003372, -0.0018…\n$ x1348_nm <dbl> 0.004585, 0.007304, 0.008786, 0.004753, 0.007269, 0…\n$ x1350_nm <dbl> 0.015708, 0.020121, 0.022739, 0.016161, 0.019779, 0…\n$ x1352_nm <dbl> 0.032342, 0.039083, 0.043209, 0.033429, 0.038201, 0…\n$ x1354_nm <dbl> 0.055075, 0.064193, 0.069797, 0.056411, 0.062477, 0…\n$ x1356_nm <dbl> 0.083261, 0.095086, 0.102595, 0.085082, 0.092206, 0…\n$ x1358_nm <dbl> 0.114275, 0.128027, 0.137954, 0.116397, 0.123556, 0…\n$ x1360_nm <dbl> 0.142234, 0.158800, 0.169100, 0.145471, 0.151961, 0…\n$ x1362_nm <dbl> 0.165117, 0.183265, 0.194530, 0.168918, 0.174343, 0…\n$ x1364_nm <dbl> 0.181014, 0.201249, 0.212629, 0.185935, 0.189944, 0…\n$ x1366_nm <dbl> 0.194140, 0.216701, 0.227677, 0.200819, 0.202222, 0…\n$ x1368_nm <dbl> 0.207728, 0.232922, 0.243204, 0.215868, 0.215936, 0…\n$ x1370_nm <dbl> 0.225084, 0.252380, 0.262941, 0.234213, 0.233155, 0…\n$ x1372_nm <dbl> 0.245205, 0.274944, 0.285764, 0.255771, 0.254445, 0…\n$ x1374_nm <dbl> 0.267331, 0.298853, 0.309733, 0.279190, 0.277749, 0…\n$ x1376_nm <dbl> 0.290558, 0.322715, 0.333065, 0.303732, 0.301495, 0…\n$ x1378_nm <dbl> 0.311237, 0.342958, 0.352530, 0.325323, 0.320695, 0…\n$ x1380_nm <dbl> 0.328961, 0.360426, 0.369372, 0.344871, 0.336187, 0…\n$ x1382_nm <dbl> 0.341901, 0.372804, 0.380216, 0.359555, 0.345160, 0…\n$ x1384_nm <dbl> 0.350544, 0.381960, 0.387849, 0.370893, 0.350677, 0…\n$ x1386_nm <dbl> 0.357751, 0.389191, 0.394677, 0.378461, 0.355522, 0…\n$ x1388_nm <dbl> 0.364172, 0.397112, 0.402078, 0.386247, 0.362320, 0…\n$ x1390_nm <dbl> 0.370471, 0.403062, 0.408582, 0.392268, 0.368313, 0…\n$ x1392_nm <dbl> 0.373154, 0.406006, 0.410932, 0.393815, 0.370052, 0…\n$ x1394_nm <dbl> 0.368176, 0.401295, 0.406170, 0.388471, 0.365085, 0…\n$ x1396_nm <dbl> 0.356007, 0.389064, 0.393839, 0.376881, 0.352979, 0…\n$ x1398_nm <dbl> 0.341120, 0.373591, 0.376215, 0.360814, 0.337215, 0…\n$ x1400_nm <dbl> 0.325587, 0.356204, 0.357096, 0.344653, 0.321339, 0…\n$ x1402_nm <dbl> 0.313571, 0.342050, 0.340687, 0.331744, 0.308459, 0…\n$ x1404_nm <dbl> 0.306847, 0.332780, 0.329190, 0.323812, 0.299508, 0…\n$ x1406_nm <dbl> 0.305310, 0.329005, 0.322999, 0.321500, 0.295653, 0…\n$ x1408_nm <dbl> 0.304471, 0.328951, 0.321164, 0.323119, 0.295204, 0…\n$ x1410_nm <dbl> 0.304125, 0.328051, 0.319110, 0.322928, 0.292860, 0…\n$ x1412_nm <dbl> 0.298698, 0.322452, 0.311924, 0.317972, 0.286862, 0…\n$ x1414_nm <dbl> 0.285589, 0.308979, 0.298671, 0.305534, 0.274217, 0…\n$ x1416_nm <dbl> 0.267933, 0.290704, 0.280373, 0.288311, 0.257335, 0…\n$ x1418_nm <dbl> 0.247573, 0.269241, 0.258819, 0.267978, 0.237130, 0…\n$ x1420_nm <dbl> 0.229774, 0.249374, 0.238800, 0.248524, 0.219594, 0…\n$ x1422_nm <dbl> 0.215690, 0.233860, 0.222677, 0.232934, 0.205760, 0…\n$ x1424_nm <dbl> 0.207192, 0.224821, 0.213798, 0.223277, 0.197859, 0…\n$ x1426_nm <dbl> 0.202051, 0.220504, 0.209824, 0.217777, 0.195243, 0…\n$ x1428_nm <dbl> 0.201819, 0.220685, 0.210649, 0.216610, 0.196015, 0…\n$ x1430_nm <dbl> 0.202353, 0.222163, 0.212908, 0.216723, 0.197585, 0…\n$ x1432_nm <dbl> 0.201612, 0.222347, 0.214331, 0.216083, 0.198133, 0…\n$ x1434_nm <dbl> 0.199291, 0.221434, 0.214048, 0.214255, 0.196885, 0…\n$ x1436_nm <dbl> 0.193770, 0.215752, 0.209266, 0.208305, 0.190623, 0…\n$ x1438_nm <dbl> 0.186669, 0.209041, 0.202558, 0.201097, 0.184152, 0…\n$ x1440_nm <dbl> 0.178426, 0.200297, 0.193675, 0.192070, 0.175524, 0…\n$ x1442_nm <dbl> 0.171231, 0.192172, 0.185359, 0.183795, 0.167989, 0…\n$ x1444_nm <dbl> 0.164144, 0.183974, 0.176963, 0.176042, 0.159801, 0…\n$ x1446_nm <dbl> 0.157259, 0.175816, 0.169181, 0.168643, 0.152625, 0…\n$ x1448_nm <dbl> 0.149194, 0.167254, 0.159964, 0.160181, 0.144600, 0…\n$ x1450_nm <dbl> 0.139789, 0.156358, 0.149142, 0.150664, 0.134160, 0…\n$ x1452_nm <dbl> 0.130173, 0.146486, 0.139082, 0.141494, 0.125008, 0…\n$ x1454_nm <dbl> 0.120745, 0.137275, 0.129739, 0.133234, 0.115873, 0…\n$ x1456_nm <dbl> 0.113733, 0.130180, 0.122022, 0.126515, 0.109185, 0…\n$ x1458_nm <dbl> 0.107519, 0.123365, 0.116297, 0.120700, 0.103171, 0…\n$ x1460_nm <dbl> 0.103765, 0.119438, 0.113173, 0.117645, 0.100027, 0…\n$ x1462_nm <dbl> 0.101175, 0.116286, 0.110954, 0.115052, 0.098121, 0…\n$ x1464_nm <dbl> 0.100106, 0.114482, 0.110095, 0.113629, 0.097745, 0…\n$ x1466_nm <dbl> 0.099068, 0.112444, 0.109058, 0.112397, 0.097652, 0…\n$ x1468_nm <dbl> 0.097315, 0.108714, 0.106665, 0.108961, 0.096193, 0…\n$ x1470_nm <dbl> 0.095724, 0.106003, 0.104405, 0.106436, 0.095738, 0…\n$ x1472_nm <dbl> 0.093093, 0.102389, 0.100941, 0.102798, 0.093557, 0…\n$ x1474_nm <dbl> 0.090002, 0.098235, 0.097474, 0.098874, 0.090709, 0…\n$ x1476_nm <dbl> 0.086612, 0.093959, 0.093497, 0.094944, 0.087039, 0…\n$ x1478_nm <dbl> 0.084922, 0.091774, 0.091673, 0.092621, 0.085000, 0…\n$ x1480_nm <dbl> 0.080780, 0.087636, 0.087493, 0.088422, 0.080375, 0…\n$ x1482_nm <dbl> 0.077849, 0.084646, 0.084593, 0.084868, 0.077113, 0…\n$ x1484_nm <dbl> 0.074536, 0.082097, 0.081474, 0.081632, 0.073986, 0…\n$ x1486_nm <dbl> 0.070910, 0.079089, 0.078340, 0.078056, 0.069980, 0…\n$ x1488_nm <dbl> 0.068700, 0.077735, 0.076682, 0.076356, 0.067515, 0…\n$ x1490_nm <dbl> 0.064584, 0.074444, 0.073916, 0.072892, 0.064092, 0…\n$ x1492_nm <dbl> 0.061726, 0.071813, 0.071351, 0.069921, 0.060768, 0…\n$ x1494_nm <dbl> 0.057395, 0.068586, 0.068287, 0.066060, 0.057105, 0…\n$ x1496_nm <dbl> 0.053704, 0.065580, 0.065568, 0.062555, 0.054163, 0…\n$ x1498_nm <dbl> 0.049329, 0.061501, 0.061216, 0.057895, 0.050508, 0…\n$ x1500_nm <dbl> 0.044695, 0.056740, 0.055821, 0.052995, 0.045830, 0…\n$ x1502_nm <dbl> 0.039937, 0.051068, 0.049529, 0.047908, 0.040213, 0…\n$ x1504_nm <dbl> 0.034641, 0.045104, 0.043150, 0.042637, 0.034258, 0…\n$ x1506_nm <dbl> 0.030164, 0.040192, 0.037184, 0.038187, 0.029202, 0…\n$ x1508_nm <dbl> 0.024898, 0.034145, 0.030828, 0.032880, 0.023555, 0…\n$ x1510_nm <dbl> 0.021588, 0.029970, 0.026308, 0.029345, 0.019996, 0…\n$ x1512_nm <dbl> 0.017102, 0.024131, 0.020805, 0.024026, 0.015186, 0…\n$ x1514_nm <dbl> 0.014860, 0.020670, 0.016907, 0.020904, 0.012201, 0…\n$ x1516_nm <dbl> 0.010854, 0.016931, 0.013194, 0.017431, 0.008836, 0…\n$ x1518_nm <dbl> 0.008205, 0.013083, 0.009759, 0.014136, 0.005561, 0…\n$ x1520_nm <dbl> 0.005869, 0.010199, 0.007271, 0.011749, 0.003463, 0…\n$ x1522_nm <dbl> 0.003920, 0.007926, 0.004796, 0.009634, 0.001051, 0…\n$ x1524_nm <dbl> 0.002107, 0.005733, 0.002720, 0.007699, -0.001164, …\n$ x1526_nm <dbl> -0.000203, 0.002833, 0.000243, 0.005545, -0.003490,…\n$ x1528_nm <dbl> -0.001072, 0.001596, -0.001196, 0.004693, -0.004459…\n$ x1530_nm <dbl> -0.003325, -0.000928, -0.003965, 0.002553, -0.00740…\n$ x1532_nm <dbl> -0.004707, -0.002545, -0.005704, 0.001494, -0.00901…\n$ x1534_nm <dbl> -0.006635, -0.003656, -0.007156, -0.000102, -0.0101…\n$ x1536_nm <dbl> -0.008403, -0.006217, -0.009860, -0.003056, -0.0127…\n$ x1538_nm <dbl> -0.009317, -0.006885, -0.010959, -0.004391, -0.0138…\n$ x1540_nm <dbl> -0.011351, -0.008307, -0.012194, -0.006868, -0.0151…\n$ x1542_nm <dbl> -0.012280, -0.009142, -0.013605, -0.008665, -0.0158…\n$ x1544_nm <dbl> -0.013474, -0.010409, -0.014564, -0.010355, -0.0171…\n$ x1546_nm <dbl> -0.014453, -0.010983, -0.015313, -0.011261, -0.0175…\n$ x1548_nm <dbl> -0.015262, -0.012033, -0.015723, -0.011896, -0.0181…\n$ x1550_nm <dbl> -0.015588, -0.012020, -0.015263, -0.011357, -0.0179…\n$ x1552_nm <dbl> -0.016349, -0.013442, -0.016537, -0.012562, -0.0191…\n$ x1554_nm <dbl> -0.016028, -0.012709, -0.015581, -0.011408, -0.0183…\n$ x1556_nm <dbl> -0.016209, -0.011563, -0.014147, -0.010164, -0.0173…\n$ x1558_nm <dbl> -0.015985, -0.010432, -0.013280, -0.009194, -0.0168…\n$ x1560_nm <dbl> -0.015483, -0.010091, -0.013291, -0.009642, -0.0167…\n$ x1562_nm <dbl> -0.014216, -0.007960, -0.012268, -0.008700, -0.0155…\n$ x1564_nm <dbl> -0.013790, -0.006697, -0.011176, -0.007899, -0.0145…\n$ x1566_nm <dbl> -0.013844, -0.005367, -0.010897, -0.006986, -0.0146…\n$ x1568_nm <dbl> -0.011773, -0.003432, -0.009383, -0.005475, -0.0131…\n$ x1570_nm <dbl> -0.010432, -0.002665, -0.008983, -0.004838, -0.0130…\n$ x1572_nm <dbl> -0.009606, -0.001307, -0.007542, -0.003785, -0.0112…\n$ x1574_nm <dbl> -0.008209, -0.000406, -0.005716, -0.002646, -0.0101…\n$ x1576_nm <dbl> -0.006732, 0.001300, -0.003888, -0.001373, -0.00784…\n$ x1578_nm <dbl> -0.004573, 0.003107, -0.001115, 0.000559, -0.005406…\n$ x1580_nm <dbl> -0.003433, 0.003075, 0.000176, 0.001422, -0.004259,…\n$ x1582_nm <dbl> -0.001200, 0.004513, 0.002464, 0.003357, -0.002039,…\n$ x1584_nm <dbl> -0.000066, 0.005336, 0.003951, 0.005481, -0.000655,…\n$ x1586_nm <dbl> 0.002759, 0.008496, 0.008541, 0.010048, 0.003930, 0…\n$ x1588_nm <dbl> 0.004036, 0.009262, 0.009676, 0.011528, 0.004579, 0…\n$ x1590_nm <dbl> 0.006620, 0.010982, 0.012153, 0.014357, 0.007207, 0…\n$ x1592_nm <dbl> 0.008709, 0.012330, 0.013639, 0.015642, 0.008612, 0…\n$ x1594_nm <dbl> 0.011810, 0.015301, 0.016898, 0.018304, 0.012129, 0…\n$ x1596_nm <dbl> 0.014426, 0.018270, 0.019847, 0.020612, 0.015541, 0…\n$ x1598_nm <dbl> 0.017254, 0.020912, 0.022470, 0.022399, 0.018535, 0…\n$ x1600_nm <dbl> 0.020633, 0.024617, 0.026069, 0.025038, 0.022522, 0…\n$ x1602_nm <dbl> 0.025032, 0.028188, 0.029714, 0.027860, 0.026739, 0…\n$ x1604_nm <dbl> 0.029496, 0.033321, 0.034281, 0.032481, 0.032223, 0…\n$ x1606_nm <dbl> 0.034242, 0.037755, 0.039133, 0.036772, 0.037103, 0…\n$ x1608_nm <dbl> 0.039735, 0.043515, 0.045228, 0.042282, 0.043411, 0…\n$ x1610_nm <dbl> 0.046803, 0.050530, 0.051833, 0.049100, 0.051336, 0…\n$ x1612_nm <dbl> 0.053743, 0.058033, 0.059036, 0.056030, 0.059517, 0…\n$ x1614_nm <dbl> 0.062788, 0.067009, 0.068655, 0.065351, 0.069594, 0…\n$ x1616_nm <dbl> 0.073531, 0.077781, 0.079319, 0.075811, 0.081145, 0…\n$ x1618_nm <dbl> 0.087246, 0.091191, 0.092339, 0.089083, 0.096240, 0…\n$ x1620_nm <dbl> 0.104000, 0.106815, 0.107952, 0.105323, 0.112862, 0…\n$ x1622_nm <dbl> 0.124210, 0.123946, 0.123435, 0.123006, 0.131164, 0…\n$ x1624_nm <dbl> 0.147924, 0.146013, 0.143631, 0.145753, 0.153401, 0…\n$ x1626_nm <dbl> 0.175024, 0.171884, 0.166498, 0.173142, 0.178326, 0…\n$ x1628_nm <dbl> 0.202711, 0.197074, 0.188171, 0.200956, 0.202094, 0…\n$ x1630_nm <dbl> 0.232547, 0.224837, 0.210848, 0.231804, 0.228229, 0…\n$ x1632_nm <dbl> 0.259828, 0.250088, 0.232041, 0.259687, 0.252492, 0…\n$ x1634_nm <dbl> 0.282117, 0.270191, 0.249246, 0.281929, 0.273077, 0…\n$ x1636_nm <dbl> 0.299937, 0.284011, 0.261166, 0.297769, 0.290969, 0…\n$ x1638_nm <dbl> 0.313416, 0.290728, 0.271317, 0.308358, 0.307760, 0…\n$ x1640_nm <dbl> 0.322870, 0.294182, 0.276957, 0.311430, 0.321736, 0…\n$ x1642_nm <dbl> 0.327217, 0.293247, 0.279662, 0.311149, 0.331647, 0…\n$ x1644_nm <dbl> 0.331907, 0.289916, 0.280366, 0.308663, 0.337756, 0…\n$ x1646_nm <dbl> 0.336522, 0.291554, 0.284626, 0.308939, 0.344823, 0…\n$ x1648_nm <dbl> 0.341847, 0.293929, 0.289691, 0.311825, 0.352189, 0…\n$ x1650_nm <dbl> 0.350585, 0.301816, 0.297987, 0.319857, 0.362563, 0…\n$ x1652_nm <dbl> 0.368032, 0.316670, 0.312909, 0.335700, 0.379202, 0…\n$ x1654_nm <dbl> 0.393494, 0.336533, 0.333932, 0.355742, 0.404428, 0…\n$ x1656_nm <dbl> 0.427370, 0.363209, 0.362284, 0.384754, 0.441534, 0…\n$ x1658_nm <dbl> 0.472357, 0.399271, 0.400352, 0.424769, 0.494011, 0…\n$ x1660_nm <dbl> 0.528721, 0.443964, 0.445686, 0.472003, 0.557655, 0…\n$ x1662_nm <dbl> 0.593168, 0.499024, 0.504338, 0.531050, 0.635493, 0…\n$ x1664_nm <dbl> 0.666647, 0.559683, 0.567039, 0.596606, 0.721107, 0…\n$ x1666_nm <dbl> 0.743945, 0.624577, 0.635874, 0.662997, 0.803044, 0…\n$ x1668_nm <dbl> 0.833972, 0.697450, 0.711690, 0.744126, 0.893778, 0…\n$ x1670_nm <dbl> 0.909368, 0.770433, 0.792939, 0.817398, 0.968985, 0…\n$ x1672_nm <dbl> 0.985332, 0.848769, 0.872021, 0.895422, 1.050312, 0…\n$ x1674_nm <dbl> 1.051159, 0.916590, 0.938915, 0.959761, 1.104573, 1…\n$ x1676_nm <dbl> 1.107395, 0.975048, 1.004690, 1.021595, 1.152964, 1…\n$ x1678_nm <dbl> 1.158488, 1.036499, 1.067934, 1.082497, 1.192131, 1…\n$ x1680_nm <dbl> 1.174795, 1.085067, 1.108813, 1.117447, 1.219254, 1…\n$ x1682_nm <dbl> 1.198461, 1.128877, 1.147964, 1.160089, 1.252712, 1…\n$ x1684_nm <dbl> 1.224243, 1.148342, 1.167798, 1.169350, 1.238013, 1…\n$ x1686_nm <dbl> 1.242645, 1.189116, 1.198287, 1.201066, 1.259616, 1…\n$ x1688_nm <dbl> 1.250789, 1.223242, 1.237383, 1.233299, 1.273713, 1…\n$ x1690_nm <dbl> 1.246626, 1.253306, 1.260979, 1.262966, 1.296524, 1…\n$ x1692_nm <dbl> 1.250985, 1.282889, 1.276677, 1.272709, 1.299507, 1…\n$ x1694_nm <dbl> 1.264189, 1.215065, 1.218871, 1.211068, 1.226448, 1…\n$ x1696_nm <dbl> 1.244678, 1.225211, 1.223132, 1.215044, 1.230718, 1…\n$ x1698_nm <dbl> 1.245913, 1.227985, 1.230321, 1.232655, 1.232864, 1…\n$ x1700_nm <dbl> 1.221135, 1.198851, 1.208742, 1.206696, 1.202926, 1…\n\nEDA\nCheck for missing values using purrr::map()\n\n\ngasoline_tidy %>% \n  purrr::map(is.na) %>% \n  map_df(sum) %>% \n  tidy() %>% \n  dplyr::select(column, mean) %>% \n  as_tibble() %>% \n  filter(mean>0) # no missing values\n\n\n# A tibble: 0 x 2\n# … with 2 variables: column <chr>, mean <dbl>\n\nFor other datasets, one should check the correlation again, but in this case, as this is a NIR dataset, I will not do this step.\nSplit dataset\n\n\n# initial split\nset.seed(20210308)\ngasoline_split <- initial_split(gasoline_tidy, prop = 0.8)\n\ngasoline_training <- gasoline_split %>% \n  training()\n\ngasoline_testing <- gasoline_split %>% \n  testing()\n\ngasoline_cv <- vfold_cv(gasoline_training) # to tune number of components later\n\n\n\nModelling\n\n\n# recipe\n\ngasoline_reciped <- recipe(octane ~ ., data = gasoline_training) %>% \n  update_role(octane, new_role = \"outcome\") %>% \n  step_normalize(all_predictors())\n\ngasoline_reciped\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor        401\n\nOperations:\n\nCentering and scaling for all_predictors()\n\n# fit model\n\npls_model <- plsmod::pls(num_comp = tune()) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"mixOmics\")\n\n# put into workflow\n\npls_workflow <- workflow() %>% \n  add_recipe(gasoline_reciped) %>% \n  add_model(pls_model)\n\n# create grid\npls_grid <- expand.grid(num_comp = seq (from = 1, to = 20, by = 1))\n\ntuned_pls_results <- pls_workflow %>% \n  tune_grid(resamples = gasoline_cv,\n            grid = pls_grid,\n            metrics = metric_set(mae, rmse, rsq))\n\n(model_results <- tuned_pls_results %>% \n  collect_metrics())\n\n\n# A tibble: 60 x 7\n   num_comp .metric .estimator  mean     n std_err .config            \n      <dbl> <chr>   <chr>      <dbl> <int>   <dbl> <chr>              \n 1        1 mae     standard   1.15     10 0.122   Preprocessor1_Mode…\n 2        1 rmse    standard   1.29     10 0.137   Preprocessor1_Mode…\n 3        1 rsq     standard   0.371    10 0.104   Preprocessor1_Mode…\n 4        2 mae     standard   0.573    10 0.0745  Preprocessor1_Mode…\n 5        2 rmse    standard   0.691    10 0.0876  Preprocessor1_Mode…\n 6        2 rsq     standard   0.716    10 0.0874  Preprocessor1_Mode…\n 7        3 mae     standard   0.212    10 0.0280  Preprocessor1_Mode…\n 8        3 rmse    standard   0.259    10 0.0289  Preprocessor1_Mode…\n 9        3 rsq     standard   0.972    10 0.00967 Preprocessor1_Mode…\n10        4 mae     standard   0.187    10 0.0177  Preprocessor1_Mode…\n# … with 50 more rows\n\nVisualize:\n\n\n# plot\n\ntuned_pls_results %>% \n  collect_metrics() %>% \n  ggplot(aes(num_comp, mean, col = .metric)) +\n  geom_point() +\n  geom_line() +\n  scale_x_continuous(n.breaks = 20) +\n  labs(x = \"Number of components\",\n       y = \"Indicator\",\n       title = \"Plot of MAE, RMSE and R-SQ vs number of components for TRAINING dataset, with 10-fold repeated cross validation\",\n       subtitle = \"Optimal number of components is 3\") +\n facet_grid(.metric ~.) +\n  theme_few() +\n  theme(legend.position = \"none\")\n\n\n\n\nCheck against numerical values:\n\n\n# check against numerical values\ntuned_best <- tuned_pls_results %>% \n  select_best(\"rsq\") \n\ntuned_best\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1        4 Preprocessor1_Model04\n\n# check with other indicators\ntuned_pls_results %>% \n  select_best(\"mae\") \n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1        4 Preprocessor1_Model04\n\ntuned_pls_results %>% \n  select_best(\"rmse\")\n\n\n# A tibble: 1 x 2\n  num_comp .config              \n     <dbl> <chr>                \n1        4 Preprocessor1_Model04\n\nIn this case, I will go for num_comp = 4. The aim of PLS is to reduce number of variables, but there would be a slight trade-off in terms of accuracy/error in favor of simplicity of model when determining the number of components.\nUpdate model and workflow:\n\n\nupdated_pls_model <-  plsmod::pls(num_comp = 4) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"mixOmics\")\n\nupdated_workflow <- pls_workflow %>% \n  update_model(updated_pls_model)\n  \n\npls_fit <- updated_workflow %>% \n  fit(data = gasoline_training)\n\n\n\nCheck the most important X variables for the updated model:\n\n\n# check the most important predictors\n\ntidy_pls <- pls_fit %>% \n  pull_workflow_fit() %>% \n  tidy()\n\n\n# variable importance\ntidy_pls %>% \n  filter(term != \"Y\", # outcome variable col name\n         component == c(1:4)) %>% \n  group_by(component) %>% \n  slice_max(abs(value), n = 20) %>% \n  ungroup() %>% \n  ggplot(aes(value, fct_reorder(term, value), fill = factor(component))) +\n  geom_col(show.legend = F) +\n  facet_wrap( ~ component, scales = \"free_y\") +\n  labs( y = NULL) +\n  theme_few()\n\n\n\n\nAssess\n\n\n# results_train\npls_fit %>% \n  predict(new_data = gasoline_training) %>% \n  mutate(truth = gasoline_training$octane) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point() +\n  geom_abline() +\n  labs(title = \"Actual vs Predicted for TRAINING dataset\",\n       x = \"Actual Octane Number\",\n       y = \"Predicted Octane Number\") +\n  theme_few()\n\n\n\npls_fit %>% \n  predict(new_data = gasoline_training) %>% \n  mutate(truth = gasoline_training$octane) %>% \n  metrics(truth = truth, estimate = .pred) %>% \n  as_tibble() # rsq is 0.981 for training dataset\n\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.207\n2 rsq     standard       0.981\n3 mae     standard       0.168\n\n# results_test\npls_fit %>% \n  predict(new_data = gasoline_testing) %>% \n  mutate(truth = gasoline_testing$octane) %>% \n  ggplot(aes(truth, .pred)) +\n  geom_point(col = \"deepskyblue3\") +\n  geom_abline(col = \"deepskyblue3\") +\n  labs(title = \"Actual vs Predicted for TESTING dataset\",\n       x = \"Actual Octane Number\",\n       y = \"Predicted Octane Number\") +\n  theme_few()\n\n\n\npls_fit %>% \n  predict(new_data = gasoline_testing) %>% \n  mutate(truth = gasoline_testing$octane) %>% \n  metrics(truth = truth, estimate = .pred) %>% \n  as_tibble() # r-sq is 0.989 for testing dataset\n\n\n# A tibble: 3 x 3\n  .metric .estimator .estimate\n  <chr>   <chr>          <dbl>\n1 rmse    standard       0.222\n2 rsq     standard       0.983\n3 mae     standard       0.151\n\nTo predict future data\nIn this case, I will just take the first dataset and check if the predicted value is the actual value. This should not be done, but I did it just to understand how the workflow is.\n\n\ntrial_data <- gasoline_tidy %>% \n  head((1)) %>% \n  dplyr::select(-octane) # octane = 85.3\n\npls_fit %>% \n  predict(trial_data) # 85.3\n\n\n# A tibble: 1 x 1\n  .pred\n  <dbl>\n1  85.2\n\n# actual value = predicted value\n\n\n\nReflections\nThrough this exercise, I learnt:\nhow to format the matrix data into a tibble dataframe. I have not worked with AsIs structure before and had to look up stacksoverflow to understand how to code to get a plot for NIR spectra\nhow to apply tidymodels for pls modelling. I also learnt that there were different types of pls algorithms. Perhaps I would compare the different model algorithms for my next practice.\nhow to extract variable importance\nhow to predict Y variable using a new dataset, although in this case I did not use new data\nI would like to learn how to model for multi-outcome data in the future.\nReferences\nhttps://stackoverflow.com/questions/64254295/predictor-importance-for-pls-model-trained-with-tidymodels\nhttps://rpubs.com/RandallThompson15/HW10_624\nhttps://www.tidyverse.org/blog/2020/04/parsnip-adjacent/\nApplied Predictive Modelling, Max Kuhn and Kjell Johnson, Chapter 6. http://appliedpredictivemodeling.com/\n\n\n\n",
    "preview": "posts/20210309_tidy models plsr gasoline/Tidy-Models---Partial-Least-Squares-Regression---gasoline_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-10T11:55:27+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210301_tidy models regression 1/",
    "title": "Tidy Models - Regression",
    "description": "Predicting numerical outcomes using linear regression and random forest",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-03-01",
    "categories": [],
    "contents": "\nOverview\nThis is an exercise for me to revise the tidymodels way of carrying out regression using linear regression (OLS) and random forest models. Only after going through the readings, my course materials, and actually trying to work through a dataset myself do I really appreciate what my Prof was trying to teach during his machine learning course..\nI will be working on a dataset familiar to me - the white wine dataset.\nMachine learning can be divided into supervised and unsupervised machine learning - the differentiating point is whether you know the outcome. In this case, I want to apply what I have learnt on predicting a known numerical outcome - this would be regression. If I want to predict a known categorical outcome - for eg whether the wine is red or white, then that would be classification. If I am unsure what are the types of wine, and just want to group them, then that would be clustering.\nFor regression, I could work on predicting the quality of wine from the white wine dataset, the quality of wine from the red wine dataset, or the quality of wine from both the red and white wine dataset.\nAs a start, let me try to predict the quality score of white wine from the various attributes.\nGeneral Workflow for Predicting Numerical Outcome using Linear Regression and Random Forest\nMost of the points mentioned below were taken from: https://jhudatascience.org/tidyversecourse/model.html#summary-of-tidymodels.\nThis was a great read for me to frame my learning and see the whole picture\nThe general steps are outlined below:\n1. Import the data\nI will import the white wine dataset from https://archive.ics.uci.edu/ml/datasets/wine+quality. The attributes are:\nFeatures/X variables\nfixed acidity\nvolatile acidity\ncitric acid\nresidual sugar\nchlorides\nfree sulfur dioxide\ntotal sulfur dioxide\ndensity\npH\nsulphates\nalcohol\nOutcome/Y variable: 12 - quality (score between 0 and 10)\n2. Define the question you want to ask the data\nCan I predict the quality score (Y variable), based on the physicochemical attributes of white wine (X variables)?\n3. Clean and transform the data\nIn this case, the data is already in a tidy format. As for transformations required, I will leave it to the pre-processing step later.\n4. Exploratory data analysis (EDA)\nWhat do you look out for when doing EDA?\nShape of data: How many Y and how many X variables are there? How many observations are there? The number of observations (n) and number of parameters (p) may affect your choice of models.\nType of variables: are they numerical or categorical? Or are they numerical, but actually can be transformed into categorical (eg month of the year), or are they categorical, but should be transformed into dummy variables (eg for regression)?\nAre there any missing values? This may affect the modelling as some models cannot handle missing values.\nWithin each variable, what is the min, max, mean, median, range? Are the datapoints normally distributed, or skewed? This affects whether modelling, for eg OLS regression can be carried out, or should further transformations be carried out first?\nHow are the X variables related to each other? Are they correlated? What is the strength of correlation? Is there a multi-collinearity issue? These are points to be addressed for linear regression.\n5. Preprocessing the data\nAfter identifying all the “touch-up” points required for successful modelling, the data may be pre-processed using the recipes package. This is really a powerful package that can transform the data the way you want, using single-line commands (as compared to multiple clicks of the mouse). However, it requires the user to know what steps are to be taken.\nA list of preprocessig steps is given below:\nhttps://recipes.tidymodels.org/reference/index.html#section-basic-functions\nLike cooking, this is part art part science. If I want to do missing values imputation, which kind of imputation do I use? I am still learning as I go along for this step..\nIn a way, this preprocessing step helps you to zoom in razor sharp to the important X variables that can be used to predict Y. These X variables may exist as hidden variables that need carving out and polishing/transformation. There may be X variables that are of not much importance, so it is important to extract relevant information and keep the number of variables as small as possible without compromising on accuracy. In other words, that is called feature engineering.\n6. Carry out modelling on the training dataset\nSplit dataset into training, testing and cross-validation. The training dataset is for you to train models with. The cross-validation dataset should be a subset of training dataset, for tuning different parameters of the model to make it an even better model. Cross-validation is useful when the number of observations isn’t big enough to split into three different datasets for training, validation and testing. Instead, the data is randomly partitioned into subsamples to be used as training dataset for one partition, and the same subsample would be used as test dataset for another partition. In repeated cross-validation, the cross-validation is repeated many times, giving random partitions of the original sample. The test dataset is for testing the trained model to see if the model is able to deliver predictive results.\nUsually, you will train more than one model, and then compare the results. The choice of model could span over simple, easy to understand linear models, or difficult to interpret but accurate models (eg neural networks which is like a blackbox). The results of the trained dataset will usually not fare too badly, since the model was built using the training dataset. Different models may require different preprocessing and different types of parameter tuning.\n7. Assessing the test dataset.\nA litmus test of whether the model works is to look at how well the model performs its predictive task when a set of completely new data is provided to the model. Models may be assessed in terms of r-sq, root mean square error or mean absolute error to judge the performance.\nThe indicators above give us an understanding of how accurate the model is in terms of predicting new data. A model that is over-fitted fits the training dataset well, but is unable to predict the test dataset well. A model that can fit the test dataset well, may not be accurate enough to give good predictions. This is known as the bias-variance tradeoff.\n8. Communicate the modelling results\nResults should be shown as visualizations/data tables to communicate the findings.\nLoad required packages\nLoad required packages:\n\n\nlibrary(pacman)\np_load(tidyverse, janitor, GGally, skimr, ggthemes, ggsci,\n       broom, gvlma, ggfortify, jtools, car, huxtable, sandwich,\n       tidymodels, parsnip, vip)\n\n\n\nImport\n\n\nwhite_rawdata <- read.table(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\", \n                            strip.white = T, sep = \";\", header = T) %>% \n  as_tibble() \n\n# save as another variable \ndata_white <- white_rawdata\n\n\n\nExploratory data analysis\n\n\nglimpse(data_white) # 12 variables, all numerical\n\n\nRows: 4,898\nColumns: 12\n$ fixed.acidity        <dbl> 7.0, 6.3, 8.1, 7.2, 7.2, 8.1, 6.2, 7.0,…\n$ volatile.acidity     <dbl> 0.27, 0.30, 0.28, 0.23, 0.23, 0.28, 0.3…\n$ citric.acid          <dbl> 0.36, 0.34, 0.40, 0.32, 0.32, 0.40, 0.1…\n$ residual.sugar       <dbl> 20.70, 1.60, 6.90, 8.50, 8.50, 6.90, 7.…\n$ chlorides            <dbl> 0.045, 0.049, 0.050, 0.058, 0.058, 0.05…\n$ free.sulfur.dioxide  <dbl> 45, 14, 30, 47, 47, 30, 30, 45, 14, 28,…\n$ total.sulfur.dioxide <dbl> 170, 132, 97, 186, 186, 97, 136, 170, 1…\n$ density              <dbl> 1.0010, 0.9940, 0.9951, 0.9956, 0.9956,…\n$ pH                   <dbl> 3.00, 3.30, 3.26, 3.19, 3.19, 3.26, 3.1…\n$ sulphates            <dbl> 0.45, 0.49, 0.44, 0.40, 0.40, 0.44, 0.4…\n$ alcohol              <dbl> 8.8, 9.5, 10.1, 9.9, 9.9, 10.1, 9.6, 8.…\n$ quality              <int> 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, …\n\nsummary(data_white) # scale and range of values are quite different\n\n\n fixed.acidity    volatile.acidity  citric.acid     residual.sugar  \n Min.   : 3.800   Min.   :0.0800   Min.   :0.0000   Min.   : 0.600  \n 1st Qu.: 6.300   1st Qu.:0.2100   1st Qu.:0.2700   1st Qu.: 1.700  \n Median : 6.800   Median :0.2600   Median :0.3200   Median : 5.200  \n Mean   : 6.855   Mean   :0.2782   Mean   :0.3342   Mean   : 6.391  \n 3rd Qu.: 7.300   3rd Qu.:0.3200   3rd Qu.:0.3900   3rd Qu.: 9.900  \n Max.   :14.200   Max.   :1.1000   Max.   :1.6600   Max.   :65.800  \n   chlorides       free.sulfur.dioxide total.sulfur.dioxide\n Min.   :0.00900   Min.   :  2.00      Min.   :  9.0       \n 1st Qu.:0.03600   1st Qu.: 23.00      1st Qu.:108.0       \n Median :0.04300   Median : 34.00      Median :134.0       \n Mean   :0.04577   Mean   : 35.31      Mean   :138.4       \n 3rd Qu.:0.05000   3rd Qu.: 46.00      3rd Qu.:167.0       \n Max.   :0.34600   Max.   :289.00      Max.   :440.0       \n    density             pH          sulphates         alcohol     \n Min.   :0.9871   Min.   :2.720   Min.   :0.2200   Min.   : 8.00  \n 1st Qu.:0.9917   1st Qu.:3.090   1st Qu.:0.4100   1st Qu.: 9.50  \n Median :0.9937   Median :3.180   Median :0.4700   Median :10.40  \n Mean   :0.9940   Mean   :3.188   Mean   :0.4898   Mean   :10.51  \n 3rd Qu.:0.9961   3rd Qu.:3.280   3rd Qu.:0.5500   3rd Qu.:11.40  \n Max.   :1.0390   Max.   :3.820   Max.   :1.0800   Max.   :14.20  \n    quality     \n Min.   :3.000  \n 1st Qu.:5.000  \n Median :6.000  \n Mean   :5.878  \n 3rd Qu.:6.000  \n Max.   :9.000  \n\nskim(data_white) # no missing values, probably need to normalize data\n\n\nTable 1: Data summary\nName\ndata_white\nNumber of rows\n4898\nNumber of columns\n12\n_______________________\n\nColumn type frequency:\n\nnumeric\n12\n________________________\n\nGroup variables\nNone\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nfixed.acidity\n0\n1\n6.85\n0.84\n3.80\n6.30\n6.80\n7.30\n14.20\n▁▇▁▁▁\nvolatile.acidity\n0\n1\n0.28\n0.10\n0.08\n0.21\n0.26\n0.32\n1.10\n▇▅▁▁▁\ncitric.acid\n0\n1\n0.33\n0.12\n0.00\n0.27\n0.32\n0.39\n1.66\n▇▆▁▁▁\nresidual.sugar\n0\n1\n6.39\n5.07\n0.60\n1.70\n5.20\n9.90\n65.80\n▇▁▁▁▁\nchlorides\n0\n1\n0.05\n0.02\n0.01\n0.04\n0.04\n0.05\n0.35\n▇▁▁▁▁\nfree.sulfur.dioxide\n0\n1\n35.31\n17.01\n2.00\n23.00\n34.00\n46.00\n289.00\n▇▁▁▁▁\ntotal.sulfur.dioxide\n0\n1\n138.36\n42.50\n9.00\n108.00\n134.00\n167.00\n440.00\n▂▇▂▁▁\ndensity\n0\n1\n0.99\n0.00\n0.99\n0.99\n0.99\n1.00\n1.04\n▇▂▁▁▁\npH\n0\n1\n3.19\n0.15\n2.72\n3.09\n3.18\n3.28\n3.82\n▁▇▇▂▁\nsulphates\n0\n1\n0.49\n0.11\n0.22\n0.41\n0.47\n0.55\n1.08\n▃▇▂▁▁\nalcohol\n0\n1\n10.51\n1.23\n8.00\n9.50\n10.40\n11.40\n14.20\n▃▇▆▃▁\nquality\n0\n1\n5.88\n0.89\n3.00\n5.00\n6.00\n6.00\n9.00\n▁▅▇▃▁\n\ndata_white %>% \n  ggpairs() # distribution of X is quite skewed, except maybe for pH.\n\n\n\n            # outcome is not unimodal --> ? \n\ndata_white %>% \n  ggcorr(label = T, label_alpha = T, label_round = 2) # some collinearity issues?\n\n\n\n\nSeems like OLS linear model isn’t really the best option for this case, since Y is multi-modal. It may be more suitable for classification to predict Y instead.\nNevertheless, let me compare the performance of OLS linear model with random forest, just for me to familiarise myself with the workflow for regression.\nTidymodels\nSplitting the dataset into training and testing datasets\n\n\nset.seed(202102212)\nwhitewine_split <- initial_split(data_white, prop = 0.8)\n\nwhitewine_train <- training(whitewine_split)\nwhitewine_test <- testing(whitewine_split)\n\n# split training dataset for cross-validation\nset.seed(20210228)\nwhite_cv <- vfold_cv(whitewine_train) # split training dataset for tuning mtry later\n\n\n\nSplitting the dataset into a training and testing dataset helps to minimise over-fitting of the model. Over-fitting the model would mean that the model fits the existing data very well, but is unable to predict for new data accurately.\nPreprocessing\nThe aim of preprocessing would be to solve the multi-collinearity issue, transform the data so that the distribution is not skewed, as well as to normalize the data.\n\n\nwhitewine_reciped <- whitewine_train %>% \n  recipe(quality ~., .) %>% \n  step_log(all_predictors(), -pH, offset = 1) %>% # do not use all numeric since will affect Y (outcome)\n  step_corr(all_predictors(), threshold = 0.5) %>%  # remove variables with r-sq > 0.5\n  step_normalize(all_predictors()) # means centering and scaling\n\nwhitewine_reciped \n\n\nData Recipe\n\nInputs:\n\n      role #variables\n   outcome          1\n predictor         11\n\nOperations:\n\nLog transformation on all_predictors(), -pH\nCorrelation filter on all_predictors()\nCentering and scaling for all_predictors()\n\nTrain the data recipe\n\n\nwhitewine_preprocessed <- prep(whitewine_reciped, verbose = T)\n\n\noper 1 step log [training] \noper 2 step corr [training] \noper 3 step normalize [training] \nThe retained training set is ~ 0.29 Mb  in memory.\n\nww_transformed <- whitewine_preprocessed %>% bake(new_data = NULL) # see preprocessed data\nww_transformed %>%  as_tibble() %>% round(., digits = 3) %>%  head()\n\n\n┌────────────────────────────────────────────────────────────────── │ fixed.ac volatile citric.a residual chloride free.sul\n│ idity .acidity cid .sugar s fur.diox\n│ ide\n├────────────────────────────────────────────────────────────────── │ 0.226 -0.042 0.264 1.83  -0.023 0.682\n│ -0.643 0.27  0.093 -1.1   0.171 -1.47 \n│ 0.46  -0.471 -0.08  0.691 0.604 0.763\n│ 0.46  -0.471 -0.08  0.691 0.604 0.763\n│ 1.45  0.063 0.598 0.436 0.219 -0.075\n│ 0.226 -0.042 0.264 1.83  -0.023 0.682\n└──────────────────────────────────────────────────────────────────\nColumn names: fixed.acidity, volatile.acidity, citric.acid, residual.sugar, chlorides, free.sulfur.dioxide, pH, sulphates, alcohol, quality\n6/10 columns shown.\n\n# check for missing values \nww_transformed %>% \n  map(is.na) %>%  \n  map_df(sum) %>% \n  tidy() %>% \n  select(column, mean) %>%  # no missing values\n  as_tibble()\n\n\n               ┌──────────────────────────────┐\n               │ column                  mean │\n               ├──────────────────────────────┤\n               │ fixed.acidity              0 │\n               │ volatile.acidity           0 │\n               │ citric.acid                0 │\n               │ residual.sugar             0 │\n               │ chlorides                  0 │\n               │ free.sulfur.dioxide        0 │\n               │ pH                         0 │\n               │ sulphates                  0 │\n               │ alcohol                    0 │\n               │ quality                    0 │\n               └──────────────────────────────┘\nColumn names: column, mean\n\nSpecify the models\nLinear regression (OLS)\n\n\nww_lr_model <- linear_reg() %>% \n  set_engine(\"lm\") %>% # there are other options available, eg glmnet\n  set_mode(\"regression\") # could also be classification for certain models, so just specify as best practice to be clear\n\nww_lr_workflow <- workflow() %>% \n  add_recipe(whitewine_reciped) %>% \n  add_model(ww_lr_model)\n\n#  fit model to training data, and get predicted values\nfinal_ww_lm_model_fit <- fit(ww_lr_workflow, whitewine_train)\n\n# understanding the lm model\n\nlm_fit_output <- final_ww_lm_model_fit %>% \n  pull_workflow_fit() %>% \n  tidy() %>% \n  as_tibble()\n\nlm_fit_output\n\n\n┌────────────────────────────────────────────────────────────┐\n│ term          estimate   std.error   statistic     p.value │\n├────────────────────────────────────────────────────────────┤\n│ (Intercept    5.88          0.012     488        0         │\n│ )                                                          │\n│ fixed.acid   -0.0459        0.0139     -3.3      0.000983  │\n│ ity                                                        │\n│ volatile.a   -0.199         0.0126    -15.8      2.61e-54  │\n│ cidity                                                     │\n│ citric.aci   -0.000888      0.0129     -0.0686   0.945     │\n│ d                                                          │\n│ residual.s    0.121         0.0142      8.53     1.98e-17  │\n│ ugar                                                       │\n│ chlorides    -0.0194        0.0133     -1.45     0.147     │\n│ free.sulfu    0.125         0.0131      9.61     1.31e-21  │\n│ r.dioxide                                                  │\n│ pH            0.0165        0.0139      1.19     0.234     │\n│ sulphates     0.0465        0.0123      3.78     0.000161  │\n│ alcohol       0.458         0.0147     31.1      4.63e-190 │\n└────────────────────────────────────────────────────────────┘\nColumn names: term, estimate, std.error, statistic, p.value\n\nlm_fit <- final_ww_lm_model_fit %>% \n  pull_workflow_fit()\n\nlm_fit\n\n\nparsnip model object\n\nFit time:  8ms \n\nCall:\nstats::lm(formula = ..y ~ ., data = data)\n\nCoefficients:\n        (Intercept)        fixed.acidity     volatile.acidity  \n          5.8775198           -0.0459302           -0.1989870  \n        citric.acid       residual.sugar            chlorides  \n         -0.0008882            0.1212696           -0.0193643  \nfree.sulfur.dioxide                   pH            sulphates  \n          0.1254698            0.0165026            0.0465193  \n            alcohol  \n          0.4576764  \n\n# Looking at the fitted values:\nlm_fitted_values <- lm_fit$fit$fitted.values\n\n# another way, from workflow\nlm_wf_fitted_values <- \n  broom::augment(lm_fit$fit, data = whitewine_train) %>% \n  select(quality, .fitted: .std.resid)\n\nglimpse(lm_wf_fitted_values)\n\n\nRows: 3,919\nColumns: 6\n$ quality    <int> 6, 6, 6, 6, 6, 6, 6, 6, 5, 5, 5, 7, 5, 6, 8, 5, 8…\n$ .fitted    <dbl> 5.469029, 5.165576, 5.863142, 5.863142, 5.686045,…\n$ .hat       <dbl> 0.0016112016, 0.0019222325, 0.0009142831, 0.00091…\n$ .sigma     <dbl> 0.7532844, 0.7532139, 0.7533292, 0.7533292, 0.753…\n$ .cooksd    <dbl> 8.032116e-05, 2.368036e-04, 3.023818e-06, 3.02381…\n$ .std.resid <dbl> 0.705488442, 1.108851543, 0.181776968, 0.18177696…\n\n# looking at variable importance\nvip_lm <- final_ww_lm_model_fit %>% \n  pull_workflow_fit() %>% # extracts the model information\n  vip(num_features = 10, \n      aesthetics = list(fill = \"deepskyblue4\")) + # most important factor is alcohol +\n  labs(title = \"Variable Importance: Linear Regression\") +\n  theme_few() +\n  theme(axis.text = element_text(face = \"bold\", size = 14))\n\nvip_lm\n\n\n\n\nRandom forest model\n\n\nrf_model <- rand_forest() %>% \n  set_args(mtry = tune()) %>% \n  set_mode(mode = \"regression\") %>% \n  set_engine(engine = \"ranger\", importance = \"impurity\")\n\nrf_model\n\n\nRandom Forest Model Specification (regression)\n\nMain Arguments:\n  mtry = tune()\n\nEngine-Specific Arguments:\n  importance = impurity\n\nComputational engine: ranger \n\nrf_workflow <- workflow() %>% \n  add_recipe(whitewine_reciped) %>% \n  add_model(rf_model)\n\nrf_grid <- expand.grid(mtry = 3:7) # choose sqrt(no. of variables) usually\n\ntuned_rf_results <- rf_workflow %>% \n  tune_grid(resamples = white_cv, # using cv dataset from training dataset\n            grid = rf_grid,\n            metrics = metric_set(rmse, rsq, mae))\n\nmodel_results <- tuned_rf_results %>% \n  collect_metrics()\n\nfinalized_rf_param <- tuned_rf_results %>% \n  select_best(metric = \"rmse\") %>% \n  as_tibble()\n\nfinalized_rf_param #M TRY = 3\n\n\n              ┌───────────────────────────────┐\n              │   mtry   .config              │\n              ├───────────────────────────────┤\n              │      3   Preprocessor1_Model1 │\n              └───────────────────────────────┘\nColumn names: mtry, .config\n\nrf_model_b <- rand_forest() %>% \n  set_args(mtry = 3) %>% \n  set_engine(engine = \"ranger\", importance = \"impurity\") %>% \n  set_mode(mode = \"regression\")\n\nrf_workflow_b <- workflow() %>% \n  add_recipe(whitewine_reciped) %>% \n  add_model(rf_model_b)\n\nfinal_ww_rf_model_fit <- fit(rf_workflow_b, whitewine_train)\n\n# understanding the rf model\n\n# for random forest, need to set importance = impurity in set_engine() to extract this\nvip_rf <- final_ww_rf_model_fit %>% \n  pull_workflow_fit() %>% # extracts the model information\n  vip(num_features = 10, \n      aesthetics = list(fill = \"darkorange\"))+ # most important factor is alcohol\n  labs(title = \"Variable Importance: Random Forest\") +\n  theme_few() +\n  theme(axis.text = element_text(face = \"bold\", size = 14))\n\nvip_rf\n\n\n\n\nComparing Linear Regression (OLS) vs Random Forest (RF)\n\n\ngridExtra::grid.arrange(vip_lm, vip_rf, nrow = 2)\n\n\n\n\nAlcohol content was the most important variable for both OLS and random forest models.\nAssessing on test data\n\n\nresults_train <- final_ww_lm_model_fit %>% \n  predict(new_data = whitewine_train) %>%  # use actual train data, not preprocessed data\n  mutate(truth = whitewine_train$quality,\n         model = \"lm\") %>% \n  bind_rows(final_ww_rf_model_fit %>% \n              predict(new_data = whitewine_train) %>% \n              mutate(truth = whitewine_train$quality,\n                     model = \"rf\")) \n  \nresults_train %>% \n  group_by(model) %>% \n  metrics(truth = truth, estimate = .pred) %>% \n  as_tibble()\n\n\n         ┌──────────────────────────────────────────┐\n         │ model   .metric   .estimator   .estimate │\n         ├──────────────────────────────────────────┤\n         │ lm      rmse      standard         0.752 │\n         │ rf      rmse      standard         0.277 │\n         │ lm      rsq       standard         0.282 │\n         │ rf      rsq       standard         0.934 │\n         │ lm      mae       standard         0.584 │\n         │ rf      mae       standard         0.198 │\n         └──────────────────────────────────────────┘\nColumn names: model, .metric, .estimator, .estimate\n\nresults_test <- final_ww_lm_model_fit %>% \n  predict(new_data = whitewine_test) %>% \n  mutate(truth = whitewine_test$quality,\n         model = \"lm\") %>% \n  bind_rows(final_ww_rf_model_fit %>% \n              predict(new_data = whitewine_test) %>% \n              mutate(truth = whitewine_test$quality,\n                     model = \"rf\")) \n\nresults_test %>% \n  group_by(model) %>% \n  metrics(truth = truth, estimate = .pred) %>% \n  as_tibble()\n\n\n         ┌──────────────────────────────────────────┐\n         │ model   .metric   .estimator   .estimate │\n         ├──────────────────────────────────────────┤\n         │ lm      rmse      standard         0.737 │\n         │ rf      rmse      standard         0.589 │\n         │ lm      rsq       standard         0.295 │\n         │ rf      rsq       standard         0.56  │\n         │ lm      mae       standard         0.581 │\n         │ rf      mae       standard         0.435 │\n         └──────────────────────────────────────────┘\nColumn names: model, .metric, .estimator, .estimate\n\nWhen comparing rmse, rf has lower rmse in training dataset but the rmse value increased in the test dataset –> overfitting and cannot predict as well.This was the same for other indicators rsq and mean absolute error.\nBear in mind that in the first place, the outcome variable Y was multi-modal. This may be the reason why OLS wasn’t a suitable learner.\nVisualizing the assessment\n\n\nresults_train %>% \n  ggplot(aes(x =  truth, y = .pred)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  geom_abline(col = \"darkorange\") +\n  labs(x = \"actual values (truth)\",\n       y = \"predicted values\",\n       title = \"Training dataset\") +\n  scale_x_continuous(breaks = c(1:10)) +\n  facet_wrap( model ~ ., ncol = 2) +\n  theme_few()\n\n\n\nresults_test %>% \n  ggplot(aes(x =  truth, y = .pred)) + \n  geom_point() + \n  geom_smooth(method = \"lm\") +\n  geom_abline(col = \"darkorange\") +\n  labs(x = \"actual values (truth)\",\n       y = \"predicted values\",\n       title = \"Testing dataset\") +\n  scale_x_continuous(breaks = c(1:10)) +\n  facet_wrap( model ~ ., ncol = 2) +\n  theme_few()\n\n\n\n\nLearning points\nIt took me a while to piece different pieces of the jigsaw together to see the whole picture for machine learning. Initially, I will be carrying out EDA blindly, simply using skim() because it is a convenient function, but not fully understanding what I should be looking out for. I would be doing pre-processing steps at random, depending on what I saw from other websites. Finally, I saw the light that the purpose of doing EDA was to understand what I should be doing for preprocessing!\nIt is always good to start with the simple OLS when I am learning regression. There are assumptions that must be met before doing OLS – these could be checked using the gvlma package, and you can carry out the necessary transformations before doing OLS. There are other types of linear regression, for example generalized linear model (GLM), which I should try as well.\nThe order of carrying out preprocessing steps matter!\nThe choice of all_numerical, all_predictors in the recipe step matters! In this case, all_numerical includes the Y variable. Although Y is multimodal, it is not skewed, so I should not log transform it (which is what would happen if I were to use step_log(all_numerical())). If I log-transformed Y, I would run into errors further along the script, as there are some bugs regarding predict function if Y is transformed. The OLS model performed relatively consistently in both training and test dataset. However, the RF model performed better in the training dataset, but performance was poorer in the test dataset. This suggested that the RF model, in this case, had over-fitting issues.\nNext steps:\nTry out regression on a dataset in which Y is suitable for regression analysis\nTry out classification on wine dataset.\nTry out step_dummy, which creates numerical variables out of categorical variables\nTry out different algorithms and their tuning parameters\n“There is only one corner of the universe you can be certain of improving, and that’s your own self.” - ― Aldous Huxley\nThis is just the beginning of my learning journey!\nReferences\nhttps://www.tmwr.org/\nhttps://online.stat.psu.edu/stat508/lesson/1a\nhttps://semba-blog.netlify.app/05/11/2020/regression-with-tidymodels/\nhttps://stackoverflow.com/questions/63239600/how-to-make-predictions-in-tidymodels-r-when-the-data-has-been-preprocessed\nhttps://stackoverflow.com/questions/13956435/setting-values-for-ntree-and-mtry-for-random-forest-regression-model\nhttps://rviews.rstudio.com/2019/06/19/a-gentle-intro-to-tidymodels/\nhttps://jhudatascience.org/tidyversecourse/model.html\nhttps://koalaverse.github.io/vip/articles/vip.html\nhttp://rstudio-pubs-static.s3.amazonaws.com/565136_b4395e2500ec4c129ab776b9e8dd24de.html#results\n\n\n\n",
    "preview": "posts/20210301_tidy models regression 1/Tidy-Models---Regression-v2_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-03-03T22:04:39+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210219_color calculations (juice)/",
    "title": "Color Analysis in Juices",
    "description": "Using R for color calculations and data visualization",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-02-19",
    "categories": [],
    "contents": "\nIntroduction\nWhy are food scientists interested in color analysis? Color is a visual quality attribute that determines food acceptance (Wrolstad and Smith (2017)). Instrumental color analysis is carried out in most commercial research and development laboratories to assess color stability, and in turn, shelf life of food products. The Hunter L a b color space is commonly used in the food industry, and was first published in 1942. Improvements were made to this system, to give more uniform color spacing, and in 1976, the CIELAB L* a* b* system was introduced. Chroma and Hue could be calculated from the a* and b* values. What do all these terms mean?\nL* : lightness (0 being black and 100 being white)\na*: red (+) and green (-)\nb*: yellow (+) and blue (-)\nChroma: a measure of how vivid/dull the color is. Chroma increases with increasing pigment concentration, and then decreases as the sample becomes darker.\nHue (in radian): the type of color - where red is defined as 0/360deg, yellow is defined as 90 deg, green is defined as 180 deg and blue is defined as 270 deg.\nThe calculations for chroma and hue are give below:\nChroma = sqrt(aˆ2 + bˆ2)\nHue is expressed in radians (multiply by 180/pi). However, this equation is only for the first quadrant. Other quadrants need to be handled so that a 360deg representation is accomodated (Mclellan, Lind, and Kime (1995)).\nFirst quadrant [+a, +b] : Hue = arc tan (b/a)\nSecond quadrant [‐a, +b] and third quadrant [−a, −b] calculations should be: hue = 180+Arc tan(b/a).\nFourth quadrant [+a, −b] calculations should be: hue = 360+ Arc tan(b/a).\nThe L* C* H* color space is more useful than just looking at L* a* b*, since it takes into account human perception of color, rather than just looking at redness/greenness and yellowness/blueness individually.\nUltimately, when color is measured, other than having an objective set of numbers to describe colors, it is also of interest to assess if there is any color difference between a reference sample and a test sample, and to peg a number to this color difference and immediately tell if the color difference is visually obvious to people.\nThere are different equations for assessing color difference:\ndeltaE-1976: the first internationally endorced color difference equation. However, it does not take into account that the human eye is more sensitive to small color differences in some regions of the color wheel but less sensitive to others.\ndeltaE-1994: Improvements were made to the 1976 equation, but it lacked accuracy in the blue-violet region of the color space.\ndeltaE-2000: Corrected for the 1994 equation to improve accuracy in the blue-violet space. This is the most updated and accurate representation of total color difference so far, and the equation is given in: http://www2.ece.rochester.edu/~gsharma/ciede2000/ciede2000noteCRNA.pdf.\nPreviously, I worked with Excel spreadsheets to carry out color data calculations. It was a nightmare when I tried to calculate hue using Excel, as the equations used for +a/-a/+b/-b could be different and it was problematic when I was trying to fill an equation down for my shelf life study. DE2000 was complicated and I tried to use a spreadsheet that I downloaded off the Internet, but I had to copy my data over to the template spreadsheet and it was a lot of copying and pasting.\nAll I want, is a workflow that can house all my data in 1 place, and automatically apply calculations with minimal manual input.\nI am so glad that I found R, and that there is an inbuilt package spacesXYZ, that can calculate the different variants of total color difference.\nObjective\nTo develop a workflow for automatic color calculations. This include writing my own function for calculating chroma, hue, and using inbuilt functions within spacesXYZ package to calculate de2000.\nTo visualize data and derive insights from shelf life data clearly.\nData\nThe data I am using is from a paper by Porto et al: https://www.mdpi.com/2306-5710/3/3/36. In this paper, the color data for five types of juices were given, and color changes were assessed in terms of L* a* b*, chroma and de1976. I went on further to look at hue, and de2000.\nWorkflow\nLoad data\nTransform data (in wide format) - calculate chroma and hue\nAdd in new columns with initial L* a* b* to facilitate calculation for de2000\nExtract initial L* a* b* values as matrix form\nExtract measured L* a* b* values as matrix form\nCalculate de2000 using spacesXYZ, as the function requires input to be in matrix form\nCalculate change in L* a* b* chroma and hue\nTransform data into long format for data visualization\nPlot de2000, L* a* b* chroma and hue, as well as change in L* a* b* chroma and hue to derive insights.\nLoad packages\n\n\nlibrary(pacman)\np_load(tidyverse, spacesXYZ, ggthemes, gridExtra, ggsci)\n\n\n\nImport Data\nThe five samples tested were:\nBJ: raw beet juice\nPBJ: pasteurized beet juice\nPOJ: pasteurized orange juice\nBOMJ_1: pasteurized beet and orange mix juice (1:1 v/v)\nBOMJ-2: pasteurized beet and orange mix juice (1:2, v/v)\n\n\ndata_l <- tribble(\n  ~Juices, ~L_d0, ~L_d5, ~L_d10, ~L_d15, ~L_d30,\n  #-------/------/-----/--------/------/-------\n  \"BJ\",     22.45, 22.87, 22.31, 22.37, 24.16 , \n  \"PBJ\",    22.37, 22.71, 22.34, 22.23, 23.72,\n  \"POJ\",    33.61, 38.18, 36.73, 37.04, 42.42,\n  \"BOMJ_1\", 23.21, 23.76, 23.15, 23.14, 24.78,\n  \"BOMJ_2\", 23.77, 24.33, 23.81, 24.15, 26.18\n)\n\n\ndata_a <- tribble(\n  ~Juices, ~a_d0, ~a_d5, ~a_d10, ~a_d15, ~a_d30,\n  #-------/------/-----/--------/------/-------\n  \"BJ\",     0.78, 0.68, 0.74, 0.81, 1.07,\n  \"PBJ\",    0.95, 0.82, 0.87, 0.91, 1.19,\n  \"POJ\",    -2.49, -2.87, -2.51, -2.63, -3.64,\n  \"BOMJ_1\", 4.80, 5.18, 4.78, 4.59, 6.13,\n  \"BOMJ_2\", 6.65, 7.07, 6.65, 6.68, 8.76\n)\n\n\ndata_b <- tribble(\n  ~Juices, ~b_d0, ~b_d5, ~b_d10, ~b_d15, ~b_d30,\n  #-------/------/-----/--------/------/-------\n  \"BJ\",    1.56, 1.52, 1.56, 1.57, 0.97, \n  \"PBJ\",   1.67, 1.61, 1.63, 1.64, 1.21,  \n  \"POJ\",  16.34, 17.03, 16.46, 15.95, 18.34,\n  \"BOMJ_1\", 2.39, 2.38, 2.35, 2.19, 2.23,\n  \"BOMJ_2\", 2.75, 2.82, 2.68, 2.26, 2.47\n)\n\n# Transform #####\ndata <- bind_cols(data_l, data_a, data_b, .name_repair = \"unique\") %>% \n  select(-Juices...7, -Juices...13) %>% \n  rename(juices = Juices...1)\n\n\n\nTransform\n\n\ndata_reshape_L <- data %>% \n  pivot_longer(cols = starts_with(\"L\"),\n               names_to = \"days_L\",\n               values_to = \"L_av\") %>% \n  select(juices, days_L, L_av)\n\n\ndata_reshape_a <- data %>% \n  pivot_longer(cols = starts_with(\"a\"),\n               names_to = \"days_a\",\n               values_to = \"a_av\") %>% \n  select(juices, days_a, a_av)\n  \ndata_reshape_b <- data %>%   \n  pivot_longer(cols = starts_with(\"b\"),\n               names_to = \"days_b\",\n               values_to = \"b_av\") %>% \n  select(juices, days_b, b_av)\n\n\ndata_reshaped <- bind_cols(data_reshape_L, data_reshape_a, data_reshape_b) %>% \n  mutate(days = parse_number(days_L)) %>% \n  select(juices...1, days, L_av, a_av, b_av) %>% \n  rename(juices = juices...1)\n\ndata_reshaped\n\n\n# A tibble: 25 x 5\n   juices  days  L_av  a_av  b_av\n   <chr>  <dbl> <dbl> <dbl> <dbl>\n 1 BJ         0  22.4  0.78  1.56\n 2 BJ         5  22.9  0.68  1.52\n 3 BJ        10  22.3  0.74  1.56\n 4 BJ        15  22.4  0.81  1.57\n 5 BJ        30  24.2  1.07  0.97\n 6 PBJ        0  22.4  0.95  1.67\n 7 PBJ        5  22.7  0.82  1.61\n 8 PBJ       10  22.3  0.87  1.63\n 9 PBJ       15  22.2  0.91  1.64\n10 PBJ       30  23.7  1.19  1.21\n# … with 15 more rows\n\nColor calculations\nWriting functions to calculate chroma and hue\n\n\ncal_chroma <- function (a_av, b_av) {\n  \n  a_sq = a_av^2\n  b_sq = b_av^2\n  chroma = sqrt(a_sq + b_sq)\n  \n}\n\ncal_hue <- function (a_av, b_av) {\n  \n  if(a_av > 0 & b_av > 0) {  # a pos, b pos\n    hue = 180*(atan(b_av/a_av)/pi)\n    \n    \n  }   else if (a_av<0 & b_av > 0) {  # a neg, b pos\n    hue = 180 + 180*(atan(b_av/a_av)/pi)\n    \n    \n  } else if (a_av<0 & b_av<0) {   # a neg, b neg\n    hue = 180 + 180*(atan(b_av/a_av)/pi)\n    \n    \n  } else {    # a pos, b neg\n    hue = 360 + 180*(atan(b_av/a_av)/pi)\n    \n  }\n  \n}\n\n\n\nAdding calculated chroma and hue columns to tibble\n\n\ndata_transformed <- data_reshaped %>% \n  mutate(chroma = map2_dbl(.x = a_av,\n                           .y = b_av,\n                           .f = cal_chroma),\n         hue = map2_dbl(.x = a_av,\n                        .y = b_av,\n                        .f = cal_hue))\n\nglimpse(data_transformed) # compares well with table\n\n\nRows: 25\nColumns: 7\n$ juices <chr> \"BJ\", \"BJ\", \"BJ\", \"BJ\", \"BJ\", \"PBJ\", \"PBJ\", \"PBJ\", \"…\n$ days   <dbl> 0, 5, 10, 15, 30, 0, 5, 10, 15, 30, 0, 5, 10, 15, 30…\n$ L_av   <dbl> 22.45, 22.87, 22.31, 22.37, 24.16, 22.37, 22.71, 22.…\n$ a_av   <dbl> 0.78, 0.68, 0.74, 0.81, 1.07, 0.95, 0.82, 0.87, 0.91…\n$ b_av   <dbl> 1.56, 1.52, 1.56, 1.57, 0.97, 1.67, 1.61, 1.63, 1.64…\n$ chroma <dbl> 1.744133, 1.665173, 1.726615, 1.766635, 1.444230, 1.…\n$ hue    <dbl> 63.43495, 65.89777, 64.62226, 62.70972, 42.19363, 60…\n\nCreating initial values tibble dataframe to calculate dE2000 later\n\n\ninitial <-  data_transformed %>% \n                      filter(days == 0) %>% \n                      select(L_av, a_av, b_av, chroma, hue) %>% \n                      rename(ini_L = L_av,\n                             ini_a = a_av,\n                             ini_b = b_av,\n                             ini_chroma = chroma,\n                             ini_hue = hue)\n\ninitial\n\n\n# A tibble: 5 x 5\n  ini_L ini_a ini_b ini_chroma ini_hue\n  <dbl> <dbl> <dbl>      <dbl>   <dbl>\n1  22.4  0.78  1.56       1.74    63.4\n2  22.4  0.95  1.67       1.92    60.4\n3  33.6 -2.49 16.3       16.5     98.7\n4  23.2  4.8   2.39       5.36    26.5\n5  23.8  6.65  2.75       7.20    22.5\n\nAdding the initial L* a* b* values to tibble\n\n\ndata_transformed_b<- data_transformed %>% \n  group_by(juices) %>% \n  nest() %>% \n  bind_cols(initial) %>% \n  unnest(cols = c(data))\n\ndata_transformed_b\n\n\n# A tibble: 25 x 12\n# Groups:   juices [5]\n   juices  days  L_av  a_av  b_av chroma   hue ini_L ini_a ini_b\n   <chr>  <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl> <dbl> <dbl> <dbl>\n 1 BJ         0  22.4  0.78  1.56   1.74  63.4  22.4  0.78  1.56\n 2 BJ         5  22.9  0.68  1.52   1.67  65.9  22.4  0.78  1.56\n 3 BJ        10  22.3  0.74  1.56   1.73  64.6  22.4  0.78  1.56\n 4 BJ        15  22.4  0.81  1.57   1.77  62.7  22.4  0.78  1.56\n 5 BJ        30  24.2  1.07  0.97   1.44  42.2  22.4  0.78  1.56\n 6 PBJ        0  22.4  0.95  1.67   1.92  60.4  22.4  0.95  1.67\n 7 PBJ        5  22.7  0.82  1.61   1.81  63.0  22.4  0.95  1.67\n 8 PBJ       10  22.3  0.87  1.63   1.85  61.9  22.4  0.95  1.67\n 9 PBJ       15  22.2  0.91  1.64   1.88  61.0  22.4  0.95  1.67\n10 PBJ       30  23.7  1.19  1.21   1.70  45.5  22.4  0.95  1.67\n# … with 15 more rows, and 2 more variables: ini_chroma <dbl>,\n#   ini_hue <dbl>\n\nCalculating de2000\n\n\n# calculate de2000 using spacesXYZ package, input must be as matrix\n\nlab_meas <- as.matrix(data_transformed_b[, c(\"L_av\", \"a_av\", \"b_av\")])\nlab_ini <- as.matrix(data_transformed_b[, c(\"ini_L\", \"ini_a\", \"ini_b\")])\n\ndata_de <- spacesXYZ::DeltaE(lab_ini, lab_meas, metric = 2000)\n  \n\ndata_transformed_c <- data_transformed_b %>% \n  bind_cols(data_de) %>% \n  rename(de2000 = ...13) %>% \n  ungroup() # remove group by juices\n\n# round off to 2 digits\ndata_transformed_c$de2000 <- round(data_transformed_c$de2000, digits = 2)\n\nglimpse(data_transformed_c)\n\n\nRows: 25\nColumns: 13\n$ juices     <chr> \"BJ\", \"BJ\", \"BJ\", \"BJ\", \"BJ\", \"PBJ\", \"PBJ\", \"PBJ…\n$ days       <dbl> 0, 5, 10, 15, 30, 0, 5, 10, 15, 30, 0, 5, 10, 15…\n$ L_av       <dbl> 22.45, 22.87, 22.31, 22.37, 24.16, 22.37, 22.71,…\n$ a_av       <dbl> 0.78, 0.68, 0.74, 0.81, 1.07, 0.95, 0.82, 0.87, …\n$ b_av       <dbl> 1.56, 1.52, 1.56, 1.57, 0.97, 1.67, 1.61, 1.63, …\n$ chroma     <dbl> 1.744133, 1.665173, 1.726615, 1.766635, 1.444230…\n$ hue        <dbl> 63.43495, 65.89777, 64.62226, 62.70972, 42.19363…\n$ ini_L      <dbl> 22.45, 22.45, 22.45, 22.45, 22.45, 22.37, 22.37,…\n$ ini_a      <dbl> 0.78, 0.78, 0.78, 0.78, 0.78, 0.95, 0.95, 0.95, …\n$ ini_b      <dbl> 1.56, 1.56, 1.56, 1.56, 1.56, 1.67, 1.67, 1.67, …\n$ ini_chroma <dbl> 1.744133, 1.744133, 1.744133, 1.744133, 1.744133…\n$ ini_hue    <dbl> 63.43495, 63.43495, 63.43495, 63.43495, 63.43495…\n$ de2000     <dbl> 0.00, 0.33, 0.11, 0.07, 1.42, 0.00, 0.31, 0.12, …\n\nThe perceptible difference is defined theoretically as de2000 being greater than 2 http://zschuessler.github.io/DeltaE/learn/. Which samples have de2000 >2?\n\n\n# threshold is de2000>2\n\nabove_threshold <- data_transformed_c %>% \n  filter(de2000>2) %>% \n  select(juices, days, de2000)\n\nabove_threshold  # POJ\n\n\n# A tibble: 5 x 3\n  juices  days de2000\n  <chr>  <dbl>  <dbl>\n1 POJ        5   3.84\n2 POJ       10   2.57\n3 POJ       15   2.85\n4 POJ       30   7.7 \n5 BOMJ_2    30   2.76\n\nColor difference was already perceptible for pasteurized orange juice from day 5. For beet and orange mixed juice (1:2 v/v), the color difference was perceptibely at day 30, at the end of shelf life.\nVisualization\n\n\n# Reshape data to make it suitable for facetting \n\ndata_viz_long <- data_transformed_c %>% \n  mutate(delta_L = L_av - ini_L,\n         delta_a = a_av - ini_a,\n         delta_b = b_av - ini_b,\n         delta_chroma = chroma - ini_chroma,\n         delta_hue = hue - ini_hue) %>% \n  select(juices, days, L_av:delta_hue) %>% \n  pivot_longer(cols = c(L_av:delta_hue),\n               names_to = \"parameters\",\n               values_to = \"readings\")\n  \n\ndata_viz_long\n\n\n# A tibble: 400 x 4\n   juices  days parameters readings\n   <chr>  <dbl> <chr>         <dbl>\n 1 BJ         0 L_av          22.4 \n 2 BJ         0 a_av           0.78\n 3 BJ         0 b_av           1.56\n 4 BJ         0 chroma         1.74\n 5 BJ         0 hue           63.4 \n 6 BJ         0 ini_L         22.4 \n 7 BJ         0 ini_a          0.78\n 8 BJ         0 ini_b          1.56\n 9 BJ         0 ini_chroma     1.74\n10 BJ         0 ini_hue       63.4 \n# … with 390 more rows\n\nde2000\n\n\ndata_viz_long %>% \n  filter(parameters == \"de2000\") %>% \n  ggplot(aes(days, readings)) +\n  geom_point(aes(col = juices), size = 2) +\n  geom_line(aes(col = juices), size = 1) +\n  scale_color_lancet() +\n  geom_hline(yintercept = 2, col = \"grey77\", lty = 2) +\n  labs(title = \"Comparison of Total Color Difference (dE2000) when stored at 4degC for 30 days\",\n       x = \"Days\",\n       y = \"Calc. dE2000\",\n       subtitle = \"Pure Orange Juice (POJ) had the greatest change in color. Addition of beet juice decreases change in color difference.\",\n       caption = \"Source: Porto et al, 2017\") +\n  facet_wrap(~juices, ncol = 3) +\n  theme_few() +\n  theme(title = element_text(face = \"bold\", size = 16),\n        legend.position = \"none\",\n        strip.text = element_text(face = \"bold\", size = 14))\n\n\n\n\nWhilst we know that pasteurized orange juice had perceptible color difference, what exactly was the difference due to? To answer this question, we will have to look at individual parameters (L* a* b* chroma and hue).\nUnderstanding each color parameter\n\n\nviz_absolute <- data_viz_long %>% \n  filter(parameters %in% c(\"L_av\", \"a_av\", \"b_av\", \"chroma\", \"hue\")) %>%\n  mutate(parameters_fct = factor(parameters,\n                                 levels = c(\"L_av\", \"a_av\", \"b_av\", \"chroma\", \"hue\"))) %>% \n  ggplot(aes(days, readings, group = juices)) +\n  geom_point(aes(col = juices), size = 2) +\n  geom_line(aes(col = juices), size = 1) +\n  scale_color_lancet() +\n  labs(title = \"Change in color over shelf life period\",\n       caption = \"Source: Porto et al, 2017\",\n       col = \"Juices\") +\n  facet_wrap( ~ parameters_fct, ncol = 5, scales = \"free\") +\n  theme_few()+\n  theme(title = element_text(face = \"bold\", size = 20),\n        strip.text = element_text(face = \"bold\", size = 16),\n        axis.text = element_text(size = 14),\n        legend.text = element_text(size = 14),\n        legend.position = \"top\")\n\nviz_change <- data_viz_long %>% \n  filter(parameters %in% c(\"delta_L\", \"delta_a\", \"delta_b\", \"delta_chroma\", \"delta_hue\")) %>% \n  mutate(parameters_fct = factor(parameters, \n                                 levels = c(\"delta_L\", \"delta_a\", \"delta_b\", \"delta_chroma\", \"delta_hue\"))) %>% \n  ggplot(aes(days, readings, group = juices)) +\n  geom_point(aes(col = juices), size = 2) +\n  geom_line(aes(col = juices), size = 1) +\n  scale_color_lancet() +\n  labs(title = \"Change in color over shelf life period\",\n       caption = \"Source: Porto et al, 2017\",\n       col = \"Juices\") +\n  facet_wrap( ~ parameters_fct, ncol = 5) +\n  theme_few()+\n  theme(title = element_text(face = \"bold\", size = 20),\n        strip.text = element_text(face = \"bold\", size = 16),\n        axis.text = element_text(size = 14),\n        legend.text = element_text(size = 14),\n        legend.position = \"top\")\n\ngrid.arrange(viz_absolute, viz_change, nrow = 2)\n\n\n\n\nInterpretation\nBeet juice is red in color and orange juice is orange-yellow in color. If we look at the L* a* b* values, from the onset, POJ had higher values for L* (ie more dark), lower a* (ie less red) and higher b* (ie more yellow). This is more easily understood by looking at the hue values, which describes the type of color (0 = red, 90 = yellow). In terms of color vividness, POJ was relatively more vivid than the other samples, and BJ and PBJ has the “dullest” color.\nHowever, for total color difference, we would be more interested in the change in each parameter. POJ had a relatively large increase in L* (ie more darkening of color). For hue, there was a slight increase for POJ, but it was less in magnitude as compared to BJ and PBJ.\n\n\ndata_viz_long %>% \n  filter(juices %in% c(\"BJ\", \"PBJ\", \"POJ\"),\n         days == 30,\n         parameters == \"delta_hue\") \n\n\n# A tibble: 3 x 4\n  juices  days parameters readings\n  <chr>  <dbl> <chr>         <dbl>\n1 BJ        30 delta_hue    -21.2 \n2 PBJ       30 delta_hue    -14.9 \n3 POJ       30 delta_hue      2.56\n\nBJ and PBJ had a decrease in hue of 21 units and 15 units. This meant that the color became less orange-red and more red. However, the change in de2000 was probably attributed to the change in L* for POJ. Even though there was a difference in hue, the total color difference was below threshold of 2 for BJ and PBJ. Color instability was mostly attributed to pasteurized orange juice, and beet juice was relatively more stable.\nBetalains were responsible for the red color in beet, and carotenoids are responsible for the orange color in oranges (Tanaka, Sasaki, and Ohmiya (2008)). Fun fact: betalains color are not pH-dependent like anthocyanins, and they do not co-exist in plants.\nReflections\nI am happy that I managed to write a function for hue calculation, and use existing functions to calculate de2000. The calculations were really cumbersome when done in excel.\nWhen looking at color difference, it is important to look at both absolute readings and change in parameter readings to get the whole picture. The former allows you to understand what the starting point was, and the latter zooms in to the change between the start and at the end of shelf life. Although the data could be expressed in numerical form in tables, properly drawn graphs give more intuitive understanding of the data. I really like the faceting function in R, as it allows me to see all the types of juices and different color parameters clearly. In addition, the grid.arrange function allows me to display more than one graph.\nIn this shelf life study, only one temperature condition was studied. What if more than one temperature/product were looked at? In such cases, repetitive color calculations may be made more efficient by using purrr.\nLinks\nhttps://www.mdpi.com/2306-5710/3/3/36 https://www.xrite.com/blog/lab-color-space https://sensing.konicaminolta.us/us/blog/identifying-color-differences-using-l-a-b-or-l-c-h-coordinates/ https://www.konicaminolta.com/instruments/knowledge/color/pdf/color_communication.pdf https://www.hdm-stuttgart.de/international_circle/circular/issues/13_01/ICJ_06_2013_02_069.pdf http://zschuessler.github.io/DeltaE/learn/\n\n\n\nMclellan, M. R., L. R. Lind, and R. W. Kime. 1995. “HUE ANGLE DETERMINATIONS AND STATISTICAL ANALYSIS FOR MULTIQUADRANT HUNTER l,a,b DATA.” Journal of Food Quality 18 (3): 235–40. https://doi.org/https://doi.org/10.1111/j.1745-4557.1995.tb00377.x.\n\n\nTanaka, Yoshikazu, Nobuhiro Sasaki, and Akemi Ohmiya. 2008. “Biosynthesis of Plant Pigments: Anthocyanins, Betalains and Carotenoids.” The Plant Journal 54 (4): 733–49. https://doi.org/https://doi.org/10.1111/j.1365-313X.2008.03447.x.\n\n\nWrolstad, Ronald E., and Daniel E. Smith. 2017. “Color Analysis.” In Food Analysis, edited by S. Suzanne Nielsen, 545–55. Food Science Text Series. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-45776-5_31.\n\n\n\n\n",
    "preview": "posts/20210219_color calculations (juice)/Color-analysis-for-Juices_files/figure-html5/unnamed-chunk-11-1.png",
    "last_modified": "2021-02-21T15:51:34+08:00",
    "input_file": {},
    "preview_width": 3072,
    "preview_height": 2304
  },
  {
    "path": "posts/20210216_durian volatiles/",
    "title": "Comparison of volatiles in Durians",
    "description": "Data visualization for volatiles in different durian varieties",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-02-16",
    "categories": [],
    "contents": "\nIntroduction\nDurian is a tropical fruit that is either much loved or much hated in Singapore. There are different varieties of durians, and the top durians such as Mao Shan Wang can command prices of around $20-30 per kg. The price depends on the supply, the quality, and of course the demand.\nTeh et al. (2017) mentioned that the durian aroma comes mainly from the sulfur compounds, which gives it the characteristic pungent smell; as well as esters, which contributes to the fruity character.\nThe data below is from the work done by Chin et al. (2007). A total of 39 volatiles were identified in three varieties of durian: D2, D24 and D101. In the paper, PCA was carried out to distinguish between the three varieties.\nObjective\nData visualization for top 10 volatile compounds (by concentration) in three different durian varieties: D2, D24 and D101\nLoad packages\n\n\nlibrary(tidyverse)\nlibrary(janitor)\n\n\n\nImport\nThe file was saved on my working directory and I imported it into R\n\n\ndurian <- read_csv(\"Durian.csv\") %>% \n  clean_names() \n\n\n\nData visualization\n\n\nd101 <- durian %>% \n  select(-peak_no, -odor_description, - category) %>% \n  pivot_longer(cols = starts_with(\"d\"),\n               names_to = \"variety\",\n               values_to = \"concentration\") %>% \n  filter(variety == \"d101\") %>% \n  top_n(10, concentration) %>% \n  ggplot(aes(fct_reorder(compound, concentration), concentration)) +\n  geom_col(fill = \"goldenrod\") +\n  labs(x = NULL,\n       title = \"D101\",\n       x = \"Relative Concentration (ug/g)\",\n       caption = \"Chin et al, 2007\") +\n  coord_flip() +\n  theme_classic() +\n  theme(title = element_text(face = \"bold\", size = 16),\n        axis.text = element_text(size = 14))\n  \nd2 <- durian %>% \n  select(-peak_no, -odor_description, - category) %>% \n  pivot_longer(cols = starts_with(\"d\"),\n               names_to = \"variety\",\n               values_to = \"concentration\") %>% \n  filter(variety == \"d2\") %>% \n  top_n(10, concentration) %>% \n  ggplot(aes(fct_reorder(compound, concentration), concentration)) +\n  geom_col(fill = \"forestgreen\") +\n  labs(x = NULL,\n       title = \"D2\",\n       x = \"Relative Concentration (ug/g)\",\n       caption = \"Chin et al, 2007\") +\n  coord_flip() +\n  theme_classic() +\n  theme(title = element_text(face = \"bold\", size = 16),\n        axis.text = element_text(size = 14))\n\nd24 <- durian %>% \n  select(-peak_no, -odor_description, - category) %>% \n  pivot_longer(cols = starts_with(\"d\"),\n               names_to = \"variety\",\n               values_to = \"concentration\") %>% \n  filter(variety == \"d24\") %>% \n  top_n(10, concentration) %>% \n  ggplot(aes(fct_reorder(compound, concentration), concentration)) +\n  geom_col(fill = \"darkorange2\") +\n  labs(x = NULL,\n       title = \"D24\",\n       x = \"Relative Concentration (ug/g)\",\n       caption = \"Chin et al, 2007\") +\n  coord_flip() +\n  theme_classic() +\n  theme(title = element_text(face = \"bold\", size = 16),\n        axis.text = element_text(size = 14))\n\n\ngridExtra::grid.arrange(d101, d2, d24, ncol = 3,\n                        top = \"Comparison of top volatiles found in different durian varieties\")\n\n\n\ndurian %>% \n  select(-peak_no, -odor_description) %>% \n  pivot_longer(cols = starts_with(\"d\"),\n               names_to = \"variety\",\n               values_to = \"concentration\") %>% \n  group_by(category, variety) %>% \n  summarize(sum_conc = sum(concentration)) %>% \n  arrange(desc(sum_conc))\n\n\n# A tibble: 9 x 3\n# Groups:   category [3]\n  category         variety sum_conc\n  <chr>            <chr>      <dbl>\n1 Ester            d2         54.7 \n2 Ester            d101       54.6 \n3 Sulfur-compounds d24        47.4 \n4 Sulfur-compounds d2         46.5 \n5 Sulfur-compounds d101       36.5 \n6 Ester            d24        30.5 \n7 Alcohol          d2          1.09\n8 Alcohol          d101        0.72\n9 Alcohol          d24         0.56\n\nInterpretation\nFrom the plot above, half of the top ten volatile compounds in D24 were sulfur-containing compounds, and the most abundant volatile was diethyl disulfide (18.76 ug/g). The odor description for diethyl disulfide is “Sulfury, roasty, cabbage-like odor”.\nFor D101, the top two most abundant volatile compounds were esters: ethyl 2-methylbutanoate (21.89 ug/g) (poweful green, fruity, apple-like odor) and propyl 2-methylbutanoate (12.67 ug/g), followed by sulfur compounds diethyl disulfide (12.42ug/g) and diethyl trisulfide (5.97ug/g).\nFor D2, ethyl 2-methylbutanoate (29.68 ug/g) was relatively higher than in D101.\nIf we look at the total concentration of esters and sulfur compounds, D24 has the highest concentration of sulfur compounds (in line with the plot above). Comparing D2 and D101, the concentration of esters is about the same, but D2 has higher concentration of sulfur-containing compounds than D101. According to Takeoka et al. (1995), branched chain esters have lower odor thresholds than their straight chain counterparts. It appeared that D101, with slightly lower concentration of sulfur-compounds, would be perceived as more fruity. However, the authors found that D2 was perceived to have a stronger sweet and fruity odor; and that D101 was perceived to have a well-balanced aroma. I’m not quite sure why, I guess I would need to taste in person to find out!\nPCA\nI attempted to do PCA with the data provided, but it was a bit silly as n = 3, as I did not have the raw data with me. In addition, the assumptions for KMO and Bartlett’s tests were not met.\nDue to the very small number of observations, I ran into this error: Error in comps[, 1:object$num_comp, drop = FALSE] : subscript out of bounds\nAfter specifying that num_comp = 3, I did not receive this error message again.\nThe script below shows my attempt to reproduce the PCA variable loadings plot. I managed to get the same plot as the authors, so probably if I have raw data with me, that would be great. Note that I did not show the scree plot, eigenvalues and variance explained plot, as n=3 is really very small and PCA should not even be conducted. Nevertheless, it was an exercise in attempting to understand the conclusions drawn by the authors.\n\n\n# PACKAGES ####\nlibrary(pacman)\np_load(tidyverse, janitor, skimr, psych, tidymodels, learntidymodels)\n\n# IMPORT ####\n\ndurian <- read_csv(\"Durian.csv\") %>% \n  clean_names() %>% \n  mutate(peak_no_2 = paste( \"peak\", peak_no, sep = \"_\")) %>% \n  select(-peak_no) %>% \n  rename(peak_no = peak_no_2) %>% \n  select(peak_no, everything())\n\nglimpse(durian)\n\n\nRows: 39\nColumns: 7\n$ peak_no          <chr> \"peak_3\", \"peak_4\", \"peak_7\", \"peak_8\", \"p…\n$ compound         <chr> \"Ethyl acetate\", \"Methyl propanoate\", \"Eth…\n$ category         <chr> \"Ester\", \"Ester\", \"Ester\", \"Ester\", \"Ester…\n$ d101             <dbl> 0.28, 0.97, 3.11, 0.46, 0.19, 0.30, 4.07, …\n$ d2               <dbl> 0.61, 0.88, 1.85, 0.51, 0.09, 0.45, 2.33, …\n$ d24              <dbl> 0.93, 0.71, 2.53, 0.52, 0.56, 0.00, 2.29, …\n$ odor_description <chr> \"Pleasant, ethereal, fruity, brandy-like o…\n\n# so that can pivot longer later\n# durian$d101 <- as.character(durian$d101)\n# durian$d2 <- as.character(durian$d2)\ndurian$d24 <- as.numeric(durian$d24)\n\n# TRANSFORM #####\n\ndurian_reshape <- durian %>% \n  \n  # remove unnecessary columns\n  select(-category, -odor_description, -compound) %>% \n  # pivot longer for variety\n  pivot_longer(cols = starts_with(\"d\"),\n               names_to = \"variety\",\n               values_to = \"concentration\") %>% \n  \n  pivot_wider(names_from = peak_no,\n              values_from = concentration) %>% \n  \n  clean_names() %>% \n\n  # pivot wider for compound names as (X)/Features\n  dplyr::group_by(variety) %>% \n  dplyr::summarize_all(sum, na.rm = T)\n\n  \nglimpse(durian_reshape)  # 40 variables: 1Y and 39 X\n\n\nRows: 3\nColumns: 40\n$ variety <chr> \"d101\", \"d2\", \"d24\"\n$ peak_3  <dbl> 0.28, 0.61, 0.93\n$ peak_4  <dbl> 0.97, 0.88, 0.71\n$ peak_7  <dbl> 3.11, 1.85, 2.53\n$ peak_8  <dbl> 0.46, 0.51, 0.52\n$ peak_9  <dbl> 0.19, 0.09, 0.56\n$ peak_10 <dbl> 0.30, 0.45, 0.00\n$ peak_11 <dbl> 4.07, 2.33, 2.29\n$ peak_12 <dbl> 0.85, 2.22, 0.04\n$ peak_13 <dbl> 4.63, 1.74, 3.81\n$ peak_14 <dbl> 21.89, 29.68, 4.97\n$ peak_15 <dbl> 0.32, 0.22, 0.22\n$ peak_17 <dbl> 0.95, 0.63, 0.95\n$ peak_18 <dbl> 12.67, 4.77, 11.30\n$ peak_19 <dbl> 0.19, 0.00, 0.38\n$ peak_20 <dbl> 0.00, 0.14, 0.00\n$ peak_22 <dbl> 0.32, 1.70, 0.00\n$ peak_23 <dbl> 0.73, 0.00, 0.60\n$ peak_26 <dbl> 1.17, 5.52, 0.00\n$ peak_28 <dbl> 0.58, 0.45, 0.31\n$ peak_29 <dbl> 0.15, 0.25, 0.15\n$ peak_32 <dbl> 0.22, 0.10, 0.00\n$ peak_33 <dbl> 0.55, 0.55, 0.26\n$ peak_6  <dbl> 0.72, 1.09, 0.56\n$ peak_1  <dbl> 5.48, 4.26, 3.55\n$ peak_2  <dbl> 5.00, 2.72, 5.77\n$ peak_5  <dbl> 0.27, 0.00, 0.13\n$ peak_16 <dbl> 0.34, 0.00, 0.31\n$ peak_21 <dbl> 0.09, 0.06, 0.32\n$ peak_24 <dbl> 12.42, 15.85, 18.76\n$ peak_25 <dbl> 0.00, 0.00, 0.33\n$ peak_27 <dbl> 3.63, 3.35, 9.04\n$ peak_30 <dbl> 0.66, 0.14, 0.66\n$ peak_31 <dbl> 0.20, 0.11, 1.03\n$ peak_34 <dbl> 5.97, 14.68, 2.52\n$ peak_35 <dbl> 0.86, 1.73, 0.68\n$ peak_36 <dbl> 0.47, 1.46, 1.74\n$ peak_37 <dbl> 0.59, 1.47, 1.71\n$ peak_38 <dbl> 0.12, 0.16, 0.11\n$ peak_39 <dbl> 0.42, 0.49, 0.71\n\ndurian_reshape$variety <- factor(durian_reshape$variety)\n\n# EDA\nskim(durian_reshape)\n\n\nTable 1: Data summary\nName\ndurian_reshape\nNumber of rows\n3\nNumber of columns\n40\n_______________________\n\nColumn type frequency:\n\nfactor\n1\nnumeric\n39\n________________________\n\nGroup variables\nNone\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\nvariety\n0\n1\nFALSE\n3\nd10: 1, d2: 1, d24: 1\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\npeak_3\n0\n1\n0.61\n0.33\n0.28\n0.44\n0.61\n0.77\n0.93\n▇▁▇▁▇\npeak_4\n0\n1\n0.85\n0.13\n0.71\n0.79\n0.88\n0.92\n0.97\n▇▁▁▇▇\npeak_7\n0\n1\n2.50\n0.63\n1.85\n2.19\n2.53\n2.82\n3.11\n▇▁▇▁▇\npeak_8\n0\n1\n0.50\n0.03\n0.46\n0.48\n0.51\n0.52\n0.52\n▃▁▁▁▇\npeak_9\n0\n1\n0.28\n0.25\n0.09\n0.14\n0.19\n0.38\n0.56\n▇▇▁▁▇\npeak_10\n0\n1\n0.25\n0.23\n0.00\n0.15\n0.30\n0.38\n0.45\n▇▁▁▇▇\npeak_11\n0\n1\n2.90\n1.02\n2.29\n2.31\n2.33\n3.20\n4.07\n▇▁▁▁▃\npeak_12\n0\n1\n1.04\n1.10\n0.04\n0.44\n0.85\n1.54\n2.22\n▇▇▁▁▇\npeak_13\n0\n1\n3.39\n1.49\n1.74\n2.78\n3.81\n4.22\n4.63\n▇▁▁▇▇\npeak_14\n0\n1\n18.85\n12.63\n4.97\n13.43\n21.89\n25.78\n29.68\n▇▁▁▇▇\npeak_15\n0\n1\n0.25\n0.06\n0.22\n0.22\n0.22\n0.27\n0.32\n▇▁▁▁▃\npeak_17\n0\n1\n0.84\n0.18\n0.63\n0.79\n0.95\n0.95\n0.95\n▃▁▁▁▇\npeak_18\n0\n1\n9.58\n4.22\n4.77\n8.04\n11.30\n11.98\n12.67\n▃▁▁▁▇\npeak_19\n0\n1\n0.19\n0.19\n0.00\n0.10\n0.19\n0.29\n0.38\n▇▁▇▁▇\npeak_20\n0\n1\n0.05\n0.08\n0.00\n0.00\n0.00\n0.07\n0.14\n▇▁▁▁▃\npeak_22\n0\n1\n0.67\n0.90\n0.00\n0.16\n0.32\n1.01\n1.70\n▇▁▁▁▃\npeak_23\n0\n1\n0.44\n0.39\n0.00\n0.30\n0.60\n0.66\n0.73\n▃▁▁▁▇\npeak_26\n0\n1\n2.23\n2.91\n0.00\n0.58\n1.17\n3.34\n5.52\n▇▇▁▁▇\npeak_28\n0\n1\n0.45\n0.14\n0.31\n0.38\n0.45\n0.52\n0.58\n▇▁▇▁▇\npeak_29\n0\n1\n0.18\n0.06\n0.15\n0.15\n0.15\n0.20\n0.25\n▇▁▁▁▃\npeak_32\n0\n1\n0.11\n0.11\n0.00\n0.05\n0.10\n0.16\n0.22\n▇▁▇▁▇\npeak_33\n0\n1\n0.45\n0.17\n0.26\n0.41\n0.55\n0.55\n0.55\n▃▁▁▁▇\npeak_6\n0\n1\n0.79\n0.27\n0.56\n0.64\n0.72\n0.90\n1.09\n▇▇▁▁▇\npeak_1\n0\n1\n4.43\n0.98\n3.55\n3.90\n4.26\n4.87\n5.48\n▇▇▁▁▇\npeak_2\n0\n1\n4.50\n1.59\n2.72\n3.86\n5.00\n5.38\n5.77\n▇▁▁▇▇\npeak_5\n0\n1\n0.13\n0.14\n0.00\n0.06\n0.13\n0.20\n0.27\n▇▁▇▁▇\npeak_16\n0\n1\n0.22\n0.19\n0.00\n0.16\n0.31\n0.32\n0.34\n▃▁▁▁▇\npeak_21\n0\n1\n0.16\n0.14\n0.06\n0.07\n0.09\n0.21\n0.32\n▇▁▁▁▃\npeak_24\n0\n1\n15.68\n3.17\n12.42\n14.13\n15.85\n17.30\n18.76\n▇▁▇▁▇\npeak_25\n0\n1\n0.11\n0.19\n0.00\n0.00\n0.00\n0.16\n0.33\n▇▁▁▁▃\npeak_27\n0\n1\n5.34\n3.21\n3.35\n3.49\n3.63\n6.33\n9.04\n▇▁▁▁▃\npeak_30\n0\n1\n0.49\n0.30\n0.14\n0.40\n0.66\n0.66\n0.66\n▃▁▁▁▇\npeak_31\n0\n1\n0.45\n0.51\n0.11\n0.16\n0.20\n0.62\n1.03\n▇▁▁▁▃\npeak_34\n0\n1\n7.72\n6.27\n2.52\n4.24\n5.97\n10.32\n14.68\n▇▇▁▁▇\npeak_35\n0\n1\n1.09\n0.56\n0.68\n0.77\n0.86\n1.29\n1.73\n▇▁▁▁▃\npeak_36\n0\n1\n1.22\n0.67\n0.47\n0.96\n1.46\n1.60\n1.74\n▇▁▁▇▇\npeak_37\n0\n1\n1.26\n0.59\n0.59\n1.03\n1.47\n1.59\n1.71\n▇▁▁▇▇\npeak_38\n0\n1\n0.13\n0.03\n0.11\n0.11\n0.12\n0.14\n0.16\n▇▁▁▁▃\npeak_39\n0\n1\n0.54\n0.15\n0.42\n0.45\n0.49\n0.60\n0.71\n▇▇▁▁▇\n\n# no missing values\n# should do auto-scale and means centering later\n\n# Check assumptions for EDA\n\ndurian_no_y <- durian_reshape %>% \n  dplyr::select(-variety)\n\n# KMO test\ndurian_no_y %>% \n  cor() %>% \n  KMO() # overall MSA = 0.5\n\n\nError in solve.default(r) : \n  system is computationally singular: reciprocal condition number = 2.35978e-20\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = .)\nOverall MSA =  0.5\nMSA for each item = \n peak_3  peak_4  peak_7  peak_8  peak_9 peak_10 peak_11 peak_12 \n    0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5 \npeak_13 peak_14 peak_15 peak_17 peak_18 peak_19 peak_20 peak_22 \n    0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5 \npeak_23 peak_26 peak_28 peak_29 peak_32 peak_33  peak_6  peak_1 \n    0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5 \n peak_2  peak_5 peak_16 peak_21 peak_24 peak_25 peak_27 peak_30 \n    0.5     0.5     0.5     0.5     0.5     0.5     0.5     0.5 \npeak_31 peak_34 peak_35 peak_36 peak_37 peak_38 peak_39 \n    0.5     0.5     0.5     0.5     0.5     0.5     0.5 \n\n# Bartlett \n\ndurian_no_y %>% \n  cor() %>% \n  cortest.bartlett(., n = 3) # p = 1, by right not suitable for PCA\n\n\n$chisq\n[1] -Inf\n\n$p.value\n[1] 1\n\n$df\n[1] 741\n\n# 3 observations - not really ok for PCA\n\n# PCA ####\nglimpse(durian_reshape)\n\n\nRows: 3\nColumns: 40\n$ variety <fct> d101, d2, d24\n$ peak_3  <dbl> 0.28, 0.61, 0.93\n$ peak_4  <dbl> 0.97, 0.88, 0.71\n$ peak_7  <dbl> 3.11, 1.85, 2.53\n$ peak_8  <dbl> 0.46, 0.51, 0.52\n$ peak_9  <dbl> 0.19, 0.09, 0.56\n$ peak_10 <dbl> 0.30, 0.45, 0.00\n$ peak_11 <dbl> 4.07, 2.33, 2.29\n$ peak_12 <dbl> 0.85, 2.22, 0.04\n$ peak_13 <dbl> 4.63, 1.74, 3.81\n$ peak_14 <dbl> 21.89, 29.68, 4.97\n$ peak_15 <dbl> 0.32, 0.22, 0.22\n$ peak_17 <dbl> 0.95, 0.63, 0.95\n$ peak_18 <dbl> 12.67, 4.77, 11.30\n$ peak_19 <dbl> 0.19, 0.00, 0.38\n$ peak_20 <dbl> 0.00, 0.14, 0.00\n$ peak_22 <dbl> 0.32, 1.70, 0.00\n$ peak_23 <dbl> 0.73, 0.00, 0.60\n$ peak_26 <dbl> 1.17, 5.52, 0.00\n$ peak_28 <dbl> 0.58, 0.45, 0.31\n$ peak_29 <dbl> 0.15, 0.25, 0.15\n$ peak_32 <dbl> 0.22, 0.10, 0.00\n$ peak_33 <dbl> 0.55, 0.55, 0.26\n$ peak_6  <dbl> 0.72, 1.09, 0.56\n$ peak_1  <dbl> 5.48, 4.26, 3.55\n$ peak_2  <dbl> 5.00, 2.72, 5.77\n$ peak_5  <dbl> 0.27, 0.00, 0.13\n$ peak_16 <dbl> 0.34, 0.00, 0.31\n$ peak_21 <dbl> 0.09, 0.06, 0.32\n$ peak_24 <dbl> 12.42, 15.85, 18.76\n$ peak_25 <dbl> 0.00, 0.00, 0.33\n$ peak_27 <dbl> 3.63, 3.35, 9.04\n$ peak_30 <dbl> 0.66, 0.14, 0.66\n$ peak_31 <dbl> 0.20, 0.11, 1.03\n$ peak_34 <dbl> 5.97, 14.68, 2.52\n$ peak_35 <dbl> 0.86, 1.73, 0.68\n$ peak_36 <dbl> 0.47, 1.46, 1.74\n$ peak_37 <dbl> 0.59, 1.47, 1.71\n$ peak_38 <dbl> 0.12, 0.16, 0.11\n$ peak_39 <dbl> 0.42, 0.49, 0.71\n\n# recipe\ndurian_recipe <- recipe(~ ., data = durian_reshape) %>% \n  update_role(variety, new_role = \"id\") %>%  \n  # step_naomit(all_predictors()) %>% \n  step_normalize(all_predictors()) %>% \n  step_pca(all_predictors(), id = \"pca\", num_comp = 3)\n\n\n# need to specify num_comp = 3 if not will have error\n# Error in comps[, 1:object$num_comp, drop = FALSE] : \n# subscript out of bounds\n\n\n# prep: estimate the required parameters from a training set\n# that can be later applied to other data sets\n# returns an updated recipe with its estimates\n\ndurian_prep <- prep(durian_recipe)\n\ntidy_pca_loadings <- durian_prep %>% \n  tidy(id = \"pca\")\n\n\n# bake\n\ndurian_bake <- bake(durian_prep, durian_reshape)\n\n\n# plot loadings for top 8\n\nloadings_top_8 <- tidy_pca_loadings %>% \n  group_by(component) %>% \n  top_n(8, abs(value)) %>% \n  ungroup() %>% \n  mutate(terms = tidytext::reorder_within(terms, abs(value), component)) %>% \n  ggplot(aes(abs(value), terms, fill = value>0)) +\n  geom_col() +\n  facet_wrap(~component, scales = \"free_y\") +\n  tidytext::scale_y_reordered() +\n  ggthemes::scale_fill_few() +\n  theme_minimal()\n\n\njuice(durian_prep) %>% \n  ggplot(aes(PC1, PC2, label = variety)) +\n  geom_point(aes(col = variety), show.legend = F) +\n  geom_text() +\n  labs(x = \"PC1\",\n       y = \"PC2\") +\n  theme_classic()\n\n\n\n# loadings only\n\n# define arrow style\narrow_style <- arrow(angle = 30,\n                     length = unit(0.02, \"inches\"),\n                     type = \"closed\")\n\n# get pca loadings into wider format\npca_loadings_wider <- tidy_pca_loadings%>% \n  pivot_wider(names_from = component, id_cols = terms)\n\n\npca_loadings_only <- pca_loadings_wider %>% \n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_segment(aes(xend = PC1, yend = PC2),\n               x = 0, \n               y = 0,\n               arrow = arrow_style) +\n  ggrepel::geom_text_repel(aes(x = PC1, y = PC2, label = terms),\n                           hjust = 0, \n                           vjust = 1,\n                           size = 4,\n                           color = \"deepskyblue4\") +\n  labs(title = \"Loadings on PCs 1 and 2 for normalized data\") +\n  theme_classic()\n\n\n\n# check raw data\n\n# PC 1\npc1_raw <- durian %>% \n  filter(peak_no %in% c(\"peak_19\",\n                        \"peak_12\",\n                        \"peak_6\",\n                        \"peak_34\",\n                        \"peak_2\",\n                        \"peak_10\",\n                        \"peak_14\",\n                        \"peak_26\"))\n\n\n\n# PC 2\npc2_raw <- durian %>% \n  filter(peak_no %in% c(\"peak_11\",\n                        \"peak_15\",\n                        \"peak_8\",\n                        \"peak_37\",\n                        \"peak_36\",\n                        \"peak_1\",\n                        \"peak_32\",\n                        \"peak_24\"))\n\n\npc1_raw %>% arrange(peak_no)\n\n\n# A tibble: 8 x 7\n  peak_no compound    category   d101    d2   d24 odor_description    \n  <chr>   <chr>       <chr>     <dbl> <dbl> <dbl> <chr>               \n1 peak_10 Methyl but… Ester      0.3   0.45  0    Apple-like odor     \n2 peak_12 Ethyl buta… Ester      0.85  2.22  0.04 Fruity odor with pi…\n3 peak_14 Ethyl 2-me… Ester     21.9  29.7   4.97 Powerful green, fru…\n4 peak_19 Propyl 3-m… Ester      0.19  0     0.38 Fruity odor         \n5 peak_2  Propanethi… Sulfur-c…  5     2.72  5.77 Cabbage, sweet onio…\n6 peak_26 Ethyl hexa… Ester      1.17  5.52  0    Powerful fruity odo…\n7 peak_34 Diethyl tr… Sulfur-c…  5.97 14.7   2.52 Sweet alliaceous od…\n8 peak_6  Ethanol     Alcohol    0.72  1.09  0.56 <NA>                \n\npc2_raw %>%  arrange(peak_no)\n\n\n# A tibble: 8 x 7\n  peak_no compound      category   d101    d2   d24 odor_description  \n  <chr>   <chr>         <chr>     <dbl> <dbl> <dbl> <chr>             \n1 peak_1  Ethanethiol   Sulfur-c…  5.48  4.26  3.55 Onion, rubber odor\n2 peak_11 Methyl 2-but… Ester      4.07  2.33  2.29 Sweet fruity, app…\n3 peak_15 Ethyl 3-meth… Ester      0.32  0.22  0.22 Fruity odor remin…\n4 peak_24 Diethyl disu… Sulfur-c… 12.4  15.8  18.8  Sulfury, roasty, …\n5 peak_32 Methyl octan… Ester      0.22  0.1   0    Powerful winey, f…\n6 peak_36 3,5-dimethyl… Sulfur-c…  0.47  1.46  1.74 Sulfury, heavy, c…\n7 peak_37 3,5-dimethyl… Sulfur-c…  0.59  1.47  1.71 Sulfury, onion od…\n8 peak_8  Ethyl 2-meth… Ester      0.46  0.51  0.52 Fruity aromatic o…\n\npca_loadings_only\n\n\n\nloadings_top_8\n\n\n\n\nLearning pointers\nI feel that data visualization is a very important data exploratory tool to better understand your data. After data visualization, PCA can be performed to further explore your data and uncover latent structures. Together with the insights from earlier visualizations, the findings of PCA could be better interpreted.\nThe number of observations should not be so small until it is a bit meaningless to carry out PCA. This, was due to me carrying out analysis on aggregated data. I would need to remember to carry out more replicates if I am doing this experiment in the lab.\nWhat I like about the paper was that there was proper documentation on how extraction efficiency was optimised through sample size, vial size, the use of salting out, as well as equilibration time. The use of salting out is rather controversial as salt alters the equilibrium space between SPME fiber coatings and headspace. The results with and without addition of salt should always be compared to understand the effect of salt addition.\nIn addition, internal standard was used as a semi-quantitative analysis for relative concentration of volatile compounds. This would be better than just comparing percentage area of compounds because it gives the concentration in “absolute” value. However, it is still a semi-quantitative method as the IS cannot correct for differences in ionization during analysis, but it is better than nothing.\nFlavor analysis is not straightforward as numbers used to describe concentration do not indicate odor threshold and intensity perceived. They also do not descripe the type of odor. I wonder if text analysis could be applied to odor descriptions in flavor analysis? Odor threshold is further influenced by chemical structure, and extraction efficiency is also affected by sample matrix and volatility of compound when SPME is used as extraction. SPME offers a snapshot of flavor of food, but it would be more robust to compare against other extraction techniques as well. The ideal extraction method should not introduce artefacts (high temperature extraction, use of solvents etc), and requires high-end techniques. Alas, not every lab is that well-equipped. However, we should always make sure that our data is “clean,” so that our insights are factually correct and not contaminated by errors in extraction. The most advanced data analytics cannot correct for erroneous data, and any further analysis on such data carries no meaning.\n\n\n\nChin, S. T., S. A. H. Nazimah, S. Y. Quek, Y. B. Che Man, R. Abdul Rahman, and D. Mat Hashim. 2007. “Analysis of Volatile Compounds from Malaysian Durians (Durio Zibethinus) Using Headspace SPME Coupled to Fast GC-MS.” Journal of Food Composition and Analysis 20 (1): 31–44. https://doi.org/10.1016/j.jfca.2006.04.011.\n\n\nTakeoka, Gary R., Ron G. Buttery, Jean G. Turnbaugh, and Mabry Benson. 1995. “Odor Thresholds of Various Branched Esters.” LWT - Food Science and Technology 28 (1): 153–56. https://doi.org/10.1016/S0023-6438(95)80028-X.\n\n\nTeh, Bin Tean, Kevin Lim, Chern Han Yong, Cedric Chuan Young Ng, Sushma Ramesh Rao, Vikneswari Rajasegaran, Weng Khong Lim, et al. 2017. “The Draft Genome of Tropical Fruit Durian ( Durio Zibethinus ).” Nature Genetics 49 (11): 1633–41. https://doi.org/10.1038/ng.3972.\n\n\n\n\n",
    "preview": "posts/20210216_durian volatiles/Durian-Volatiles_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-02-16T23:53:30+08:00",
    "input_file": {},
    "preview_width": 2304,
    "preview_height": 1152
  },
  {
    "path": "posts/20210209_tidy_tuesday_coffee_ratings/",
    "title": "Tidy Tuesday on Coffee Ratings Dataset",
    "description": "Exploratory Data Analysis on Coffee Ratings",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-02-09",
    "categories": [],
    "contents": "\nObjective\nTo practice data transformation and visualization on a tidytuesday dataset that is relatable to food (since I am a food science graduate).\nThe main areas that I will focus on would be the scoring differences between types of coffee (Arabica vs Robusta), processing methods (Wet vs Dry), country of origin/companies (top 6 by score), as well as varieties (top 6 by count).\nLoad packages\n\n\nlibrary(pacman)\np_load(tidyverse,skimr,tidytuesdayR, ggthemes, GGally, broom)\n\n\n\nImport\n\n\ntuesdata <- tidytuesdayR::tt_load(2020, week = 28)\n\n\n\n    Downloading file 1 of 1: `coffee_ratings.csv`\n\ncoffee_ratings <- tuesdata$coffee_ratings\n\n\n\nUnderstanding the data\nSkimming the data using the skimr package.\n\n\nskim(coffee_ratings)\n\n\nTable 1: Data summary\nName\ncoffee_ratings\nNumber of rows\n1339\nNumber of columns\n43\n_______________________\n\nColumn type frequency:\n\ncharacter\n24\nnumeric\n19\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nspecies\n0\n1.00\n7\n7\n0\n2\n0\nowner\n7\n0.99\n3\n50\n0\n315\n0\ncountry_of_origin\n1\n1.00\n4\n28\n0\n36\n0\nfarm_name\n359\n0.73\n1\n73\n0\n571\n0\nlot_number\n1063\n0.21\n1\n71\n0\n227\n0\nmill\n315\n0.76\n1\n77\n0\n460\n0\nico_number\n151\n0.89\n1\n40\n0\n847\n0\ncompany\n209\n0.84\n3\n73\n0\n281\n0\naltitude\n226\n0.83\n1\n41\n0\n396\n0\nregion\n59\n0.96\n2\n76\n0\n356\n0\nproducer\n231\n0.83\n1\n100\n0\n691\n0\nbag_weight\n0\n1.00\n1\n8\n0\n56\n0\nin_country_partner\n0\n1.00\n7\n85\n0\n27\n0\nharvest_year\n47\n0.96\n3\n24\n0\n46\n0\ngrading_date\n0\n1.00\n13\n20\n0\n567\n0\nowner_1\n7\n0.99\n3\n50\n0\n319\n0\nvariety\n226\n0.83\n4\n21\n0\n29\n0\nprocessing_method\n170\n0.87\n5\n25\n0\n5\n0\ncolor\n218\n0.84\n4\n12\n0\n4\n0\nexpiration\n0\n1.00\n13\n20\n0\n566\n0\ncertification_body\n0\n1.00\n7\n85\n0\n26\n0\ncertification_address\n0\n1.00\n40\n40\n0\n32\n0\ncertification_contact\n0\n1.00\n40\n40\n0\n29\n0\nunit_of_measurement\n0\n1.00\n1\n2\n0\n2\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\ntotal_cup_points\n0\n1.00\n82.09\n3.50\n0\n81.08\n82.50\n83.67\n90.58\n▁▁▁▁▇\nnumber_of_bags\n0\n1.00\n154.18\n129.99\n0\n14.00\n175.00\n275.00\n1062.00\n▇▇▁▁▁\naroma\n0\n1.00\n7.57\n0.38\n0\n7.42\n7.58\n7.75\n8.75\n▁▁▁▁▇\nflavor\n0\n1.00\n7.52\n0.40\n0\n7.33\n7.58\n7.75\n8.83\n▁▁▁▁▇\naftertaste\n0\n1.00\n7.40\n0.40\n0\n7.25\n7.42\n7.58\n8.67\n▁▁▁▁▇\nacidity\n0\n1.00\n7.54\n0.38\n0\n7.33\n7.58\n7.75\n8.75\n▁▁▁▁▇\nbody\n0\n1.00\n7.52\n0.37\n0\n7.33\n7.50\n7.67\n8.58\n▁▁▁▁▇\nbalance\n0\n1.00\n7.52\n0.41\n0\n7.33\n7.50\n7.75\n8.75\n▁▁▁▁▇\nuniformity\n0\n1.00\n9.83\n0.55\n0\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\n0\n1.00\n9.84\n0.76\n0\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\n0\n1.00\n9.86\n0.62\n0\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\ncupper_points\n0\n1.00\n7.50\n0.47\n0\n7.25\n7.50\n7.75\n10.00\n▁▁▁▇▁\nmoisture\n0\n1.00\n0.09\n0.05\n0\n0.09\n0.11\n0.12\n0.28\n▃▇▅▁▁\ncategory_one_defects\n0\n1.00\n0.48\n2.55\n0\n0.00\n0.00\n0.00\n63.00\n▇▁▁▁▁\nquakers\n1\n1.00\n0.17\n0.83\n0\n0.00\n0.00\n0.00\n11.00\n▇▁▁▁▁\ncategory_two_defects\n0\n1.00\n3.56\n5.31\n0\n0.00\n2.00\n4.00\n55.00\n▇▁▁▁▁\naltitude_low_meters\n230\n0.83\n1750.71\n8669.44\n1\n1100.00\n1310.64\n1600.00\n190164.00\n▇▁▁▁▁\naltitude_high_meters\n230\n0.83\n1799.35\n8668.81\n1\n1100.00\n1350.00\n1650.00\n190164.00\n▇▁▁▁▁\naltitude_mean_meters\n230\n0.83\n1775.03\n8668.63\n1\n1100.00\n1310.64\n1600.00\n190164.00\n▇▁▁▁▁\n\nThis is an incomplete dataset. I am not familiar with all the terms, such as ICO number, altitude, certification details.\nTo address my focal questions, I would need to take note that there are missing values in:\ncountry of origin\nvariety\nprocessing method\nThe distribution for scoring criteria is quite right-skewed. The total cup points is also very right skewed, most of the coffee graded are probably good coffee, so this may not be a representative dataset since it only contains information on above average coffee, but does not show data for average and sub-par coffee.\nSpecies: Arabica vs Robusta\n\n\ncoffee_ratings %>% \n  select(species) %>% \n  count(species) %>% # equivalent to df %>% group_by(a, b) %>% summarise(n = n()).\n  mutate(percentage = n/sum(n)*100) %>%  # need not group by first\n  ggplot(aes(species,percentage)) +\n  geom_col(aes(fill = species)) +\n  scale_fill_few() + # ggthemes: Color scales from Few's \"Practical Rules for Using Color in Charts\"\n  labs(title = \"Distribution of Arabica and Robusta coffe\",\n       subtitle = \"Most of the coffee graded are Arabica coffee\",\n       x = \"Species\",\n       y = \"Percentage of samples\",\n       caption = \"Source: Coffee Quality Institute\") +\n  theme_clean()\n\n\n\n\nEven though there is very little representation from Robusta coffee, which is considered to be a more inferior type, out of curiosity and for data exploratory purposes, I will look at the averate total cup score. Personally, I prefer the Robusta type of coffee unique to Singapore and Malaysia because of the way coffee beans are fried with butter and sugar, which gives it a unique aromatic taste.\n\n\ncoffee_ratings %>% \n  select(species,total_cup_points) %>% \n  group_by(species) %>% \n  summarise(mean = mean(total_cup_points)) %>% \n  ggplot(aes(x = species, y = mean, label = round(mean,1))) +\n  geom_col(aes(fill = species)) +\n  geom_text(aes(label = round(mean,1)), vjust = -0.5) +\n  scale_fill_few() +\n  labs(title = \"Mean Total Cup Points for Arabica and Robusta\",\n       subtitle = \"Arabica has higher mean score than Robusta\",\n       caption  = \"Source: Coffee Quality Institute\") +\n  ylim(0,100) +\n  theme_clean()\n\n\n\n\nProcessing method\nTo compare like with like, I will look the effect of processing methods on scores for Arabica coffee only.\n\n\narabica <- coffee_ratings %>% \n  filter(species == \"Arabica\")\n\n\n\nThe plot below shows what the commonly used processing methods are.\n\n\narabica %>% \n  filter(!is.na(processing_method)) %>% \n  count(processing_method) %>% \n  mutate(percentage = n/sum(n)*100) %>% \n  arrange(desc(percentage)) %>% \n  ggplot(aes(reorder(processing_method, percentage),percentage),\n         label = round(percentage,1)) +\n  geom_col(aes(fill = processing_method), width = 0.75) +\n  scale_color_few() +\n  geom_text(aes(label = round(percentage,1), hjust = -0.15)) +\n  labs(title = \"Distribution by Processing Method\",\n       subtitle = \"Most of the Arabica Coffee were either Wet or Dry Processed\",\n       caption = \"Source: Coffee Quality Institute\",\n       x = NULL) +\n  coord_flip() +\n  theme_clean() +\n  theme(legend.position = \"none\")\n\n\n\n\nI did some reading online (see Reference section below), and found that there were three main types of processing methods:\nWet/Washed: Most specialty coffees are washed, and the fruit flesh is removed from the bean before the beans are dried. There should be enough inherently present natural sugars in the bean so that sweetness will not be compromised.\nDry/Natural: The fruit remains on the bean and dries undisturbed. This is considered to be a lower quality method that may lead to inconsistent flavors due to unripe fruit drying and turning brown alongside ripe fruits.\nHoney: Often has a rounded acidity than washed coffees, with intense sweetness and complex mouthfeel.\nOthers: May include anaerobic processing, carbonic maceration etc.\nFor the purpose of comparing the scores across processing methods, I will just look at Wet vs Dry processing.\nHowever, it is important to compare like with like for different processing methods. What does the total cup points mean? The total cup points could be used as a classifier:\n95 - 100: Super Premium Specialty\n90 - 94: Premium Specialty\n85 - 89: Specialty\n80 - 84: Premium\n75 - 79: Usual Good Quality\n70 - 74: Average Quality\n60 - 70: Exchange grade\n50 - 60: Commercial grade\nI will add in the class into the dataset to compare effect of processing method in the class with the most datapoints.\nEDA on total cup points\n\n\nsensory <- coffee_ratings %>% \n  select(total_cup_points, species, country_of_origin,\n         processing_method:category_two_defects)\n\nsensory %>% \n  ggplot(aes(total_cup_points)) +\n  geom_histogram(fill = \"chocolate4\") +\n  theme_few()\n\n\n\nmin(sensory$total_cup_points)  # 0 : has missing values\n\n\n[1] 0\n\ntable(sensory$total_cup_points) # 1 missing value, lowest is 59.83\n\n\n\n    0 59.83 63.08 67.92 68.33 69.17 69.33 70.67 70.75    71 71.08 \n    1     1     1     1     1     2     1     1     1     1     1 \n71.75 72.33 72.58 72.83 72.92 73.42  73.5 73.67 73.75 73.83    74 \n    1     1     1     1     1     1     1     1     1     1     1 \n74.33 74.42 74.67 74.75 74.83 74.92    75 75.08 75.17  75.5 75.58 \n    2     1     1     2     1     1     1     1     3     1     2 \n75.67 75.83    76 76.08 76.17 76.25 76.33 76.42  76.5 76.75 76.83 \n    1     1     1     1     3     1     2     1     1     1     1 \n   77 77.17 77.25 77.33 77.42  77.5 77.58 77.67 77.83 77.92    78 \n    1     2     2     3     1     1     1     1     3     3     8 \n78.08 78.17 78.25 78.33 78.42  78.5 78.58 78.67 78.75 78.83 78.92 \n    2     1     2     5     2     3     7     2     6     1     2 \n   79 79.08 79.17 79.25 79.33 79.42  79.5 79.58 79.67 79.75 79.83 \n    6     6     8     2     6     3     5     4     8    13     5 \n79.92    80 80.08 80.17 80.25 80.33 80.42  80.5 80.58 80.67 80.75 \n    9     8     8    11    11     8     7    12     9    11    12 \n80.83 80.92    81 81.08 81.17 81.25 81.33 81.42  81.5 81.58 81.67 \n    7    18    15    12    15    10    12    17    26    17    25 \n81.75 81.83 81.92    82 82.08 82.17 82.25 82.33 82.42  82.5 82.58 \n   12    26    18    21    17    21    22    29    32    23    21 \n82.67 82.75 82.83 82.92    83 83.08 83.17 83.25 83.33 83.38 83.42 \n   26    30    19    26    39    18    38    25    20     1    20 \n 83.5 83.58 83.67 83.75 83.83 83.92    84 84.08 84.13 84.17 84.25 \n   25    16    21    20    21    16    18     8     1    21    19 \n84.33 84.42  84.5 84.58 84.67 84.75 84.83 84.92    85 85.08 85.17 \n   12     8    13    14    19     5     5     9    10     8     2 \n85.25 85.33 85.42  85.5 85.58 85.75 85.83 85.92    86 86.08 86.17 \n    3     8     5     5     3     3     4     3     6     3     4 \n86.25 86.33 86.42  86.5 86.58 86.67 86.83 86.92 87.08 87.17 87.25 \n    5     1     1     1     2     1     1     2     2     2     3 \n87.33 87.42 87.58 87.83 87.92 88.08 88.25 88.42 88.67 88.75 88.83 \n    1     1     1     1     3     1     1     1     1     1     2 \n   89 89.75 89.92 90.58 \n    1     1     1     1 \n\nCreating a classification variable\n\n\nsensory_with_category <- sensory %>% \n  filter(total_cup_points != 0) %>% # remove zero score\n  mutate(classification = ifelse(total_cup_points > 95, \"Super Premium Specialty\",\n                                 ifelse(total_cup_points >90, \"Premium Specialty\",\n                                        ifelse(total_cup_points >85, \"Specialty\",\n                                               ifelse(total_cup_points >80, \"Premium\",\n                                                      ifelse(total_cup_points >75, \"Usual Good Quality\",\n                                                             ifelse(total_cup_points >70, \"Average Quality\",\n                                                                    ifelse(total_cup_points >60, \"Exchange grade\",\n                                                                           \"Commercial grade\"))))))))\n\n\n\nUnderstanding the coffee with the highest score:\n\n\nsensory_with_category %>% \n  select(total_cup_points, classification) %>% \n  arrange(desc(total_cup_points))\n\n\n# A tibble: 1,338 x 2\n   total_cup_points classification   \n              <dbl> <chr>            \n 1             90.6 Premium Specialty\n 2             89.9 Specialty        \n 3             89.8 Specialty        \n 4             89   Specialty        \n 5             88.8 Specialty        \n 6             88.8 Specialty        \n 7             88.8 Specialty        \n 8             88.7 Specialty        \n 9             88.4 Specialty        \n10             88.2 Specialty        \n# … with 1,328 more rows\n\nmin(coffee_ratings$total_cup_points)\n\n\n[1] 0\n\n# which coffee had the highest score?\ncoffee_ratings %>% \n  filter(total_cup_points == max(coffee_ratings$total_cup_points)) %>% \n  t() # transpose\n\n\n                      [,1]                                      \ntotal_cup_points      \"90.58\"                                   \nspecies               \"Arabica\"                                 \nowner                 \"metad plc\"                               \ncountry_of_origin     \"Ethiopia\"                                \nfarm_name             \"metad plc\"                               \nlot_number            NA                                        \nmill                  \"metad plc\"                               \nico_number            \"2014/2015\"                               \ncompany               \"metad agricultural developmet plc\"       \naltitude              \"1950-2200\"                               \nregion                \"guji-hambela\"                            \nproducer              \"METAD PLC\"                               \nnumber_of_bags        \"300\"                                     \nbag_weight            \"60 kg\"                                   \nin_country_partner    \"METAD Agricultural Development plc\"      \nharvest_year          \"2014\"                                    \ngrading_date          \"April 4th, 2015\"                         \nowner_1               \"metad plc\"                               \nvariety               NA                                        \nprocessing_method     \"Washed / Wet\"                            \naroma                 \"8.67\"                                    \nflavor                \"8.83\"                                    \naftertaste            \"8.67\"                                    \nacidity               \"8.75\"                                    \nbody                  \"8.5\"                                     \nbalance               \"8.42\"                                    \nuniformity            \"10\"                                      \nclean_cup             \"10\"                                      \nsweetness             \"10\"                                      \ncupper_points         \"8.75\"                                    \nmoisture              \"0.12\"                                    \ncategory_one_defects  \"0\"                                       \nquakers               \"0\"                                       \ncolor                 \"Green\"                                   \ncategory_two_defects  \"0\"                                       \nexpiration            \"April 3rd, 2016\"                         \ncertification_body    \"METAD Agricultural Development plc\"      \ncertification_address \"309fcf77415a3661ae83e027f7e5f05dad786e44\"\ncertification_contact \"19fef5a731de2db57d16da10287413f5f99bc2dd\"\nunit_of_measurement   \"m\"                                       \naltitude_low_meters   \"1950\"                                    \naltitude_high_meters  \"2200\"                                    \naltitude_mean_meters  \"2075\"                                    \n\n# which coffee had the lowest score?\ncoffee_ratings %>% \n  filter(total_cup_points == 59.83) %>% \n  t() # transpose\n\n\n                      [,1]                                      \ntotal_cup_points      \"59.83\"                                   \nspecies               \"Arabica\"                                 \nowner                 \"juan luis alvarado romero\"               \ncountry_of_origin     \"Guatemala\"                               \nfarm_name             \"finca el limon\"                          \nlot_number            NA                                        \nmill                  \"beneficio serben\"                        \nico_number            \"11/853/165\"                              \ncompany               \"unicafe\"                                 \naltitude              \"4650\"                                    \nregion                \"nuevo oriente\"                           \nproducer              \"WILLIAM ESTUARDO MARTINEZ PACHECO\"       \nnumber_of_bags        \"275\"                                     \nbag_weight            \"1 kg\"                                    \nin_country_partner    \"Asociacion Nacional Del Café\"            \nharvest_year          \"2012\"                                    \ngrading_date          \"May 24th, 2012\"                          \nowner_1               \"Juan Luis Alvarado Romero\"               \nvariety               \"Catuai\"                                  \nprocessing_method     \"Washed / Wet\"                            \naroma                 \"7.5\"                                     \nflavor                \"6.67\"                                    \naftertaste            \"6.67\"                                    \nacidity               \"7.67\"                                    \nbody                  \"7.33\"                                    \nbalance               \"6.67\"                                    \nuniformity            \"8\"                                       \nclean_cup             \"1.33\"                                    \nsweetness             \"1.33\"                                    \ncupper_points         \"6.67\"                                    \nmoisture              \"0.1\"                                     \ncategory_one_defects  \"0\"                                       \nquakers               \"0\"                                       \ncolor                 \"Green\"                                   \ncategory_two_defects  \"4\"                                       \nexpiration            \"May 24th, 2013\"                          \ncertification_body    \"Asociacion Nacional Del Café\"            \ncertification_address \"b1f20fe3a819fd6b2ee0eb8fdc3da256604f1e53\"\ncertification_contact \"724f04ad10ed31dbb9d260f0dfd221ba48be8a95\"\nunit_of_measurement   \"ft\"                                      \naltitude_low_meters   \"1417.32\"                                 \naltitude_high_meters  \"1417.32\"                                 \naltitude_mean_meters  \"1417.32\"                                 \n\n# min score is actually 0, which is a missing datapoint.\n\n\n\n\n\n# distribution of types of coffee\nsensory_with_category %>% \n  filter(species == \"Arabica\",\n         processing_method %in% c(\"Natural / Dry\", \"Washed / Wet\")) %>% \n  count(classification, processing_method) %>% \n  ggplot(aes(fct_reorder(classification, n), n, label = n)) + \n  geom_col(aes(fill = classification)) +\n  scale_color_few() +\n  labs(title = \"Distribution of types of Arabica coffees, by processing method\",\n       subtitle = \"Most of the premium coffee (with cup scores 80 - 84) are processed by Washed/Wet method.\",\n       caption = \"Source: Coffee Quality Institute\") +\n  facet_grid(processing_method ~. ) +\n  theme_clean() +\n  coord_flip() +\n  theme(legend.position = \"none\")\n\n\n\n\nThe Premium category has the most number of datapoints, and I will focus on this category for analysis.\n\n\nplot_sensory_total_boxplot <- sensory_with_category %>% \n  filter(classification == \"Premium\",\n         species == \"Arabica\",\n         processing_method %in% c(\"Natural / Dry\", \"Washed / Wet\")) %>% \n  mutate(processing_mtd_fct = ifelse(processing_method == c(\"Natural / Dry\"), \"Dry\",\n                                     \"Wet\")) %>% \n  select(total_cup_points, processing_mtd_fct) %>% \n  ggplot(aes(x = processing_mtd_fct, y = total_cup_points)) +\n  geom_boxplot(aes(col = processing_mtd_fct),notch = T) +\n  stat_summary(fun.data = \"mean_cl_normal\",\n           geom = \"errorbar\",\n           fun.args = (conf.int = 0.95),\n           color = \"forestgreen\") +\n  geom_jitter(aes(col = processing_mtd_fct), alpha = 0.3) +\n  scale_color_manual(values = c(\"Dry\" = \"chocolate4\",\n                                \"Wet\" = \"cadetblue4\")) +\n  labs(title = \"Comparison of Mean Total Cup Points for Dry vs Wet Processing in Arabica Coffee\",\n       subtitle = \"The Mean Total Cup Points are very similar for both processing methods\",\n       caption = \"Source: Coffee Quality Institute\",\n       x = \"Processing Method\",\n       y = \"Total Cup Points\") +\n  theme_few() +\n  theme(legend.position = \"none\")\n\nplot_sensory_total_boxplot \n\n\n\n\n\n\nplot_sensory_boxplot <- sensory_with_category %>% \n  filter(classification == \"Premium\",\n         species == \"Arabica\",\n         processing_method %in% c(\"Natural / Dry\", \"Washed / Wet\")) %>% \n  mutate(processing_mtd_fct = ifelse(processing_method == c(\"Natural / Dry\"), \"Dry\",\n                                     \"Wet\")) %>% \n  select(-quakers, -color, - category_one_defects, \n         - category_two_defects, - processing_method) %>% \n  pivot_longer(cols = aroma:cupper_points,\n               names_to = \"parameters\",\n               values_to = \"score\") %>% \n  mutate(parameters_fct = factor(parameters,\n                                 levels = c(\"acidity\", \"aroma\", \"clean_cup\",\n                                            \"sweetness\", \"uniformity\", \"aftertaste\",\n                                            \"balance\", \"body\", \"cupper_points\", \"flavor\"\n                                 ))) %>% \n  ggplot(aes(x = processing_mtd_fct, y = score)) +\n  geom_boxplot(aes(col = processing_mtd_fct), notch = T, size = 1) +\n  geom_jitter(aes(col = processing_mtd_fct), alpha = 0.1) +\n  scale_color_manual(values = c(\"Dry\" = \"chocolate4\",\n                                \"Wet\" = \"cadetblue4\")) +\n  facet_wrap(vars(parameters_fct), scales = \"free\", ncol= 5) +\n  labs(x = NULL,\n       title = \"Comparison of mean score for Arabica coffee: Dry vs Wet Processing\",\n       subtitle = \"Wet processed coffee has higher average scores for acidity, aroma, clean_cup, sweetness, uniformity.\",\n       caption = \"Source: Coffee Quality Institute\") +\n  theme_few() +\n  theme(legend.position = \"none\")\n\nplot_sensory_boxplot\n\n\n\n\nCountry of origin/Owner\n\n\n# plot to see which countries are above/below mean rating\n\narabica_dotplot <- arabica %>% \n  filter(!is.na(country_of_origin)) %>% # 1 missing value\n  group_by(country_of_origin) %>% \n  summarise(mean_rating = mean(total_cup_points)) %>% \n  mutate(above_below_mean = as.factor(ifelse(mean_rating > mean(arabica$total_cup_points),\n                                             \"above_mean\", \"below_mean\"))) %>% \n  ggplot(aes(x = reorder(country_of_origin, mean_rating), \n             y = mean_rating, \n             col = above_below_mean,\n             label = round(mean_rating,1))) +\n  geom_point(aes(col = above_below_mean), stat = \"identity\", size = 9) +\n  scale_color_few() +\n  geom_text(col = \"black\", size = 4) +\n  geom_hline(aes(yintercept = mean(arabica$total_cup_points)), size = 2,\n             col = \"grey\")+\n  labs(title = \"Dot plot for Arabica Coffee Ratings\",\n       subtitle = \"Countries with ratings above mean values are coloured blue,\\nand countries below mean values are colored orange.\",\n       x =  \"Country of Origin\",\n       y = \"Mean Rating\",\n       caption = \"Source: Coffee Quality Institute\") +\n  coord_flip() +\n  theme_clean() +\n  theme(legend.position = \"none\",\n        axis.title = element_text(size = 16, face = \"bold\"),\n        axis.text = element_text(size = 14),\n        title = element_text(size = 20, face = \"bold\"))\n\narabica_dotplot\n\n\n\n\n\n\n# sensory scores for arabica coffee, top scorers for sensory\n\nsensory_by_country <- coffee_ratings %>% \n  filter(species == \"Arabica\",\n         !total_cup_points %in% 0,\n         !is.na(country_of_origin),\n         !is.na(owner)) %>% \n  select(country_of_origin, owner, \n         total_cup_points, aroma:cupper_points)\n\n\n\n\n\nskim(sensory_by_country)\n\n\nTable 2: Data summary\nName\nsensory_by_country\nNumber of rows\n1302\nNumber of columns\n13\n_______________________\n\nColumn type frequency:\n\ncharacter\n2\nnumeric\n11\n________________________\n\nGroup variables\nNone\nVariable type: character\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\ncountry_of_origin\n0\n1\n4\n28\n0\n36\n0\nowner\n0\n1\n3\n50\n0\n305\n0\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\ntotal_cup_points\n0\n1\n82.18\n2.69\n59.83\n81.17\n82.50\n83.67\n90.58\n▁▁▁▇▁\naroma\n0\n1\n7.57\n0.32\n5.08\n7.42\n7.58\n7.75\n8.75\n▁▁▂▇▁\nflavor\n0\n1\n7.52\n0.34\n6.08\n7.33\n7.58\n7.75\n8.83\n▁▂▇▃▁\naftertaste\n0\n1\n7.40\n0.35\n6.17\n7.25\n7.42\n7.58\n8.67\n▁▃▇▂▁\nacidity\n0\n1\n7.54\n0.32\n5.25\n7.33\n7.50\n7.75\n8.75\n▁▁▃▇▁\nbody\n0\n1\n7.52\n0.29\n5.25\n7.33\n7.50\n7.67\n8.58\n▁▁▁▇▁\nbalance\n0\n1\n7.52\n0.35\n6.08\n7.33\n7.50\n7.75\n8.75\n▁▂▇▃▁\nuniformity\n0\n1\n9.84\n0.49\n6.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\n0\n1\n9.84\n0.72\n0.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\n0\n1\n9.91\n0.46\n1.33\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\ncupper_points\n0\n1\n7.50\n0.43\n5.17\n7.25\n7.50\n7.75\n10.00\n▁▂▇▁▁\n\nLooking at the coffee with clean cup score = 0: Is it really that the coffee had a score of 0? Or was it a data entry mistake?\n\n\n# there is one datapoint in which clean_cup score = 0\ncoffee_ratings %>% \n  filter(clean_cup == 0) %>% \n  t() # transpose\n\n\n                      [,1]                                                  \ntotal_cup_points      \"68.33\"                                               \nspecies               \"Arabica\"                                             \nowner                 \"juan carlos garcia lopez\"                            \ncountry_of_origin     \"Mexico\"                                              \nfarm_name             \"el centenario\"                                       \nlot_number            NA                                                    \nmill                  \"la esperanza, municipio juchique de ferrer, veracruz\"\nico_number            \"1104328663\"                                          \ncompany               \"terra mia\"                                           \naltitude              \"900\"                                                 \nregion                \"juchique de ferrer\"                                  \nproducer              \"JUAN CARLOS GARCÍA LOPEZ\"                            \nnumber_of_bags        \" 12\"                                                 \nbag_weight            \"1 kg\"                                                \nin_country_partner    \"AMECAFE\"                                             \nharvest_year          \"2012\"                                                \ngrading_date          \"September 17th, 2012\"                                \nowner_1               \"JUAN CARLOS GARCIA LOPEZ\"                            \nvariety               \"Bourbon\"                                             \nprocessing_method     \"Washed / Wet\"                                        \naroma                 \"7.08\"                                                \nflavor                \"6.83\"                                                \naftertaste            \"6.25\"                                                \nacidity               \"7.42\"                                                \nbody                  \"7.25\"                                                \nbalance               \"6.75\"                                                \nuniformity            \"10\"                                                  \nclean_cup             \"0\"                                                   \nsweetness             \"10\"                                                  \ncupper_points         \"6.75\"                                                \nmoisture              \"0.11\"                                                \ncategory_one_defects  \"0\"                                                   \nquakers               \"0\"                                                   \ncolor                 \"None\"                                                \ncategory_two_defects  \"20\"                                                  \nexpiration            \"September 17th, 2013\"                                \ncertification_body    \"AMECAFE\"                                             \ncertification_address \"59e396ad6e22a1c22b248f958e1da2bd8af85272\"            \ncertification_contact \"0eb4ee5b3f47b20b049548a2fd1e7d4a2b70d0a7\"            \nunit_of_measurement   \"m\"                                                   \naltitude_low_meters   \" 900\"                                                \naltitude_high_meters  \" 900\"                                                \naltitude_mean_meters  \" 900\"                                                \n                      [,2]                                      \ntotal_cup_points      \" 0.00\"                                   \nspecies               \"Arabica\"                                 \nowner                 \"bismarck castro\"                         \ncountry_of_origin     \"Honduras\"                                \nfarm_name             \"los hicaques\"                            \nlot_number            \"103\"                                     \nmill                  \"cigrah s.a de c.v.\"                      \nico_number            \"13-111-053\"                              \ncompany               \"cigrah s.a de c.v\"                       \naltitude              \"1400\"                                    \nregion                \"comayagua\"                               \nproducer              \"Reinerio Zepeda\"                         \nnumber_of_bags        \"275\"                                     \nbag_weight            \"69 kg\"                                   \nin_country_partner    \"Instituto Hondureño del Café\"            \nharvest_year          \"2017\"                                    \ngrading_date          \"April 28th, 2017\"                        \nowner_1               \"Bismarck Castro\"                         \nvariety               \"Caturra\"                                 \nprocessing_method     NA                                        \naroma                 \"0.00\"                                    \nflavor                \"0.00\"                                    \naftertaste            \"0.00\"                                    \nacidity               \"0.00\"                                    \nbody                  \"0.00\"                                    \nbalance               \"0.00\"                                    \nuniformity            \" 0\"                                      \nclean_cup             \"0\"                                       \nsweetness             \" 0\"                                      \ncupper_points         \"0.00\"                                    \nmoisture              \"0.12\"                                    \ncategory_one_defects  \"0\"                                       \nquakers               \"0\"                                       \ncolor                 \"Green\"                                   \ncategory_two_defects  \" 2\"                                      \nexpiration            \"April 28th, 2018\"                        \ncertification_body    \"Instituto Hondureño del Café\"            \ncertification_address \"b4660a57e9f8cc613ae5b8f02bfce8634c763ab4\"\ncertification_contact \"7f521ca403540f81ec99daec7da19c2788393880\"\nunit_of_measurement   \"m\"                                       \naltitude_low_meters   \"1400\"                                    \naltitude_high_meters  \"1400\"                                    \naltitude_mean_meters  \"1400\"                                    \n\n# one is missing value, already filtered out for total_cup_points = 0\n# the remaining one looks like it really has 0 for clean cup score\n\n7.08 + 6.83 + 6.25 + 7.42 + 7.25 + 6.75 + 10 + 10  + 6.75 # 68.33\n\n\n[1] 68.33\n\nIt turned out that total cup points is a summation of scores for aroma, flavor, aftertaste, acidity, body, balance, uniformity, clean_cup, sweetness and cupper_points.\n\n\ncountry_mean_score <- sensory_by_country %>% \n  group_by(country_of_origin, owner) %>% \n  summarise(mean_score = mean(total_cup_points)) %>% \n  arrange(desc(mean_score)) \n\ncountry_mean_score\n\n\n# A tibble: 350 x 3\n# Groups:   country_of_origin [36]\n   country_of_origin owner                              mean_score\n   <chr>             <chr>                                   <dbl>\n 1 Ethiopia          metad plc                                89.8\n 2 Guatemala         grounds for health admin                 89.8\n 3 Ethiopia          yidnekachew dabessa                      89  \n 4 Brazil            ji-ae ahn                                88.8\n 5 Peru              hugo valdivia                            88.8\n 6 Ethiopia          diamond enterprise plc                   88.2\n 7 Ethiopia          mohammed lalo                            88.1\n 8 Indonesia         grounds for health admin                 87.4\n 9 United States     cqi q coffee sample representative       87.3\n10 Mexico            roberto licona franco                    87.2\n# … with 340 more rows\n\nmin(country_mean_score$mean_score) # 68.33\n\n\n[1] 68.33\n\nmax(country_mean_score$mean_score) # 89.7767\n\n\n[1] 89.77667\n\nHow do the top 8 coffee owners by country compare against each other in terms of the ten scoring criteria?\n\n\n# plot profile for top 8 owners\n\ntop_owners_data <- sensory_by_country%>% \n  group_by(country_of_origin, owner) %>% \n  summarise_at(.vars = vars(total_cup_points:cupper_points),\n               .funs = c(mean = \"mean\")) %>% \n  ungroup() %>% \n  mutate(country_owner = str_c(country_of_origin, owner, sep = \",\"),\n         country_owner_fct = factor(country_owner, \n                                    levels =c(\"Ethiopia,metad plc\", \n                                             \"Guatemala,grounds for health admin\", \n                                             \"Ethiopia,yidnekachew dabessa\",\n                                             \"Brazil,ji-ae ahn\",\n                                             \"Peru,hugo valdivia\",\n                                             \"Ethiopia,diamond enterprise plc\",\n                                             \"Ethiopia,mohammed lalo\",\n                                             \"Indonesia,grounds for health admin\"))) %>% \n  group_by(country_owner_fct) %>% \n  arrange(desc(total_cup_points_mean)) %>% \n  ungroup() %>% \n  slice_max(total_cup_points_mean, n = 8) %>% \n  pivot_longer(cols = c(aroma_mean:cupper_points_mean),\n               names_to = \"parameters\",\n               values_to = \"score\") %>% \n\n  ggplot(aes(x = fct_rev(factor(parameters)), y = score, label = round(score, 1))) +\n  geom_point(stat = \"identity\", aes(col = factor(parameters)), size = 8) +\n  geom_text(col = \"black\", size = 4) +\n  facet_wrap(country_owner_fct~., scales = \"free_y\", ncol = 4) +\n  coord_flip() +\n  theme_few() +\n  theme(legend.position = \"none\")\n  \ntop_owners_data\n\n\n\n\nThe scores for clean_cup, sweetness, uniformity is a perfect 10 for all 8 owners. Slight differences were observed for mean scores for cupper_points, aftertaste and body. These were probably the distinguishing parameters.\nVariety\nThe first few sections above looked mainly at highly scored coffee. Would there be any differenced in scoring profile, if I were to look at different varieties of coffee?\n\n\nvariety_count <- coffee_ratings %>% \n  count(variety) %>% \n  arrange(desc(n)) # 30 observations\n\nhead(variety_count, 8) # NA: 226, Other: 226\n\n\n# A tibble: 8 x 2\n  variety            n\n  <chr>          <int>\n1 Caturra          256\n2 Bourbon          226\n3 <NA>             226\n4 Typica           211\n5 Other            110\n6 Catuai            74\n7 Hawaiian Kona     44\n8 Yellow Bourbon    35\n\ntail(variety_count)\n\n\n# A tibble: 6 x 2\n  variety                 n\n  <chr>               <int>\n1 Ethiopian Heirlooms     1\n2 Marigojipe              1\n3 Moka Peaberry           1\n4 Pache Comun             1\n5 Sulawesi                1\n6 Sumatra Lintong         1\n\ndata_variety <- coffee_ratings %>% \n  select(total_cup_points, species, owner, country_of_origin, processing_method,\n         variety, aroma:cupper_points, color) %>% \n  filter(variety %in% c(\"Caturra\", \"Bourbon\", \"Typica\", \"Catuai\", \n                        \"Hawaiian Kona\", \"Yellow Bourbon\")) %>% \n  group_by(variety)\n\nglimpse(data_variety)\n\n\nRows: 846\nColumns: 17\nGroups: variety [6]\n$ total_cup_points  <dbl> 89.75, 87.17, 86.92, 86.67, 86.42, 86.33,…\n$ species           <chr> \"Arabica\", \"Arabica\", \"Arabica\", \"Arabica…\n$ owner             <chr> \"grounds for health admin\", \"the coffee s…\n$ country_of_origin <chr> \"Guatemala\", \"Costa Rica\", \"Brazil\", \"Hon…\n$ processing_method <chr> NA, \"Washed / Wet\", \"Natural / Dry\", NA, …\n$ variety           <chr> \"Bourbon\", \"Caturra\", \"Bourbon\", \"Caturra…\n$ aroma             <dbl> 8.42, 8.08, 8.50, 8.17, 8.50, 8.17, 8.08,…\n$ flavor            <dbl> 8.50, 8.25, 8.50, 8.08, 8.17, 7.83, 8.17,…\n$ aftertaste        <dbl> 8.42, 8.00, 8.00, 8.08, 8.00, 8.00, 8.00,…\n$ acidity           <dbl> 8.42, 8.17, 8.00, 8.00, 7.75, 8.08, 7.92,…\n$ body              <dbl> 8.33, 8.00, 8.00, 8.08, 8.00, 7.83, 7.92,…\n$ balance           <dbl> 8.42, 8.33, 8.00, 8.00, 8.00, 8.00, 7.83,…\n$ uniformity        <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ clean_cup         <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ sweetness         <dbl> 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 1…\n$ cupper_points     <dbl> 9.25, 8.33, 7.92, 8.25, 8.00, 8.42, 8.33,…\n$ color             <chr> NA, \"Green\", \"Green\", \"Green\", \"Green\", \"…\n\n# Top 6 coffee by number of datapoints\n\ndata_variety %>% \n  count(species) # all arabica\n\n\n# A tibble: 6 x 3\n# Groups:   variety [6]\n  variety        species     n\n  <chr>          <chr>   <int>\n1 Bourbon        Arabica   226\n2 Catuai         Arabica    74\n3 Caturra        Arabica   256\n4 Hawaiian Kona  Arabica    44\n5 Typica         Arabica   211\n6 Yellow Bourbon Arabica    35\n\ndata_variety %>% \n  count(processing_method) # quite varied\n\n\n# A tibble: 30 x 3\n# Groups:   variety [6]\n   variety processing_method             n\n   <chr>   <chr>                     <int>\n 1 Bourbon Natural / Dry                38\n 2 Bourbon Other                         2\n 3 Bourbon Pulped natural / honey        2\n 4 Bourbon Semi-washed / Semi-pulped    11\n 5 Bourbon Washed / Wet                170\n 6 Bourbon <NA>                          3\n 7 Catuai  Natural / Dry                18\n 8 Catuai  Pulped natural / honey        2\n 9 Catuai  Semi-washed / Semi-pulped     6\n10 Catuai  Washed / Wet                 48\n# … with 20 more rows\n\ndata_variety %>% \n  ungroup() %>% \n  count(country_of_origin) %>% \n  arrange(desc(n))\n\n\n# A tibble: 25 x 2\n   country_of_origin          n\n   <chr>                  <int>\n 1 Mexico                   195\n 2 Guatemala                157\n 3 Colombia                 132\n 4 Brazil                    92\n 5 Taiwan                    66\n 6 Costa Rica                44\n 7 Honduras                  44\n 8 United States (Hawaii)    44\n 9 El Salvador               13\n10 Nicaragua                 13\n# … with 15 more rows\n\n\n\ndata_variety %>% \n  ungroup() %>% \n  group_by(variety) %>% \n  skim()\n\n\nTable 3: Data summary\nName\nPiped data\nNumber of rows\n846\nNumber of columns\n17\n_______________________\n\nColumn type frequency:\n\ncharacter\n5\nnumeric\n11\n________________________\n\nGroup variables\nvariety\nVariable type: character\nskim_variable\nvariety\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\nspecies\nBourbon\n0\n1.00\n7\n7\n0\n1\n0\nspecies\nCatuai\n0\n1.00\n7\n7\n0\n1\n0\nspecies\nCaturra\n0\n1.00\n7\n7\n0\n1\n0\nspecies\nHawaiian Kona\n0\n1.00\n7\n7\n0\n1\n0\nspecies\nTypica\n0\n1.00\n7\n7\n0\n1\n0\nspecies\nYellow Bourbon\n0\n1.00\n7\n7\n0\n1\n0\nowner\nBourbon\n0\n1.00\n4\n50\n0\n44\n0\nowner\nCatuai\n2\n0.97\n5\n41\n0\n29\n0\nowner\nCaturra\n5\n0.98\n4\n45\n0\n45\n0\nowner\nHawaiian Kona\n0\n1.00\n15\n32\n0\n2\n0\nowner\nTypica\n0\n1.00\n8\n47\n0\n83\n0\nowner\nYellow Bourbon\n0\n1.00\n8\n25\n0\n6\n0\ncountry_of_origin\nBourbon\n0\n1.00\n6\n28\n0\n11\n0\ncountry_of_origin\nCatuai\n0\n1.00\n6\n10\n0\n8\n0\ncountry_of_origin\nCaturra\n0\n1.00\n4\n10\n0\n13\n0\ncountry_of_origin\nHawaiian Kona\n0\n1.00\n22\n22\n0\n1\n0\ncountry_of_origin\nTypica\n0\n1.00\n4\n11\n0\n9\n0\ncountry_of_origin\nYellow Bourbon\n0\n1.00\n6\n6\n0\n2\n0\nprocessing_method\nBourbon\n3\n0.99\n5\n25\n0\n5\n0\nprocessing_method\nCatuai\n0\n1.00\n12\n25\n0\n4\n0\nprocessing_method\nCaturra\n7\n0.97\n5\n25\n0\n5\n0\nprocessing_method\nHawaiian Kona\n0\n1.00\n12\n13\n0\n2\n0\nprocessing_method\nTypica\n3\n0.99\n5\n25\n0\n5\n0\nprocessing_method\nYellow Bourbon\n3\n0.91\n5\n25\n0\n5\n0\ncolor\nBourbon\n12\n0.95\n4\n12\n0\n4\n0\ncolor\nCatuai\n2\n0.97\n4\n12\n0\n4\n0\ncolor\nCaturra\n21\n0.92\n4\n12\n0\n4\n0\ncolor\nHawaiian Kona\n6\n0.86\n5\n12\n0\n3\n0\ncolor\nTypica\n27\n0.87\n4\n12\n0\n4\n0\ncolor\nYellow Bourbon\n3\n0.91\n4\n12\n0\n4\n0\nVariable type: numeric\nskim_variable\nvariety\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\ntotal_cup_points\nBourbon\n0\n1\n81.95\n2.54\n68.33\n81.00\n82.33\n83.40\n89.75\n▁▁▂▇▁\ntotal_cup_points\nCatuai\n0\n1\n81.30\n3.91\n59.83\n81.17\n81.88\n83.06\n85.83\n▁▁▁▁▇\ntotal_cup_points\nCaturra\n0\n1\n82.44\n5.59\n0.00\n82.00\n83.12\n83.77\n87.17\n▁▁▁▁▇\ntotal_cup_points\nHawaiian Kona\n0\n1\n81.58\n3.10\n72.58\n80.31\n82.62\n83.33\n86.25\n▁▁▃▇▃\ntotal_cup_points\nTypica\n0\n1\n81.02\n2.59\n67.92\n79.79\n81.50\n82.67\n85.33\n▁▁▁▇▇\ntotal_cup_points\nYellow Bourbon\n0\n1\n82.43\n1.58\n78.00\n81.54\n82.42\n83.16\n86.17\n▁▃▇▅▁\naroma\nBourbon\n0\n1\n7.56\n0.32\n6.17\n7.42\n7.58\n7.67\n8.50\n▁▁▆▇▁\naroma\nCatuai\n0\n1\n7.49\n0.30\n6.67\n7.33\n7.50\n7.67\n8.50\n▁▃▇▂▁\naroma\nCaturra\n0\n1\n7.58\n0.56\n0.00\n7.50\n7.67\n7.75\n8.25\n▁▁▁▁▇\naroma\nHawaiian Kona\n0\n1\n7.51\n0.24\n6.92\n7.33\n7.50\n7.67\n8.08\n▁▅▇▃▂\naroma\nTypica\n0\n1\n7.47\n0.28\n6.67\n7.25\n7.50\n7.67\n8.17\n▁▅▇▇▂\naroma\nYellow Bourbon\n0\n1\n7.50\n0.33\n6.92\n7.25\n7.42\n7.62\n8.42\n▂▇▃▂▂\nflavor\nBourbon\n0\n1\n7.50\n0.36\n6.08\n7.33\n7.50\n7.67\n8.50\n▁▁▇▇▁\nflavor\nCatuai\n0\n1\n7.43\n0.34\n6.17\n7.33\n7.50\n7.58\n8.00\n▁▁▂▇▃\nflavor\nCaturra\n0\n1\n7.53\n0.54\n0.00\n7.42\n7.58\n7.75\n8.33\n▁▁▁▁▇\nflavor\nHawaiian Kona\n0\n1\n7.53\n0.29\n6.92\n7.33\n7.50\n7.75\n8.17\n▃▆▆▇▁\nflavor\nTypica\n0\n1\n7.38\n0.34\n6.33\n7.17\n7.42\n7.58\n8.17\n▁▃▇▇▂\nflavor\nYellow Bourbon\n0\n1\n7.54\n0.24\n7.00\n7.38\n7.58\n7.67\n8.00\n▂▃▇▃▃\naftertaste\nBourbon\n0\n1\n7.32\n0.36\n6.17\n7.17\n7.33\n7.50\n8.42\n▁▂▇▂▁\naftertaste\nCatuai\n0\n1\n7.31\n0.35\n6.17\n7.17\n7.33\n7.50\n8.00\n▁▁▅▇▂\naftertaste\nCaturra\n0\n1\n7.44\n0.54\n0.00\n7.33\n7.50\n7.67\n8.08\n▁▁▁▁▇\naftertaste\nHawaiian Kona\n0\n1\n7.47\n0.31\n6.83\n7.33\n7.50\n7.69\n8.00\n▃▂▇▇▅\naftertaste\nTypica\n0\n1\n7.28\n0.33\n6.17\n7.08\n7.33\n7.50\n8.00\n▁▂▇▇▃\naftertaste\nYellow Bourbon\n0\n1\n7.40\n0.27\n6.83\n7.25\n7.42\n7.58\n8.00\n▂▃▇▅▁\nacidity\nBourbon\n0\n1\n7.55\n0.27\n6.83\n7.42\n7.54\n7.67\n8.42\n▁▅▇▂▁\nacidity\nCatuai\n0\n1\n7.49\n0.32\n6.50\n7.33\n7.50\n7.67\n8.33\n▁▂▇▃▁\nacidity\nCaturra\n0\n1\n7.52\n0.57\n0.00\n7.33\n7.58\n7.75\n8.25\n▁▁▁▁▇\nacidity\nHawaiian Kona\n0\n1\n7.59\n0.27\n6.92\n7.40\n7.58\n7.83\n8.00\n▁▅▂▇▇\nacidity\nTypica\n0\n1\n7.40\n0.27\n6.67\n7.25\n7.42\n7.58\n8.33\n▂▇▇▃▁\nacidity\nYellow Bourbon\n0\n1\n7.47\n0.23\n6.92\n7.33\n7.50\n7.67\n8.00\n▂▇▆▇▁\nbody\nBourbon\n0\n1\n7.50\n0.27\n6.33\n7.33\n7.50\n7.67\n8.33\n▁▁▇▆▁\nbody\nCatuai\n0\n1\n7.41\n0.28\n6.50\n7.27\n7.42\n7.58\n7.92\n▁▂▇▇▅\nbody\nCaturra\n0\n1\n7.54\n0.54\n0.00\n7.48\n7.58\n7.75\n8.17\n▁▁▁▁▇\nbody\nHawaiian Kona\n0\n1\n7.61\n0.26\n6.92\n7.42\n7.67\n7.83\n8.08\n▁▂▇▇▅\nbody\nTypica\n0\n1\n7.40\n0.25\n6.75\n7.25\n7.42\n7.50\n8.33\n▁▇▇▂▁\nbody\nYellow Bourbon\n0\n1\n7.57\n0.27\n6.92\n7.42\n7.50\n7.71\n8.33\n▁▅▇▁▁\nbalance\nBourbon\n0\n1\n7.47\n0.32\n6.50\n7.33\n7.50\n7.67\n8.42\n▁▃▇▅▁\nbalance\nCatuai\n0\n1\n7.40\n0.37\n6.17\n7.25\n7.42\n7.67\n8.00\n▁▁▃▇▆\nbalance\nCaturra\n0\n1\n7.57\n0.58\n0.00\n7.42\n7.58\n7.75\n8.58\n▁▁▁▁▇\nbalance\nHawaiian Kona\n0\n1\n7.64\n0.34\n6.83\n7.42\n7.67\n7.92\n8.25\n▁▃▇▅▅\nbalance\nTypica\n0\n1\n7.35\n0.31\n6.58\n7.17\n7.33\n7.50\n8.25\n▁▃▇▂▁\nbalance\nYellow Bourbon\n0\n1\n7.57\n0.24\n7.17\n7.42\n7.50\n7.67\n8.17\n▅▇▇▃▂\nuniformity\nBourbon\n0\n1\n9.87\n0.39\n8.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nuniformity\nCatuai\n0\n1\n9.85\n0.51\n8.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nuniformity\nCaturra\n0\n1\n9.89\n0.72\n0.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nuniformity\nHawaiian Kona\n0\n1\n9.47\n0.81\n6.67\n9.33\n10.00\n10.00\n10.00\n▁▁▁▅▇\nuniformity\nTypica\n0\n1\n9.75\n0.60\n6.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nuniformity\nYellow Bourbon\n0\n1\n9.83\n0.59\n6.67\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\nBourbon\n0\n1\n9.85\n0.80\n0.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\nCatuai\n0\n1\n9.77\n1.10\n1.33\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\nCaturra\n0\n1\n9.89\n0.79\n0.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\nHawaiian Kona\n0\n1\n9.53\n0.94\n6.67\n9.33\n10.00\n10.00\n10.00\n▁▁▁▂▇\nclean_cup\nTypica\n0\n1\n9.76\n0.89\n2.67\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nclean_cup\nYellow Bourbon\n0\n1\n9.96\n0.16\n9.33\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\nBourbon\n0\n1\n9.91\n0.35\n6.67\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\nCatuai\n0\n1\n9.75\n1.13\n1.33\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\nCaturra\n0\n1\n9.92\n0.71\n0.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\nHawaiian Kona\n0\n1\n9.67\n0.75\n6.67\n9.33\n10.00\n10.00\n10.00\n▁▁▁▂▇\nsweetness\nTypica\n0\n1\n9.94\n0.36\n6.00\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\nsweetness\nYellow Bourbon\n0\n1\n9.98\n0.11\n9.33\n10.00\n10.00\n10.00\n10.00\n▁▁▁▁▇\ncupper_points\nBourbon\n0\n1\n7.43\n0.41\n6.00\n7.25\n7.50\n7.67\n9.25\n▁▃▇▁▁\ncupper_points\nCatuai\n0\n1\n7.41\n0.36\n6.33\n7.27\n7.42\n7.58\n8.17\n▁▁▇▆▂\ncupper_points\nCaturra\n0\n1\n7.55\n0.57\n0.00\n7.42\n7.58\n7.75\n8.50\n▁▁▁▁▇\ncupper_points\nHawaiian Kona\n0\n1\n7.56\n0.35\n6.92\n7.33\n7.50\n7.83\n8.33\n▅▇▇▆▂\ncupper_points\nTypica\n0\n1\n7.30\n0.39\n5.25\n7.00\n7.33\n7.58\n8.17\n▁▁▃▇▃\ncupper_points\nYellow Bourbon\n0\n1\n7.60\n0.31\n7.00\n7.38\n7.58\n7.75\n8.25\n▅▇▇▃▃\n\n\n\ndata_variety %>% \n  select(variety, total_cup_points) %>% \n  filter(total_cup_points != 0) %>% \n  ggplot(aes(fct_reorder(variety, total_cup_points), total_cup_points)) +\n  geom_boxplot(aes(col = variety), show.legend = F) +\n  labs(title = \"Comparison of Total Cup Points across top 6 varieties \\n(by count)\",\n       subtitle = \"Caturra has the higest mean Total Cup Score. Catuai had a wider distribution of scores.\",\n       x = NULL,\n       y = \"Total Cup Points\",\n       caption = \"Source: Coffee Quality Institute\") +\n  geom_jitter(aes(col = variety), alpha = 0.2, show.legend = F) +\n  scale_color_few() +\n  coord_flip()  +\n  theme_few()\n\n\n\n\n\n\ndot_plot_variety <- data_variety %>% \n  filter(total_cup_points != 0) %>% \n  select(variety, aroma:cupper_points) %>% \n  group_by(variety) %>% \n  summarise(across(c(aroma:cupper_points), mean)) %>% \n  pivot_longer(cols = c(aroma:cupper_points),\n               names_to = \"parameters\",\n               values_to = \"score\") %>% \n  ggplot(aes(x = fct_reorder(factor(variety), score), y = score, label = round(score, 1))) +\n  geom_point(stat = \"identity\", aes(col = factor(variety)), size = 8) +\n  geom_text(col = \"black\", size = 4) +\n  facet_wrap(parameters~., scales = \"free\", ncol = 4) +\n  labs(title = \"Breakdown of scoring criteria for top 5 coffee (by count)\",\n       subtitle = \"Scores were quite close for all categories, within +/- 0.2. \nMain areas of differences were in balance, clean_cup, cupper_points, sweetness, uniformity\",\n       caption = \"Source: Coffee Quality Institute\",\n       x = \"Variety\",\n       y = \"Score\") +\n  coord_flip() +\n  theme_few() +\n  theme(legend.position = \"none\",\n        axis.title = element_text(face = \"bold\"))\n\ndot_plot_variety\n\n\n\n\nCanturra had an edge over Hawaiian Kona for aroma, clean_cup, sweetness and uniformity, resulting in higher mean total_cup_points. What is Canturra coffee? It’s actually a mutated type of Bourbon coffee that is known for great flavor.\nAs mentioned at the beginning, most of the coffee had very high scores in this dataset. Hence, the plots only show a snapshot of the flavor profiles of the scored coffee, but not all the coffee.\nMain Learning Pointers\nI am really glad to have found this #tidytuesday hashtag, which allows me to practice on readily available datasets and understand how different people in the community approach exploratory data analysis! I am really amazed that there is a dedicated package for loading the dataset with convenience, and this dataset even comes with a data dictionary to understand what each variable means. The R community is really committed to sharing and becoming better, together.\nThe process of EDA is about getting to know your dataset, through asking questions, which are to be answered by carrying out data transformations and creating data visualizations. One question often leads to another, and EDA is a repetitive process until you finish getting to know your data. There were several aspects that I did not look at, such as the effect of altitude, and the grading dates. I may have concentrated too much on the sensory aspect of coffee since that was the more familiar aspect to me, and should have also looked at geographical region and coffee varieties. As an initial learning exercise, I sharpened my focus and concentrated on the effect of species, variety, processing methods, country/owners.\nAs the total cup points is a summation of the scores for attributes such as aroma, flavor, etc, I think it is hard to do classification based on these scores. I would prefer to have physicochemical data as well so that differentiation is more objective and to better countercheck the sensorial data. However, this may be a personal bias as I work in the analytical chemistry field. :)\nI think coffee is really complex. You can have a poorer grade (Robusta), but the roasting process plays a very important role in flavor development. You can have a very good variety, but the processing method may spoil/enhance its flavor profile. You can have a very good farm/owner, but maybe the year of harvest was particularly good or bad. Hence, it is really important to consider all (both familar and unfamilar) aspects when carrying out data analysis, and this is one area I need to improve on.\nCoding wise, I got a chance to practice ggplots, data transformation, filtering and selecting rows and columns, as well as calculating means efficiently by using summarise(across, var, mean). I also managed to create new classifications using ifelse, and used fct_reorder to make my plot better. I like to use theme_clean and scale_color_few for my plots, making aesthetically pleasant plots are a breeze as compared to using Microsoft Excel.\nReferences\nhttps://perfectdailygrind.com/2016/07/washed-natural-honey-coffee-processing-101/ https://www.baristainstitute.com/blog/jori-korhonen/january-2020/coffee-processing-methods-drying-washing-or-honey https://www.coffeechemistry.com/cupping-fundamentals https://www.data-to-viz.com/caveat/spider.html https://www.javapresse.com/blogs/buying-coffee/beginners-guide-coffee-varieties\n\n\n\n",
    "preview": "posts/20210209_tidy_tuesday_coffee_ratings/Tidy-Tuesday---Coffee-Ratings_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-02-12T00:21:12+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210202_Clustering wine/",
    "title": "Clustering Analysis on Wine Dataset",
    "description": "A continuation from PCA analysis of wine dataset: k-means clustering and hierarchical clustering",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-02-02",
    "categories": [],
    "contents": "\nSummary\nPCA is used as an exploratory data analysis tool, and may be used for feature engineering and/or clustering. This is a continuation of clustering analysis on the wines dataset in the kohonen package, in which I carry out k-means clustering using the tidymodels framework, as well as hierarchical clustering using factoextra pacage.\nWorkflow\nImport data\nExploratory data analysis\nskim\nggcorr\nggpairs\nCheck assumptions on whether PCA can be carried out\nKMO\nBartlett\nCarry out PCA using tidymodels workflow\nAlways use only continuous variables, ensure that there are no missing data. Determine the number of components using eigenvalues, scree plots and parallel analysis.\nrecipe : preprocess the data (missing values, center and scale, ensuring that variables are continuous)\nprep : evaluate the data\nbake : get the PCA Scores results\nvisualize\ncommunicate results: show the scree plot, PCA loadings, variance explained by each component, loadings and score plot.\nThe loading shows the linear combinations of the original variables - ie the new dimension.\nThe scores show the coordinates of the individual wine samples in the new low-dimensional space.\nUse the loadings to carry out k-means clustering and hierarchical clustering.\nLoading packages\n\n\nlibrary(pacman)\np_load(corrr, GGally, tidymodels, tidytext, tidyverse, psych,\n       skimr, gridExtra, kohonen, janitor, learntidymodels, kohonen,\n       cluster, factoextra)\n\n\n\nImport\nThis dataset is from the kohonen package. It contains 177 rows and 13 columns.\nThese data are the results of chemical analyses of wines grown in the same region in Italy (Piedmont) but derived from three different cultivars: Nebbiolo, Barberas and Grignolino grapes. The wine from the Nebbiolo grape is called Barolo. The data contain the quantities of several constituents found in each of the three types of wines, as well as some spectroscopic variables.\nPCA analysis was performed earlier, and k-means clustering and hierarchical clustering analysis (HCA) will be built upon the PCA loadings.\n\n\ndata(wines)\n\nwines <- as.data.frame(wines) %>% \n  janitor::clean_names() %>%  # require data.frame\n  as_tibble() \n \nglimpse(wines) # does not contain the types of wine (Y variable)\n\n\nRows: 177\nColumns: 13\n$ alcohol          <dbl> 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, …\n$ malic_acid       <dbl> 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, …\n$ ash              <dbl> 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, …\n$ ash_alkalinity   <dbl> 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, …\n$ magnesium        <dbl> 100, 101, 113, 118, 112, 96, 121, 97, 98, …\n$ tot_phenols      <dbl> 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, …\n$ flavonoids       <dbl> 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, …\n$ non_flav_phenols <dbl> 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, …\n$ proanth          <dbl> 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, …\n$ col_int          <dbl> 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, …\n$ col_hue          <dbl> 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, …\n$ od_ratio         <dbl> 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, …\n$ proline          <dbl> 1050, 1185, 1480, 735, 1450, 1290, 1295, 1…\n\nEDA\nRefer to the post for PCA of wine analysis\nTidymodels (PCA)\nRecipe\nThe dataset did not include the y variable (type of wine), so the update_role() function will be omitted.\nstep_normalize() combines step_center() and step_scale()\nNote that step_pca is the second step –> will need to retrieve the PCA results from the second list later.\n\n\nwines_recipe <- recipe(~ ., data = wines) %>% \n  # update_role(vintages, new_role = \"id\") %>%  # skipped\n  # step_naomit(all_predictors()) %>% \n  step_normalize(all_predictors()) %>% \n  step_pca(all_predictors(), id = \"pca\")\n\nwines_recipe # 13 predictors\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n predictor         13\n\nOperations:\n\nCentering and scaling for all_predictors()\nNo PCA components were extracted.\n\nPreparation\n\n\nwines_prep <- prep(wines_recipe)\n\nwines_prep # trained\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n predictor         13\n\nTraining data contained 177 data points and no missing data.\n\nOperations:\n\nCentering and scaling for alcohol, malic_acid, ... [trained]\nPCA extraction with alcohol, malic_acid, ... [trained]\n\ntidy_pca_loadings <- wines_prep %>% \n  tidy(id = \"pca\")\n\ntidy_pca_loadings # values here are the loading\n\n\n# A tibble: 169 x 4\n   terms               value component id   \n   <chr>               <dbl> <chr>     <chr>\n 1 alcohol          -0.138   PC1       pca  \n 2 malic_acid        0.246   PC1       pca  \n 3 ash               0.00432 PC1       pca  \n 4 ash_alkalinity    0.237   PC1       pca  \n 5 magnesium        -0.135   PC1       pca  \n 6 tot_phenols      -0.396   PC1       pca  \n 7 flavonoids       -0.424   PC1       pca  \n 8 non_flav_phenols  0.299   PC1       pca  \n 9 proanth          -0.313   PC1       pca  \n10 col_int           0.0933  PC1       pca  \n# … with 159 more rows\n\nBake\n\n\nwines_bake <- bake(wines_prep, wines)\nwines_bake  # has the PCA SCORES to run HCA and k-means clustering\n\n\n# A tibble: 177 x 5\n     PC1    PC2    PC3      PC4     PC5\n   <dbl>  <dbl>  <dbl>    <dbl>   <dbl>\n 1 -2.22 -0.301 -2.03   0.281   -0.259 \n 2 -2.52  1.06   0.974 -0.734   -0.198 \n 3 -3.74  2.80  -0.180 -0.575   -0.257 \n 4 -1.02  0.886  2.02   0.432    0.274 \n 5 -3.04  2.16  -0.637  0.486   -0.630 \n 6 -2.45  1.20  -0.985  0.00466 -1.03  \n 7 -2.06  1.64   0.143  1.20     0.0105\n 8 -2.51  0.958 -1.78  -0.104   -0.871 \n 9 -2.76  0.822 -0.986 -0.374   -0.437 \n10 -3.48  1.35  -0.428 -0.0399  -0.316 \n# … with 167 more rows\n\nCheck number of PC\nOnly the scree plot is showed below. Refer to PCA analysis of wine for other options in determining number of PCs.\n\n\n# b. Scree plot/Variance plot\n\nwines_prep %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms ==  \"percent variance\") %>% \n  ggplot(aes(x = component, y = value)) +\n  geom_point(size = 2) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = 1:13) +\n  labs(title = \"% Variance explained\",\n       y = \"% total variance\",\n       x = \"PC\",\n       caption = \"Source: Wines dataset from kohonen package\") +\n  theme_classic() +\n  theme(axis.title = element_text(face = \"bold\", size = 12),\n        axis.text = element_text(size = 10),\n        plot.title = element_text(size = 14, face = \"bold\"))  # 2 or 3\n\n\n\n\nVisualize\nLoadings plot\n\n\nplot_loadings <- tidy_pca_loadings %>% \n  filter(component %in% c(\"PC1\", \"PC2\", \"PC3\")) %>% \n  mutate(terms = tidytext::reorder_within(terms, \n                                          abs(value), \n                                          component)) %>% \n  ggplot(aes(abs(value), terms, fill = value>0)) +\n  geom_col() +\n  facet_wrap( ~component, scales = \"free_y\") +\n  scale_y_reordered() + # appends ___ and then the facet at the end of each string\n  scale_fill_manual(values = c(\"deepskyblue4\", \"darkorange\")) +\n  labs( x = \"absolute value of contribution\",\n        y = NULL,\n        fill = \"Positive?\",\n        title = \"PCA Loadings Plot\",\n        subtitle = \"Number of PC should be 3, compare the pos and the neg\",\n        caption = \"Source: ChemometricswithR\") +\n  theme_minimal() +\n  theme(title = element_text(size = 24, face = \"bold\"),\n        axis.text = element_text(size = 16),\n        axis.title = element_text(size = 20))\n\n\nplot_loadings\n\n\n\n# PC1: flavonoids, tot_phenols, od_ratio, proanthocyanidins, col_hue, 36%\n# PC2: col_int, alcohol, proline, ash, magnesium; 19.2%\n# PC3: ash, ash_alkalinity, non_flav phenols; 11.2%\n\n\n\nLoadings only\n\n\n# define arrow style\narrow_style <- arrow(angle = 30,\n                     length = unit(0.2, \"inches\"),\n                     type = \"closed\")\n\n# get pca loadings into wider format\npca_loadings_wider <- tidy_pca_loadings%>% \n  pivot_wider(names_from = component, id_cols = terms)\n\n\npca_loadings_only <- pca_loadings_wider %>% \n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_segment(aes(xend = PC1, yend = PC2),\n               x = 0, \n               y = 0,\n               arrow = arrow_style) +\n  ggrepel::geom_text_repel(aes(x = PC1, y = PC2, label = terms),\n            hjust = 0, \n            vjust = 1,\n            size = 5,\n            color = \"red\") +\n  labs(title = \"Loadings on PCs 1 and 2 for normalized data\") +\n  theme_classic()\n\n\n\nScores plot\n\n\n# Scores plot #####\n# PCA SCORES are in bake\npc1pc2_scores_plot <- wines_bake %>% \n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(color = vintages, shape = vintages), \n             alpha = 0.8, size = 2) +\n  scale_color_manual(values = c(\"deepskyblue4\", \"darkorange\", \"purple\")) +\n  labs(title = \"Scores on PCs 1 and 2 for normalized data\",\n       x = \"PC1 (36%)\",\n       y = \"PC2 (19.2%)\") +\n  theme_classic() +\n  theme(legend.position = \"none\") \n\n\n\nFinalised plots\n\n\ngrid.arrange(pc1pc2_scores_plot, pca_loadings_only, ncol = 2)\n\n\n\n\nk-means clustering\nThe PCA scores will be used for clustering analysis\n\n\nwines_bake\n\n\n# A tibble: 177 x 5\n     PC1    PC2    PC3      PC4     PC5\n   <dbl>  <dbl>  <dbl>    <dbl>   <dbl>\n 1 -2.22 -0.301 -2.03   0.281   -0.259 \n 2 -2.52  1.06   0.974 -0.734   -0.198 \n 3 -3.74  2.80  -0.180 -0.575   -0.257 \n 4 -1.02  0.886  2.02   0.432    0.274 \n 5 -3.04  2.16  -0.637  0.486   -0.630 \n 6 -2.45  1.20  -0.985  0.00466 -1.03  \n 7 -2.06  1.64   0.143  1.20     0.0105\n 8 -2.51  0.958 -1.78  -0.104   -0.871 \n 9 -2.76  0.822 -0.986 -0.374   -0.437 \n10 -3.48  1.35  -0.428 -0.0399  -0.316 \n# … with 167 more rows\n\nNumber of clusters?\nThere are 3 common ways for determining the number of clusters:\ngap statistic method\nwithin sum of square method\nsilhouette method\nLet us look at all three of them.\nGap Statistic Method\n\n\ngap_statistic <- cluster::clusGap(wines_bake,\n                                  FUN = kmeans,\n                                  nstart = 50, \n                                  K.max = 10, # max number of clusters\n                                  B = 1000) # bootstrap\n\nfactoextra::fviz_gap_stat(gap_statistic) # theoretically should have only 3 clusters\n\n\n\n\nWithin Sum of Square Method\n\n\nfviz_nbclust(wines_bake,\n             kmeans,\n             method = \"wss\") # this suggests 3 clusters, in line with theory\n\n\n\n\nSilhouette Method\n\n\nfviz_nbclust(wines_bake,\n             FUN = hcut,\n             method = \"silhouette\") # this suggests 3 clusters\n\n\n\n\nAll three methods agree that there should be 3 clusters. This may not always be the case. In any case, we know that there are 3 different types of wine in the dataset.\nTidymodels workflow for k-means clustering\n\n\n# exploring different k numbers #####\nkclusts_explore <- tibble(k = 1:10) %>% \n  mutate(kclust = purrr::map(k, ~kmeans(wines_bake, .x)),\n         tidied = purrr::map(kclust, tidy),\n         glanced = purrr::map(kclust, glance),\n         augmented = purrr::map(kclust, augment, wines_bake))\n\nkclusts_explore\n\n\n# A tibble: 10 x 5\n       k kclust   tidied            glanced          augmented        \n   <int> <list>   <list>            <list>           <list>           \n 1     1 <kmeans> <tibble [1 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 2     2 <kmeans> <tibble [2 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 3     3 <kmeans> <tibble [3 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 4     4 <kmeans> <tibble [4 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 5     5 <kmeans> <tibble [5 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 6     6 <kmeans> <tibble [6 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 7     7 <kmeans> <tibble [7 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 8     8 <kmeans> <tibble [8 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n 9     9 <kmeans> <tibble [9 × 8]>  <tibble [1 × 4]> <tibble [177 × 6…\n10    10 <kmeans> <tibble [10 × 8]> <tibble [1 × 4]> <tibble [177 × 6…\n\n# turn this into 3 separate datasets, each representing a\n# different type of data\n\n#\nclusters <- kclusts_explore %>% \n  unnest(cols = c(tidied))\n\nclusters\n\n\n# A tibble: 55 x 12\n       k kclust       PC1       PC2       PC3       PC4       PC5\n   <int> <list>     <dbl>     <dbl>     <dbl>     <dbl>     <dbl>\n 1     1 <kmea… -1.03e-15 -2.78e-16 -3.22e-16 -5.39e-17  1.76e-16\n 2     2 <kmea…  1.91e+ 0 -8.65e- 2 -1.73e- 3  7.28e- 2 -1.41e- 2\n 3     2 <kmea… -1.89e+ 0  8.56e- 2  1.71e- 3 -7.20e- 2  1.39e- 2\n 4     3 <kmea…  2.71e+ 0  1.10e+ 0 -2.35e- 1 -6.17e- 2  7.64e- 2\n 5     3 <kmea…  4.43e- 4 -1.76e+ 0  1.85e- 1 -7.36e- 2  7.54e- 2\n 6     3 <kmea… -2.26e+ 0  9.55e- 1 -5.83e- 4  1.30e- 1 -1.44e- 1\n 7     4 <kmea…  2.90e- 1 -1.77e+ 0 -8.54e- 1  4.55e- 1  1.35e- 1\n 8     4 <kmea…  2.76e+ 0  1.21e+ 0 -1.52e- 1 -1.11e- 1  5.10e- 2\n 9     4 <kmea… -2.39e+ 0  1.04e+ 0 -2.57e- 1  1.29e- 1 -2.19e- 1\n10     4 <kmea… -3.41e- 1 -1.30e+ 0  1.28e+ 0 -4.39e- 1  1.17e- 1\n# … with 45 more rows, and 5 more variables: size <int>,\n#   withinss <dbl>, cluster <fct>, glanced <list>, augmented <list>\n\n#\nassignments <- kclusts_explore %>% \n  unnest(cols = c(augmented))\n\nassignments  # can be used to plot, with each point colored according to predicted cluster\n\n\n# A tibble: 1,770 x 10\n       k kclust tidied glanced   PC1    PC2    PC3      PC4     PC5\n   <int> <list> <list> <list>  <dbl>  <dbl>  <dbl>    <dbl>   <dbl>\n 1     1 <kmea… <tibb… <tibbl… -2.22 -0.301 -2.03   0.281   -0.259 \n 2     1 <kmea… <tibb… <tibbl… -2.52  1.06   0.974 -0.734   -0.198 \n 3     1 <kmea… <tibb… <tibbl… -3.74  2.80  -0.180 -0.575   -0.257 \n 4     1 <kmea… <tibb… <tibbl… -1.02  0.886  2.02   0.432    0.274 \n 5     1 <kmea… <tibb… <tibbl… -3.04  2.16  -0.637  0.486   -0.630 \n 6     1 <kmea… <tibb… <tibbl… -2.45  1.20  -0.985  0.00466 -1.03  \n 7     1 <kmea… <tibb… <tibbl… -2.06  1.64   0.143  1.20     0.0105\n 8     1 <kmea… <tibb… <tibbl… -2.51  0.958 -1.78  -0.104   -0.871 \n 9     1 <kmea… <tibb… <tibbl… -2.76  0.822 -0.986 -0.374   -0.437 \n10     1 <kmea… <tibb… <tibbl… -3.48  1.35  -0.428 -0.0399  -0.316 \n# … with 1,760 more rows, and 1 more variable: .cluster <fct>\n\n#\nclusterings <- kclusts_explore %>% \n  unnest(cols = c(glanced))\n\nclusterings\n\n\n# A tibble: 10 x 8\n       k kclust  tidied  totss tot.withinss betweenss  iter augmented \n   <int> <list>  <list>  <dbl>        <dbl>     <dbl> <int> <list>    \n 1     1 <kmean… <tibbl… 1834.        1834. -2.50e-12     1 <tibble […\n 2     2 <kmean… <tibbl… 1834.        1192.  6.42e+ 2     1 <tibble […\n 3     3 <kmean… <tibbl… 1834.         820.  1.01e+ 3     2 <tibble […\n 4     4 <kmean… <tibbl… 1834.         730.  1.10e+ 3     3 <tibble […\n 5     5 <kmean… <tibbl… 1834.         659.  1.18e+ 3     3 <tibble […\n 6     6 <kmean… <tibbl… 1834.         603.  1.23e+ 3     4 <tibble […\n 7     7 <kmean… <tibbl… 1834.         562.  1.27e+ 3     3 <tibble […\n 8     8 <kmean… <tibbl… 1834.         545.  1.29e+ 3     5 <tibble […\n 9     9 <kmean… <tibbl… 1834.         471.  1.36e+ 3     3 <tibble […\n10    10 <kmean… <tibbl… 1834.         465.  1.37e+ 3     4 <tibble […\n\n#  visualize\n\n# number of clusters\nclusterings %>% # from glance\n  ggplot(aes(k, tot.withinss)) + # total within cluster sum of squares, keep low\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 1:10) +\n  labs(title = \"Plot of Total Within Sum of Squares for different number of clusters\",\n       subtitle = \"Additional clusters beyond k = 3 have little value\") +\n  theme_classic()\n\n\n\n# how datapoints are separated\nglimpse(assignments)\n\n\nRows: 1,770\nColumns: 10\n$ k        <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ kclust   <list> [<1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n$ tidied   <list> [<tbl_df[1 x 8]>, <tbl_df[1 x 8]>, <tbl_df[1 x 8]…\n$ glanced  <list> [<tbl_df[1 x 4]>, <tbl_df[1 x 4]>, <tbl_df[1 x 4]…\n$ PC1      <dbl> -2.223934, -2.524760, -3.744056, -1.017245, -3.040…\n$ PC2      <dbl> -0.30145757, 1.05925179, 2.79737289, 0.88586726, 2…\n$ PC3      <dbl> -2.0271695, 0.9739613, -0.1798599, 2.0181445, -0.6…\n$ PC4      <dbl> 0.281108579, -0.733645703, -0.575492236, 0.4315681…\n$ PC5      <dbl> -0.25880549, -0.19804048, -0.25714173, 0.27445613,…\n$ .cluster <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…\n\nassignments %>% # from augment\n  ggplot(aes(x = PC1, y = PC2)) + # use PCA data\n  geom_point(aes(color = .cluster), size = 2, alpha = 0.8) +\n  facet_wrap( ~ k) +\n  # to see the center of the clusters\n  geom_point(data = clusters, size = 9, shape  = \"x\") +\n  labs(x = \"PC1 (36% variance)\",\n       y = \"PC2 (19.2% variance\",\n       title = \"Visualization of k-means clustering\",\n       subtitle = \"Optimal k = 3\",\n       caption = \"Source: Wines dataset from kohonen package\") +\n  theme_minimal()\n\n\n\n\nHierarchical Clustering Analysis\n\n\nwines_HC <- wines_bake %>% \n    dist(.,method = \"euclidean\") %>% \n    hclust(., method = \"ward.D2\")\n\n# 3 clusters:\nfviz_dend(wines_HC,\n          k = 3,\n          rect = T,\n          rect_border = \"jco\",\n          rect_fill = T)\n\n\n\n\nLearning pointers:\nInitially, I was stuck at the visualization part for k-means clustering as I didn’t know how to bring in my x and y-axis data. I had been using the original dataset all along, and was wondering why plots created using the factoextra::fviz_cluster() could report Dim 1 for x axis and Dim 2 for y axis. I finally had the eureka moment when I realised I should use the PCA scores from the bake step earlier.\nI really like the tidymodels way of allowing for visualizing how the clusters are separated when different values of k are used. The functions augment, tidy and glance were very efficient in reporting the results for k-means clustering. Previously I only used tidy and glance for regression, and I didn’t know they could be extended to cluster analysis as well.\nLastly, I find dendrograms very aesthetically intuitive and I like how the colors and types of dendrograms could be customised. However, the assumption is that there must be some structure in the data in the first place, otherwise HCA would give very misleading results.\nReferences\nhttps://rdrr.io/github/tidymodels/learntidymodels/f/inst/tutorials/pca_recipes/pca_recipes.Rmd\nhttps://allisonhorst.github.io/palmerpenguins/articles/articles/pca.html\nhttps://www.ibm.com/support/knowledgecenter/en/SSLVMB_subs/statistics_casestudies_project_ddita/spss/tutorials/fac_telco_kmo_01.html\nhttps://www.tidymodels.org/learn/statistics/k-means/\nhttps://www.r-bloggers.com/2019/07/use-the-k-means-clustering-luke/\nhttps://statsandr.com/blog/clustering-analysis-k-means-and-hierarchical-clustering-by-hand-and-in-r/\nhttps://agroninfotech.blogspot.com/2020/06/visualizing-clusters-in-r-hierarchical.html\n\n\n\n",
    "preview": "posts/20210202_Clustering wine/Clustering-wine_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-02-03T00:24:11+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210130_Volatiles_tomato/",
    "title": "Volatile Compounds in Tomato and Tomato Products",
    "description": "Scraping information from journal article",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-02-01",
    "categories": [],
    "contents": "\nOverview of tomato volatiles\nTomato flavor is the result of interaction of aroma and taste, arising from the interplay of mixture of acids, sugars, amino acids, minerals and volatile compounds. Presence of sugar or organic acids alters taste panel perception of aroma descriptions of samples with the same concentration of volatile compounds.\nVolatile compounds may originate from different biosynthesis pathways (Buttery, Teranishi, and Ling 1987), (Baldwin et al. 1998), (Yilmaz 2001):\nDerived from amino acids: Amino acids are acted upon by transaminase enzymes and converted into alpha-keto acids; which then undergo decarboxylation to form aldehydes, which may be reduced to form ketones.\nDerived from carotenoids: eg 6-methyl-5-hepten-2-one and geranial\nDerived from lipid degradation by lipoxygenase: eg C6 volatiles\nDerived from peroxide lyase and alcohol dehydrogenase enzymes (ADH catalysed alcohol formation from aldehydes)\nMaillard reaction products: furans, pyrroles (Strecker degradation products), pyrazines. These are usually seen in thermally processed tomato products/flavors\nDerived from action of endogenous glycosidases (eg guaiacol, eugenol, methyl salicylate)\nThe amount and types of volatiles are also influenced by:\nTissue disruption\nRipening of fruit\nCultivar\nProcessing/Heating\nAddition of other ingredients (eg herbs and spices for pasta sauces)\nAim of this exercise:\nThe aim of this exercise was to scrape the table of approximately 400 compounds from the pdf, and to visualize them by chemical categories.\nThere is a very handy package, tabulizer, which allows for scraping of information from pdf articles. I tried out text scraping, and text cleaning, from the article (Petro‐Turza 1986)\nWorkflow\nImport data using tabulizer\nText cleaning using stringr package\nVisualize using ggplot2\nLoading packages\n\n\nlibrary(rJava)\nlibrary(tabulizer)\nlibrary(tidyverse)\nlibrary(stringr)\nlibrary(stringi)\n\n\n\nImport & Tidy\nI am interested in scraping the table of approximately 400 volatiles from page 18 to page 28. A copy of the pdf is saved in my working directory. I used the locate_area() function to determine the coordinates for scraping, and then extracted the text using extract_text() function.\n\n\nfile <- \"1986-tomato volatiles.pdf\"\n# locate_areas(file)\n\np18 <- extract_text(file, pages = 18, area = list(c(115.5, 33.05, 641.923, 443.94)))\n\np19 <- extract_text(file, pages = 19, \n                    area = list(c(117.15636,  53.20364, 651.60000, 465.22545 )))\n\np20 <- extract_text(file, pages = 20, \n                    area = list(c(114.52364,  38.72364, 650.28364, 449.42909 )))\n\np21 <- extract_text(file, pages = 21, \n                    area = list(c(119.29273 , 48.89727, 656.76545, 457.90091 )))\n\np22 <- extract_text(file, pages = 22, \n                    area = list(c(113.20727,  30.32545, 643.70182, 466.04182)))\n\np23 <- extract_text(file, pages = 23, \n                    area = list(c(122.25273,  49.84636, 652.01455, 473.13000 )))\n\np24 <- extract_text(file, pages = 24, \n                    area = list(c(113.,  34, 645.01818, 430.50000 )))\n\np25 <- extract_text(file, pages = 25, \n                    area = list(c(119,  58 ,642.81273, 478.38818)))\n\np26 <- extract_text(file, pages = 26, \n                    area = list(c(114,  35, 668.17818, 443.44000)))\n\np27 <- extract_text(file, pages = 27, \n                    area = list(c(114.20727,  41.24545, 656.36364, 442.94000)))\n\np28 <- extract_text(file, pages = 28, \n                    area = list(c(115.52000,  33.86909, 404.32000, 469.69455 )))\n\ncombined <- tribble(~page, ~text,\n                    \"p18\", p18,\n                    \"p19\", p19,\n                    \"p20\", p20,\n                    \"p20\", p20,\n                    \"p21\", p21,\n                    \"p22\", p22,\n                    \"p23\", p23,\n                    \"p24\", p24,\n                    \"p25\", p25,\n                    \"p26\", p26,\n                    \"p27\", p27,\n                    \"p28\", p28) %>% \n  dplyr::mutate(text_2 = gsub(\"\\\\n\", \"; \", text),\n         text_3 = str_split(text_2, \"; \")) %>% # split by ; into new columns\n  unnest() \n\n\n\nI combined all the text that was extracted into a tibble. Then I replaced all the “” with “;” and then used str_split() to split the compounds into individual rows.\nFollowing which, I used a series to str_replace_all to clean up the text. The list of things to remove include:\ndigits that followed after chemical names\nseries of commars\ntext that mentioned unknown structure\nalternative synonoyms of chemical compounds that were located within square brackets\nodd chemical names\nAs I replaced the commars, some of the chemical names were also changed. For example, 2,6-dimethylpyrazine became 26-dimethylpyrazine. I had to change the names by looking for numbers 26, and replacing them as 2,6.\nI visually scanned through the list again and made changes where necessary, for eg, 2-formylpyiTole is actually 2-formylpyrrole.\n\n\ncleaned_text <- combined %>% \n  dplyr::select(text_3) # 521\n\n\ncleaned_text_b <- cleaned_text %>% \n  filter(!text_3 %in% c(\"(Continued)\", \"\")) %>%  # 503\n  filter(!is.na(text_3)) %>% \n  mutate(text_4 = str_replace_all(text_3, \"[0-9]{2,3}\", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\ , \", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\,,+\", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\(unknown structure\\\\)\", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\[[^\\\\]\\\\[]*]\", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\[[^\\\\[]*\", \"\"),\n         text_4 = str_replace_all(text_4, \" \\\\,\", \"\"),\n         text_4 = str_replace_all(text_4, \" \\\\/\", \"\"),\n         # replace last commar, dash\n         text_4 = stri_replace_last(text_4, fixed = \",\", \"\"),\n         text_4 = str_replace_all(text_4, \"PJ-dimethyl-ö-octen-l-ol]\", \"\"),\n         text_4 = str_replace_all(text_4, \"/\", \"\"),\n         \n         # correct for commar replacement\n         text_4 = str_replace_all(text_4, \"45\", \"4,5\"),\n         text_4 = str_replace_all(text_4, \"26\", \"2,6\"),\n         text_4 = str_replace_all(text_4, \"25\", \"2,5\"),\n         text_4 = str_replace_all(text_4, \"33\", \"3,3\"),\n         text_4 = str_replace_all(text_4, \"23\", \"2,3\"),\n         text_4 = str_replace_all(text_4, \"14\", \"1,4\"),\n         text_4 = str_replace_all(text_4, \"12\", \"1,2\"),\n         text_4 = str_replace_all(text_4, \"24\", \"2,4\"),\n         text_4 = str_replace_all(text_4, \"34\", \"3,4\"),\n         text_4 = str_replace_all(text_4, \"11\", \"1,1\"),\n         text_4 = str_replace_all(text_4, \"2E 4Z\", \"2E,4Z\"),\n         text_4 = str_replace_all(text_4, \"2E4E\", \"2E,4E\"),\n         text_4 = str_replace_all(text_4, \"2E4Z\", \"2E,4Z\"),\n         text_4 = str_replace_all(text_4, \"hy droxy\", \"hydroxy\"),\n         text_4 = str_replace_all(text_4, \"66\", \"6,6\"),\n         text_4 = str_replace_all(text_4, \"3E5E\", \"3E,5E\"),\n         \n         # further clean up\n         text_4 = str_replace_all(text_4, \"\\\\(unknownstructure\\\\)\", \"\"),\n         text_4 = str_replace_all(text_4, \"\\\\(unknown\", \"\"),\n         text_4 = str_replace_all(text_4, \"2-formylpyiTole \", \"2-formylpyrrole\"),\n         text_4 = str_replace_all(text_4, \"neraUcis-SJ-dimethyl\\\\^.o-octadienal]\", \n                                  \"neral\"),\n         text_4 = str_replace_all(text_4, \"2-methyl-l-propanol -\", \n                                  \"2-methyl-l-propanol\"),\n         text_4 = str_replace_all(text_4, \"propanal 9 09 39 49 5\",\n                                  \"propanal\"),\n         text_4 = str_replace_all(text_4, \"\\\\(methylthioH-propanol\",\n                                  \"\\\\(methylthio)-propanol\"),\n         text_4 = str_replace_all(text_4, \"\\\\(2,2,6-trimethyl-7-oxabicyclo\",\n                                  \"\"),\n         text_4 = str_replace_all(text_4, \"._.\", \"\"),\n         \n         # remove white space\n         text_4 = str_replace_all(text_4, \" \\\\s\", \"\"),\n         \n         # remove quotation marks\n         text_4 = str_remove_all(text_4, \"\\\"\")) \n\n\n\nOne final clean:\n\n\ntomato_cleaned <- cleaned_text_b$text_4 %>% \n  as.data.frame() %>% \n  unique()\n\nnames(tomato_cleaned) <- c(\"compounds\")\n\ntomato_cleaned <- print(tomato_cleaned, quote = FALSE) %>% \n  filter(!compounds == \" \",\n         !compounds == \"\") # remove quotation marks \n\n\n                                                 compounds\n1                                             HYDROCARBONS\n2                                                 heptane \n3                                                  octane \n4                                                  nonane \n5                                                  decane \n6                                                 undecane\n7                                             pentadecane \n8                                                ethylene \n9                                                camphene \n10                                                3-carene\n11                                               limonene \n12                                                myrcene \n13                                          a-phellandrene\n14                                          ß-phellandrene\n15                                               o-pinene \n16                                                3-pinene\n17                                               sabinene \n18                                            terpinolene \n19                                          triisobutylene\n20                                                benzene \n21                                                 toluene\n22                                            ethylbenzene\n23                                                 styrene\n24                                          propylbenzene \n25                                                  cumene\n26                                           butylbenzene \n27                                                o-xylene\n28                                               m-xylene \n29                                                p-xylene\n30                                 l-ethyl-4-methylbenzene\n31                                         diethylbenzene \n32                                                  cymene\n33                                               p-cymene \n34                                       trimethylbenzene \n35                                           hemimellitene\n36                                            pseudocumene\n37                                              mesitylene\n38                                                biphenyl\n39                                             naphtalene \n40                                                ALCOHOLS\n41                                               methanol \n42                                                 ethanol\n43                                                        \n44                                             1-propanol \n46                                              2-propanol\n47                                     2-methyl-l-propanol\n48                                                        \n49                                    2-methyl-2-propanol \n50                                           2-propen-l-ol\n51                                               1-butanol\n53                                              2-butanol \n54                                              buten-1-ol\n55                                     2-methyl-l-butanol \n56                                      3-methyl-l-butanol\n58                                   3-methyl-2-buten-l-ol\n59                                  2-methyl-3-buten-2-ol \n60                                         2,3-butanediol \n61                                             1-pentanol \n63                                             2-pentanol \n64                                             3-pentanol \n65                                      cis-3-penten-l-ol \n66                                          l-penten-3-ol \n67                                    2-methyl-l-pentanol \n68                                    3-methyl-l-pentanol \n69                                     2-methyl-2-pentanol\n70                                               1-hexanol\n72                                              2-hexanol \n73                                                 hexenol\n74                                            2-hexen-l-ol\n75                                        cis-2-hexen-l-ol\n76                                     trans-2-hexen-l-ol \n77                                           3-hexen-l-ol \n78                                       cis-3-hexen-l-ol \n80                                     trans-3-hexen-1 -ol\n81                                       cis-4-hexen-l-ol \n82                                           methylhexanol\n83                                     2-methyl-3-hexanol \n84                                    5-methyl-1 -hexanol \n85                                             1-heptanol \n86                                              2-heptanol\n87                                              4-heptanol\n88                                 6-methyl-5-hepten-2-ol \n89                                               1-octanol\n91                                           l-octen-3-ol \n92                                     7-methyl-l-octanol \n93                                              1-decanol \n94                                            8-p-cymenol \n95                                             citronellol\n96                                                farnesol\n97                                               geraniol \n98                                                linalool\n100                                                 nerol \n101                                              nerolidol\n102                                          terpinen-4-ol\n103                                            o-terpineol\n104                                        benzyl alcohol \n105                                        2-phenylethanol\n107                              4-isopropylbenzyl alcohol\n108                                               menthol \n109                                                PHENOLS\n110                                                phenol \n111                                              o-cresol \n112                                               p-cresol\n113                                          4-ethylphenol\n114                                         4-vinylphenol \n115                                               guaiacol\n116                                4-ethyl-2-methoxyphenol\n117                                2-methoxy-4-vinylphenol\n118                                                eugenol\n119                                           3,4-xylenol \n120                                                 ETHERS\n121                                          diethyl ether\n122                                   1,1-dipropoxyethane \n123                    1-ethoxy-l (3-methylbutoxy)-ethane \n124                              1-ethoxy-l-pentoxyethane \n125                             l-methoxy-4-methylbenzene \n126                              isopropyl-methoxybenzene \n127                                    2-methoxy-biphenyl \n128                                              ALDEHYDES\n129                                           formaldehyde\n130                                           acetaldehyde\n175                                              propanal \n176                                               acrolein\n177                                       2-methylpropanal\n178                                                butanal\n179                                              2-butenal\n180                                       2-methylbutanal \n181                                       3-methylbutanal \n183                                    2-methyl-2-butenal \n184                                          tiglaldehyde \n185                                               pentanal\n186                                            2-pentenal \n187                                       trans-2-pentenal\n188                                             3-pentenal\n189                                         methylpentenal\n190                                                hexanal\n192                                               hexenal \n193                                              2-hexenal\n194                                          cis-2-hexenal\n195                                        trans-2-hexenal\n197                                         cis-3-hexenal \n198                                      2E,4Z-hexadienal \n199                                      2E,4E-hexadienal \n200                                               heptanal\n201                                              heptenal \n202                                             2-heptenal\n203                                       trans-2-heptenal\n204                                      2E,4Z-heptadienal\n205                                     2E,4E-heptadienal \n206                                               octanal \n207                                             2-octenal \n208                                       trans-2-octenal \n209                                               nonanal \n210                                              2-nonenal\n211                                        trans-2-nonenal\n212                                      2E,4E-nonadienal \n213                                               decanal \n214                                             2-decenal \n215                                        2,4-decadienal \n216                                       2E,4Z-decadienal\n217                                      2E,4E-decadienal \n218                                             undecanal \n219                                              dodecanal\n221                                                citral \n222                                                 neral \n223                                              geranial \n224                                           citronellal \n225                                               farnesal\n226                                          benzaldehyde \n228                                  3-methylbenzaldehyde \n229                                  4-methylbenzaldehyde \n230                                        salicylaldehyde\n231                                  4-hydroxybenzaldehyde\n232                                 3-methoxybenzaldehyde \n233                                          anisaldehyde \n234                                  2-phenylacetaldehyde \n235                                      3-phenylpropanal \n236                                         cinnamaldehyde\n237                                                KETONES\n238                                                acetone\n239                                            2-butanone \n240                                  3-hydroxy-2-butanone \n241                                          3-buten-2-one\n242                                            2-pentanone\n243                                           3-pentanone \n244                                         l-penten-3-one\n245                                         3-penten-2-one\n246                                         cyclopentanone\n247                                 2-ethylcyclopentanone \n248                                  2-methyl-3-pentanone \n249                                          mesityl oxide\n250                       4-hydroxy- 4-methyl-2-pentanone \n251                              2,4-dimethylpentan-3-one \n252                                            2-hexanone \n253                2-hydroxy-2,6,6-trimethylcyclohexanone \n254                                           2-heptanone \n255                                  4-methyl-3-heptanone \n256                                        methylheptenone\n257                               6-methyl-5-hepten-2-one \n259                           6-methyl-35-heptadien-2-one \n260                                             2-octanone\n261                                   3E,5E-octadien-2-one\n262                                            2-nonanone \n263                                   trans-2-nonen-4-one \n265                                             undecanone\n266                                          2-dodecanone \n267                                         pseudo-ionone \n268                                        geranylacetone \n270                                       farnesylacetone \n272                                              tt-ionone\n273                                               ß-ionone\n275                                              •y-ionone\n276                                         epoxy-0-ionone\n278                                                carvone\n279                                          acetophenone \n280                                  4-methylacetophenone \n281                                 2-hydroxyacetophenone \n282                                  4-methoxyacetophenone\n283                         4-methyl-4-phenyl-2-pentanone \n284                                   l-phenyl-2-propanone\n285                                    l-phenyl-2-butanone\n286                                   DICARBONYL COMPOUNDS\n287                                               glyoxal \n288                                          methylglyoxal\n289                                              biacetyl \n291                                       2-oxo-3-butenal \n292                                       2,3-pentanedione\n293                                      2,3-heptanedione \n294                                                  ACIDS\n295                                                formic \n296                                                 acetic\n297                                             propanoic \n298                                     2-methylpropanoic \n299                                              butanoic \n300                                      2-methylbutanoic \n301                                      3-methylbutanoic \n302                                              pentanoic\n303                                     4-methylpentanoic \n304                                              hexanoic \n305                                             4-hexenoic\n306                                             heptanoic \n308                                              octanoic \n309                                                geranic\n310                                               nonanoic\n311                                               myristic\n312                                         pentadecanoic \n313                                               palmitic\n314                                               stearic \n315                                                 oleic \n316                                               linoleic\n317                                             linolenic \n318                                               benzoic \n319                                              salicylic\n320                                        2-phenylacetic \n321                                              cinnamic \n322                                     4-hydroxycinnamic \n323                                                 ESTERS\n324                                        methyl fonnate \n325                                         ethyl fonnate \n326                                        pentyl fonnate \n327                                      phenetyl fonnate \n328                                        methyl acetate \n329                                         ethyl acetate \n330                                        propyl acetate \n331                                          butyl acetate\n332                                 2-methylbutyl acetate \n333                                     isopentyl acetate \n334                                         pentyl acetate\n335                                         hexyl acetate \n336                               trans-2-hexenyl acetate \n337                                     3-hexenyl acetate \n338                                  cis-3-hexenyl acetate\n339                              trans-3-hexeny 1 acetate2\n340                                        heptyl acetate \n341                               6 -methylheptyl acetate \n342                                         nonyl acetate \n343                                      phenethyl acetate\n344                                    citronellyl acetate\n345                                       geranyl acetate \n346                                        linalyl acetate\n347                                       ethyl propanoate\n348                                  isopentyl propanoate \n349                                 citronellyl propanoate\n350                                      methyl butanoate \n352                                     2-butyl butanoate \n353                                   isopentyl butanoate \n354                                 citronellyl butanoate \n355                                     geranyl butanoate \n356                            isobutyl 3-methylbutanoate \n357                       2-methylbutyl 3-methylbutanoate \n358                           isopentyl 3-methylbutanoate \n359                                   isobutyl pentanoate \n360                                   isopentyl pentanoate\n361                                      methyl hexanoate \n362                                        ethyl hexanoate\n363                                        butyl hexanoate\n364                                   isopentyl hexanoate \n365                                        hexyl hexanoate\n366                                  isopentyl heptanoate \n367                                       methyl octanoate\n368                                      propyl nonanoate \n369                                   isopentyl nonanoate \n370                                      propyl decanoate \n371                                   isopentyl decanoate \n372                                       methyl myristate\n373                                        ethyl myristate\n374                                  methyl pentadecanoate\n375                                      methyl palmitate \n376                                        ethyl palmitate\n377                                          methyl oléate\n378                                      methyl linoleate \n379                                        ethyl linoleate\n380                                      methyl linolenate\n381                                       ethyl linolenate\n382                                       methylsalicylate\n383                                       ethyl salicylate\n384                                               LACTONES\n385                                        7-butyrolactone\n386                                 2-methyl-4-butanolide \n387                              3-methyl-2-buten-4-olide \n388                                          4-pentanolide\n389                                3-methyl-4-pentanolide \n390                                          4-hexanolide \n391                                6-hydroxy-5-hexanolide \n392                                          4-octanolide \n393                                          5-octanolide \n394                                           4-nonanolide\n396                          2,4-dimethyl-2-nonen-4-olide \n397                                  dihydroactinidiolide \n399                                              phtalide \n400                                       SULFUR COMPOUNDS\n401                                       hydrogen sulfide\n402                                       dimethyl sulfide\n403                                   ethylmethyl sulfide \n404                                    dimethyl disulfide \n405                                methylpropyl disulfide \n406                                          methanethiol \n407                                 2-(methylthio)ethanol \n408                                 3(methylthio)-propanol\n409                              5(methylthio)-l-pentanol \n410                            2(methylthio)-acetaldehyde \n411                                3(methylthio)-propanal \n412                           methyl-methanethiosulfonate \n413                                     NITROGEN COMPOUNDS\n414                                            methylamine\n415                                             ethylamine\n416                                          dimethylamine\n417                                        trimethylamine \n418                                            propylamine\n419                                            butylamine \n420                                          isobutylamine\n421                                    dimethylethylamine \n422                                           diethylamine\n423                                    2-methylbutylamine \n424                                           pentylamine \n425                                         isopentylamine\n426                                         diphenylamine \n427                                 3-methylbutanal-oxime \n428                                         butanenitrile \n429                                  3-methylbutanenitrile\n430                                        pentanenitrile \n431                                         benzyl cyanide\n432                                    3-methylnitrobutane\n433                          3-hydroxy-3-methylnitrobutane\n434                                      HALOGEN COMPOUNDS\n435                                       trichloromethane\n436                                     trichloroethylene \n437                                    1,2-dichlorobenzene\n438               OXYGEN-CONTAINING HETEROCYCLIC COMPOUNDS\n439                                                 furan \n440                                          2-methylfuran\n441                                          2-ethylfuran \n442                                         2-propylfuran \n443                                      2-isobutenylfuran\n444                             2-isopropyl-5-methylfuran \n445                            2-isopropenyl-5-methylfuran\n446                              2-methyl-5-propenylfuran \n447                                         2-pentylfuran \n448                                          2-hexylfuran \n449                                         2-heptylfuran \n450                                            acetylfuran\n451                                         2-acetylfuran \n452                                              furfural \n454                                       5-methylfurfural\n455                                 2-acetyl-5-methylfuran\n456                              2-acetonyl-5-methylfuran \n457                             methyl-2-furancarboxylate \n458                                          dibenzofuran \n459                                      furfuryl alcohol \n460                                2-furancarboxylic acid \n461                          2-methyltetrahydro-3-furanone\n462                               linalool oxide I. or II.\n463                                     linalool oxide I. \n465                                    linalool oxide II. \n467                                   linalool III. or IV.\n468                                            structure) \n469                                           1,4-dioxane \n470                          2,2,4-trimethyl-l3-dioxolane \n471               2,7-dioxa-l,3,3-trimethylbicycloheptane \n472                   ó.S-dioxa-l.S-dimethylbicyclooctane \n473               SULFUR-CONTAINING HETEROCYCLIC COMPOUNDS\n474                                     2-forraylthiophene\n475                                     3-formylthiophene \n476                            2-formyl-5-methylthiophene \n477                                     2-acetylthiophene \n478                            2-thiophenecarboxyIic acid \n479             NITROGEN-CONTAINING HETEROCYCLIC COMPOUNDS\n480                                               pyrrole \n481                                   2,5-dimethylpyrrole \n483                                        2-formylpyrrole\n484                                       2-acetylpyrrole \n485                                              pyridine \n486                                      2-formylpyridine \n487                                         methylpyrazine\n488                                      2-methylpyrazine \n489                                  2,6-dimethylpyrazine \n490                               2-ethyl-6-vinylpyrazine \n491                          2-isopropyl-3-methoxypyrazine\n492                                                indene \n493 SULFUR- AND NITROGEN-CONTAINING HETEROCYCLIC COMPOUNDS\n494                                      2-propylthiazole \n495                                    2-isobutylthiazole \n497                                   2-sec-butylthiazole \n498                          2-isopropyl-4-methylthiazole \n499                                          benzothiazole\n500               NITROGEN-AND OXYGEN-CONTAINING COMPOUNDS\n501                                        4-butyloxazole \n502                               5-pentyl-4-ethyloxazole \n503                                 4,5-dimethylisoxazole \n\ntomato_cleaned$compounds <- str_remove_all(tomato_cleaned$compounds, \n                                           \"\\\\s\")\n\n\n\nThe file could be exported as a .csv file as a back up, in case it is needed again in the future.\nTRANSFORM\nNext, I had to split the compounds into various chemical categories.\n\n\nhydrocarbons <- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(2:39) %>% \n  mutate(category = \"hydrocarbons\")\n\n\nalcohols <- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(41:98) %>% \n  mutate(category = \"alcohols\")\n\nphenols <- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(99:108) %>% \n  mutate(category = \"phenols\")\n\nethers<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(111:116) %>% \n  mutate(category = \"ethers\")\n\naldehydes<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(118:176) %>% \n  mutate(category = \"aldehydes\")\n\nketones<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(178:219) %>% \n  mutate(category = \"ketones\")\n\ndicarbonyl<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(221:226) %>% \n  mutate(category = \"dicarbonyl\")\n\nacids<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(228:254) %>% \n  mutate(category = \"acids\")\n\nesters<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(256:314) %>% \n  mutate(category = \"esters\")\n\nlactones<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(316:328) %>% \n  mutate(category = \"lactones\")\n\nsulfur<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(330:341) %>% \n  mutate(category = \"sulfur\")\n\nnitrogen<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(343:362) %>% \n  mutate(category = \"nitrogen\")\n\nhalogen<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(364:366) %>% \n  mutate(category = \"halogen\")\n\noxygen<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(368:398) %>% \n  mutate(category = \"oxygen\")\n\nsulfur_heterocyclic<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(400:404) %>% \n  mutate(category = \"sulfur_heterocyclic\")\n\nnitrogen_heterocyclic<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(406:417) %>% \n  mutate(category = \"nitrogen_heterocyclic\")\n\nnitrogen_sulfur_heterocyclic<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(419:423) %>% \n  mutate(category = \"nitrogen_sulfur_heterocyclic\")\n\nnitrogen_oxygen_heterocyclic<- tomato_cleaned %>% \n  as_tibble() %>% \n  slice(425:427) %>% \n  mutate(category = \"nitrogen_oxygen_heterocyclic\")\n\n\n\nVisualize\n\n\ntomatoes_compounds <- bind_rows(hydrocarbons,\n                                alcohols,\n                                phenols,\n                                ethers,\n                                aldehydes,\n                                ketones,\n                                dicarbonyl,\n                                acids,\n                                esters,\n                                lactones,\n                                sulfur,\n                                halogen,\n                                oxygen,\n                                sulfur_heterocyclic,\n                                nitrogen_heterocyclic,\n                                nitrogen_sulfur_heterocyclic,\n                                nitrogen_oxygen_heterocyclic\n                                )\n\nplot <- tomatoes_compounds %>% \n  group_by(category) %>% \n  summarise(count = n()) %>% \n  ggplot(aes(x = reorder(category, count), y = count, label = count)) +\n  geom_col(fill = \"tomato2\") +\n  geom_text(aes(label = count), hjust = -0.5, size = 5) +\n  scale_y_continuous(expand = c(0,0), limits = c(0, 80)) +\n  labs(y = \"No. of compounds\",\n       x = \"Category\",\n       title = \"Number of volatile compounds identified in tomatoes, sorted by chemical category\",\n       subtitle = \"Esters, aldehydes and alcohols dominate the types of compounds identified\",\n       caption = \"Petro-Turza(1989): Flavor of tomato and tomato products \") +\n  coord_flip() +\n  theme_classic() +\n   theme(title = element_text(size = 28),\n        axis.title = element_text(size = 24, face = \"bold\"),\n        axis.text = element_text(size = 20))\n\nplot\n\n\n\n\nReflections\nIt may have been easier to type out the list of 400 compounds, which would only take an hour or less, with the formatting done properly on the onset. However, if the table was much longer, text cleaning would be more effectively carried out by stringr. Some improvements could be made to the script so that I do not have to carry out multiple str_replace_all, and to automatically filter out by categories instead of manually defining them. However, it was a good beginner’s practice on text cleaning using the stringr package as I do not often have the chance to use regular expressions, and I found the str_detect, str_which and str_view_all functions extremely useful in locating regex matches.\nThe plot above only lists the number of compounds identified so far by chemical classes, but does not show which are the character impact compounds that contribute significantly to tomatoes.\nHistorically, researchers focused on identifying volatiles, quantifying them and classifying them based on their odor thresholds to determine which compounds played a contributory role to tomato flavor. However, the new trend is in assessing the importance of compounds based on how much they contribute to the liking of tomato flavor, and this could be by means of targeted metabolomics, or by generating prediction models for different descriptors of tomato flavor using regression analysis of both volatile and non-volatile compounds, or by carrying out multivariate modelling on physicochemical, volatile and sensory parameters(Rambla et al. 2013).\nIt would be interesting to try to apply prediction models and multivariate analysis in R.\nReferences\nhttps://www.r-bloggers.com/2019/09/pdf-scraping-in-r-with-tabulizer/\n\n\n\nBaldwin, E. A., J. W. Scott, M. A. Einstein, T. M. M. Malundo, B. T. Carr, R. L. Shewfelt, and K. S. Tandon. 1998. “Relationship Between Sensory and Instrumental Analysis for Tomato Flavor.” Journal of the American Society for Horticultural Science 123 (5): 906–15.\n\n\nButtery, Ron G., Roy Teranishi, and Louisa C. Ling. 1987. “Fresh Tomato Aroma Volatiles: A Quantitative Study.” Journal of Agricultural and Food Chemistry 35 (4): 540–44.\n\n\nPetro‐Turza, Martha. 1986. “Flavor of Tomato and Tomato Products.” Food Reviews International 2 (3): 309–51. https://doi.org/10.1080/87559128609540802.\n\n\nRambla, José L., Yury M. Tikunov, Antonio J. Monforte, Arnaud G. Bovy, and Antonio Granell. 2013. “The Expanded Tomato Fruit Volatile Landscape.” Journal of Experimental Botany 65 (16): 4613–23.\n\n\nYilmaz, Emin. 2001. “The Chemistry of Fresh Tomato Flavor.” Turkish Journal of Agriculture and Forestry 25 (3): 149–55.\n\n\n\n\n",
    "preview": "posts/20210130_Volatiles_tomato/Volatiles_tomatoes_files/figure-html5/unnamed-chunk-6-1.png",
    "last_modified": "2021-02-01T23:36:52+08:00",
    "input_file": {},
    "preview_width": 3840,
    "preview_height": 2304
  },
  {
    "path": "posts/20210123_PCA wine/",
    "title": "PCA Wine",
    "description": "PCA (using tidymodels) with wine dataset",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-23",
    "categories": [],
    "contents": "\nSummary\nPCA is a data reduction technique, to uncover latent variables that are uncorrelated. It is an unsupervised way of classification. Not all of the variables in high-dimensional data are required. Some are highly correlated with others and these variables may be omitted, while retaining a similar level of information in the dataset in terms of explaining the variance.\nIt is used as an exploratory data analysis tool, and may be used for feature engineering and/or clustering.\nWorkflow\nImport data\nExploratory data analysis\nskim\nggcorr\nggpairs\nCheck assumptions on whether PCA can be carried out\nKMO\nBartlett\nCarry out PCA using tidymodels workflow\nAlways use only continuous variables, ensure that there are no missing data. Determine the number of components using eigenvalues, scree plots and parallel analysis.\nrecipe : preprocess the data (missing values, center and scale, ensuring that variables are continuous)\nprep : evaluate the data\nbake : get the PCA Scores results\nvisualize\ncommunicate results: show the scree plot, PCA loadings, variance explained by each component, loadings and score plot.\nThe scores plot show the positions of the individual wine samples in the coordinate system of the PCs.\nThe loadings plot shows the contribution of the X variables to the PCs.\nLoading packages\n\n\nlibrary(pacman)\np_load(corrr, palmerpenguins, GGally, tidymodels, tidytext, tidyverse, psych,\n       skimr, gridExtra, kohonen, janitor, learntidymodels, kohonen)\n\n\n\nImport\nThis dataset is from the kohonen package. It contains 177 rows and 13 columns.\nThese data are the results of chemical analyses of wines grown in the same region in Italy (Piedmont) but derived from three different cultivars: Nebbiolo, Barberas and Grignolino grapes. The wine from the Nebbiolo grape is called Barolo. The data contain the quantities of several constituents found in each of the three types of wines, as well as some spectroscopic variables.\nThe dataset requires some cleaning, and the type of wine was added to the datset.\n\n\ndata(wines)\n\nwines <- as.data.frame(wines) %>% \n  janitor::clean_names() %>%  # require data.frame\n  as_tibble() %>% \n  cbind(vintages)  # vintages = Y outcome = category\n \nglimpse(wines)\n\n\nRows: 177\nColumns: 14\n$ alcohol          <dbl> 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, 1…\n$ malic_acid       <dbl> 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, 1…\n$ ash              <dbl> 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, 2…\n$ ash_alkalinity   <dbl> 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, 1…\n$ magnesium        <dbl> 100, 101, 113, 118, 112, 96, 121, 97, 98, 1…\n$ tot_phenols      <dbl> 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, 2…\n$ flavonoids       <dbl> 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, 2…\n$ non_flav_phenols <dbl> 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, 0…\n$ proanth          <dbl> 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, 1…\n$ col_int          <dbl> 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, 5…\n$ col_hue          <dbl> 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, 1…\n$ od_ratio         <dbl> 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, 2…\n$ proline          <dbl> 1050, 1185, 1480, 735, 1450, 1290, 1295, 10…\n$ vintages         <fct> Barolo, Barolo, Barolo, Barolo, Barolo, Bar…\n\nEDA\nSome exploratory data analysis was carried out:\nWhat are the types of variables? Categorical or numerical?\nWhat is the distribution like? Skewed?\nAre there any missing values?\nAre there any outliers?\nCheck the types of wine\nAre the variables quite correlated with each other?\nskimr\n\n\nskim(wines) # 177 x 13, all numeric + Y outcome\n\n\nTable 1: Data summary\nName\nwines\nNumber of rows\n177\nNumber of columns\n14\n_______________________\n\nColumn type frequency:\n\nfactor\n1\nnumeric\n13\n________________________\n\nGroup variables\nNone\nVariable type: factor\nskim_variable\nn_missing\ncomplete_rate\nordered\nn_unique\ntop_counts\nvintages\n0\n1\nFALSE\n3\nGri: 71, Bar: 58, Bar: 48\nVariable type: numeric\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\nalcohol\n0\n1\n12.99\n0.81\n11.03\n12.36\n13.05\n13.67\n14.83\n▂▇▇▇▃\nmalic_acid\n0\n1\n2.34\n1.12\n0.74\n1.60\n1.87\n3.10\n5.80\n▇▅▂▂▁\nash\n0\n1\n2.37\n0.28\n1.36\n2.21\n2.36\n2.56\n3.23\n▁▂▇▅▁\nash_alkalinity\n0\n1\n19.52\n3.34\n10.60\n17.20\n19.50\n21.50\n30.00\n▁▆▇▃▁\nmagnesium\n0\n1\n99.59\n14.17\n70.00\n88.00\n98.00\n107.00\n162.00\n▅▇▃▁▁\ntot_phenols\n0\n1\n2.29\n0.63\n0.98\n1.74\n2.35\n2.80\n3.88\n▅▇▇▇▁\nflavonoids\n0\n1\n2.02\n1.00\n0.34\n1.20\n2.13\n2.86\n5.08\n▆▆▇▂▁\nnon_flav_phenols\n0\n1\n0.36\n0.12\n0.13\n0.27\n0.34\n0.44\n0.66\n▃▇▅▃▂\nproanth\n0\n1\n1.59\n0.57\n0.41\n1.25\n1.55\n1.95\n3.58\n▃▇▆▂▁\ncol_int\n0\n1\n5.05\n2.32\n1.28\n3.21\n4.68\n6.20\n13.00\n▇▇▃▂▁\ncol_hue\n0\n1\n0.96\n0.23\n0.48\n0.78\n0.96\n1.12\n1.71\n▅▇▇▃▁\nod_ratio\n0\n1\n2.60\n0.71\n1.27\n1.93\n2.78\n3.17\n4.00\n▆▃▆▇▂\nproline\n0\n1\n745.10\n314.88\n278.00\n500.00\n672.00\n985.00\n1680.00\n▇▇▅▃▁\n\nGGally\n\n\nwines %>% \n  select(-vintages) %>% \n  ggcorr(label = T, label_alpha = T, label_round = 2)\n\n\n\nwines %>% \n  ggpairs(aes(col = vintages))\n\n\n\n\nChecking assumptions\nIs the dataset suitable for PCA analysis?\n\n\n# Continuous Y\n# No missing data\n# Check assumptions for PCA #####\nwines_no_y <- wines %>% \n  select(-vintages)\n\nglimpse(wines_no_y)\n\n\nRows: 177\nColumns: 13\n$ alcohol          <dbl> 13.20, 13.16, 14.37, 13.24, 14.20, 14.39, 1…\n$ malic_acid       <dbl> 1.78, 2.36, 1.95, 2.59, 1.76, 1.87, 2.15, 1…\n$ ash              <dbl> 2.14, 2.67, 2.50, 2.87, 2.45, 2.45, 2.61, 2…\n$ ash_alkalinity   <dbl> 11.2, 18.6, 16.8, 21.0, 15.2, 14.6, 17.6, 1…\n$ magnesium        <dbl> 100, 101, 113, 118, 112, 96, 121, 97, 98, 1…\n$ tot_phenols      <dbl> 2.65, 2.80, 3.85, 2.80, 3.27, 2.50, 2.60, 2…\n$ flavonoids       <dbl> 2.76, 3.24, 3.49, 2.69, 3.39, 2.52, 2.51, 2…\n$ non_flav_phenols <dbl> 0.26, 0.30, 0.24, 0.39, 0.34, 0.30, 0.31, 0…\n$ proanth          <dbl> 1.28, 2.81, 2.18, 1.82, 1.97, 1.98, 1.25, 1…\n$ col_int          <dbl> 4.38, 5.68, 7.80, 4.32, 6.75, 5.25, 5.05, 5…\n$ col_hue          <dbl> 1.05, 1.03, 0.86, 1.04, 1.05, 1.02, 1.06, 1…\n$ od_ratio         <dbl> 3.40, 3.17, 3.45, 2.93, 2.85, 3.58, 3.58, 2…\n$ proline          <dbl> 1050, 1185, 1480, 735, 1450, 1290, 1295, 10…\n\n# KMO: Indicates the proportion of variance in the variables that may be caused by underlying factors. High values (close to 1) indicate that factor analysis may be useful.\nwines_no_y %>% \n  cor() %>% \n  KMO() # .70 above : YES\n\n\nKaiser-Meyer-Olkin factor adequacy\nCall: KMO(r = .)\nOverall MSA =  0.78\nMSA for each item = \n         alcohol       malic_acid              ash   ash_alkalinity \n            0.73             0.80             0.44             0.68 \n       magnesium      tot_phenols       flavonoids non_flav_phenols \n            0.67             0.87             0.81             0.82 \n         proanth          col_int          col_hue         od_ratio \n            0.85             0.62             0.79             0.86 \n         proline \n            0.81 \n\n# Bartlett's test of sphericity: tests the hypothesis that the correlation matrix is an identity matrix (ie variables are unrelated and not suitable for structure detection.) For factor analysis, the p. value should be <0.05.\n\nwines_no_y %>% \n  cor() %>% \n  cortest.bartlett(., n = 177) # p<0.05\n\n\n$chisq\n[1] 1306.787\n\n$p.value\n[1] 3.302319e-222\n\n$df\n[1] 78\n\nTidymodels (PCA)\nRecipe\nWith the use of update_role(), the types of wine information is retained in the dataset.\nstep_normalize() combines step_center() and step_scale()\nNote that step_pca is the second step –> will need to retrieve the PCA results from the second list later.\n\n\nwines_recipe <- recipe(~ ., data = wines) %>% \n  update_role(vintages, new_role = \"id\") %>%  \n  # step_naomit(all_predictors()) %>% \n  step_normalize(all_predictors()) %>% \n  step_pca(all_predictors(), id = \"pca\")\n\nwines_recipe # 13 predictors\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n        id          1\n predictor         13\n\nOperations:\n\nCentering and scaling for all_predictors()\nNo PCA components were extracted.\n\nPreparation\n\n\nwines_prep <- prep(wines_recipe)\n\nwines_prep # trained\n\n\nData Recipe\n\nInputs:\n\n      role #variables\n        id          1\n predictor         13\n\nTraining data contained 177 data points and no missing data.\n\nOperations:\n\nCentering and scaling for alcohol, malic_acid, ... [trained]\nPCA extraction with alcohol, malic_acid, ... [trained]\n\ntidy_pca_loadings <- wines_prep %>% \n  tidy(id = \"pca\")\n\ntidy_pca_loadings # values here are the loading\n\n\n# A tibble: 169 x 4\n   terms               value component id   \n   <chr>               <dbl> <chr>     <chr>\n 1 alcohol          -0.138   PC1       pca  \n 2 malic_acid        0.246   PC1       pca  \n 3 ash               0.00432 PC1       pca  \n 4 ash_alkalinity    0.237   PC1       pca  \n 5 magnesium        -0.135   PC1       pca  \n 6 tot_phenols      -0.396   PC1       pca  \n 7 flavonoids       -0.424   PC1       pca  \n 8 non_flav_phenols  0.299   PC1       pca  \n 9 proanth          -0.313   PC1       pca  \n10 col_int           0.0933  PC1       pca  \n# … with 159 more rows\n\nBake\n\n\nwines_bake <- bake(wines_prep, wines)\nwines_bake  # has the PCA LOADING VECTORS that we are familiar with\n\n\n# A tibble: 177 x 6\n   vintages   PC1    PC2    PC3      PC4     PC5\n   <fct>    <dbl>  <dbl>  <dbl>    <dbl>   <dbl>\n 1 Barolo   -2.22 -0.301 -2.03   0.281   -0.259 \n 2 Barolo   -2.52  1.06   0.974 -0.734   -0.198 \n 3 Barolo   -3.74  2.80  -0.180 -0.575   -0.257 \n 4 Barolo   -1.02  0.886  2.02   0.432    0.274 \n 5 Barolo   -3.04  2.16  -0.637  0.486   -0.630 \n 6 Barolo   -2.45  1.20  -0.985  0.00466 -1.03  \n 7 Barolo   -2.06  1.64   0.143  1.20     0.0105\n 8 Barolo   -2.51  0.958 -1.78  -0.104   -0.871 \n 9 Barolo   -2.76  0.822 -0.986 -0.374   -0.437 \n10 Barolo   -3.48  1.35  -0.428 -0.0399  -0.316 \n# … with 167 more rows\n\nCheck number of PC\n\n\n# a. Eigenvalues: Keep components greater than 1\n# data is stored in penguins_prep, step 3\n\nwines_prep$steps[[2]]$res$sdev # 3\n\n\n [1] 2.1628220 1.5815708 1.2055413 0.9614802 0.9282978 0.8030241\n [7] 0.7429548 0.5922321 0.5377546 0.4967984 0.4748054 0.4103374\n[13] 0.3224124\n\n# b. Scree plot/Variance plot\n\nwines_prep %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms ==  \"percent variance\") %>% \n  ggplot(aes(x = component, y = value)) +\n  geom_point(size = 2) +\n  geom_line(size = 1) +\n  scale_x_continuous(breaks = 1:4) +\n  labs(title = \"% Variance explained\",\n       y = \"% total variance\",\n       x = \"PC\",\n       caption = \"Source: ChemometricswithR book\") +\n  geom_text(aes(label = round(value, 2)), vjust = -0.3, size = 4) +\n  theme_minimal() +\n  theme(axis.title = element_text(face = \"bold\", size = 12),\n        axis.text = element_text(size = 10),\n        plot.title = element_text(size = 14, face = \"bold\"))  # 2 or 3\n\n\n\n# bii: Cumulative variance plot\n\nwines_prep %>% \n  tidy(id = \"pca\", type = \"variance\") %>% \n  filter(terms == \"cumulative percent variance\") %>%\n  ggplot(aes(component, value)) +\n  geom_col(fill= \"forestgreen\") +\n  labs(x = \"Principal Components\", \n       y = \"Cumulative variance explained (%)\",\n       title = \"Cumulative Variance explained\") +\n  geom_text(aes(label = round(value, 2)), vjust = -0.2, size = 4) +\n  theme_minimal() +\n  theme(axis.title = element_text(face = \"bold\", size = 12),\n        axis.text = element_text(size = 10),\n        plot.title = element_text(size = 14, face = \"bold\")) \n\n\n\n# c. Parallel analysis\n\nfa.parallel(cor(wines_no_y),\n            n.obs = 333,\n            cor = \"cor\",\n            plot = T)  # 3\n\n\n\nParallel analysis suggests that the number of factors =  4  and the number of components =  3 \n\nVisualize\nLoadings plot\n\n\nplot_loadings <- tidy_pca_loadings %>% \n  filter(component %in% c(\"PC1\", \"PC2\", \"PC3\", \"PC4\")) %>% \n  mutate(terms = tidytext::reorder_within(terms, \n                                          abs(value), \n                                          component)) %>% \n  ggplot(aes(abs(value), terms, fill = value>0)) +\n  geom_col() +\n  facet_wrap( ~component, scales = \"free_y\") +\n  scale_y_reordered() + # appends ___ and then the facet at the end of each string\n  scale_fill_manual(values = c(\"deepskyblue4\", \"darkorange\")) +\n  labs( x = \"absolute value of contribution\",\n        y = NULL,\n        fill = \"Positive?\",\n        title = \"PCA Loadings Plot\",\n        subtitle = \"Number of PC should be 3, compare the pos and the neg\",\n        caption = \"Source: ChemometricswithR\") +\n  theme_minimal()\n\n\nplot_loadings\n\n\n\n# PC1: flavonoids, tot_phenols, od_ratio, proanthocyanidins, col_hue, 36%\n# PC2: col_int, alcohol, proline, ash, magnesium; 19.2%\n# PC3: ash, ash_alkalinity, non_flav phenols; 11.2%\n# PC4: malic acid?\n\n\n\nAn alternative way to plot:\n\n\n# alternate plot loadings\n\nlearntidymodels::plot_top_loadings(wines_prep,\n                  component_number <= 4, n = 5) +\n  scale_fill_manual(values = c(\"deepskyblue4\", \"darkorange\")) +\n  theme_minimal()\n\n\n\n\nLoadings only\n\n\n# define arrow style\narrow_style <- arrow(angle = 30,\n                     length = unit(0.2, \"inches\"),\n                     type = \"closed\")\n\n# get pca loadings into wider format\npca_loadings_wider <- tidy_pca_loadings%>% \n  pivot_wider(names_from = component, id_cols = terms)\n\n\npca_loadings_only <- pca_loadings_wider %>% \n  ggplot(aes(x = PC1, y = PC2)) +\n  geom_segment(aes(xend = PC1, yend = PC2),\n               x = 0, \n               y = 0,\n               arrow = arrow_style) +\n  ggrepel::geom_text_repel(aes(x = PC1, y = PC2, label = terms),\n            hjust = 0, \n            vjust = 1,\n            size = 5,\n            color = \"red\") +\n  labs(title = \"Loadings on PCs 1 and 2 for normalized data\") +\n  theme_classic()\n\n\n\nScores plot\n\n\n# Scores plot #####\n# PCA SCORES are in bake\npc1pc2_scores_plot <- wines_bake %>% \n  ggplot(aes(PC1, PC2)) +\n  geom_point(aes(color = vintages, shape = vintages), \n             alpha = 0.8, size = 2) +\n  scale_color_manual(values = c(\"deepskyblue4\", \"darkorange\", \"purple\")) +\n  labs(title = \"Scores on PCs 1 and 2 for normalized data\",\n       x = \"PC1 (36%)\",\n       y = \"PC2 (19.2%)\") +\n  theme_classic() +\n  theme(legend.position = \"none\") \n\n\n\nFinalised plots\n\n\ngrid.arrange(pc1pc2_scores_plot, pca_loadings_only, ncol = 2)\n\n\n\n\nCheck against Data\n\n\nwines %>% \n  group_by(vintages) %>% \n  summarise(across(c(flavonoids, col_int, ash, malic_acid),\n                   mean,\n                   na.rm = T))\n\n\n# A tibble: 3 x 5\n  vintages   flavonoids col_int   ash malic_acid\n  <fct>           <dbl>   <dbl> <dbl>      <dbl>\n1 Barbera         0.781    7.40  2.44       3.33\n2 Barolo          2.98     5.53  2.46       2.02\n3 Grignolino      2.08     3.09  2.24       1.93\n\nInterpretation of results\nPCA allows for exploratory characterizing of x variables that are associated with each other.\nPC1: flavanoids, total phenols, OD_ratio. PC2: color intensity, alcohol, proline PC3: ash, ash_alkalinity PC4: malic acid (by right 3 components are sufficient)\nBarbera, indicated in blue, has the largest score on PC 1 and PC2. Barolo, indicated in orange, has the smallest score on PC 1. Grignolo, indicated in purple, has the lowest score on PC 2.\nBarbera has low flavonoids, high col_int and high malic acid Barolo has high flavonoids, medium col_int and intermediate malic acid Grignolino has intermediate flavonoids, high col_int and low malic acid.\nReferences\nhttps://rdrr.io/github/tidymodels/learntidymodels/f/inst/tutorials/pca_recipes/pca_recipes.Rmd\nhttps://allisonhorst.github.io/palmerpenguins/articles/articles/pca.html\nhttps://www.ibm.com/support/knowledgecenter/en/SSLVMB_subs/statistics_casestudies_project_ddita/spss/tutorials/fac_telco_kmo_01.html\n\n\n\n",
    "preview": "posts/20210123_PCA wine/PCA-wine_files/figure-html5/unnamed-chunk-4-1.png",
    "last_modified": "2021-03-16T20:20:38+08:00",
    "input_file": {},
    "preview_width": 3072,
    "preview_height": 2304
  },
  {
    "path": "posts/20210120_statistical concepts/",
    "title": "Statistical Concepts",
    "description": "Definition of terms",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-20",
    "categories": [],
    "contents": "\nThis is a glossary of terms, in alphabetical order.\nCorrelation, r : whether there is any relationship between two variables. If so, whether the relationship is weak or strong, and what the direction of relationship is.\nPrincipal Component Analysis (PCA): a multivariate technique used to reduce the number of dimensions to explain the total variation in the data with a few linear combinations of original variables, which are uncorrelated.\nVariance, Sˆ2: the average of squared deviations of the values from mean. Square root of variance = standard deviation.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-21T00:34:33+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210115_kovats/",
    "title": "Kovats Index",
    "description": "R script for calculating Kovats Index",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\nContents\nBackground\nR workflow\nExampleLoad packages\nImport\nTransform\n\nReferences:\n\nBackground\nAbout 70% of my time at work is spent on interpreting GCMS and GC data. It is more of a qualitative type of identifying what each peak is, and this requires a seach based on mass spectra found in the GCMS library, as well as using the retention index. When working on GC data, I am even more reliant on the retention index for cross checking of peaks on GCMS, since there is no spectra information available.\nRetention time is influenced by GC conditions and column types. Using retention time alone is not useful when you are trying to compare with retention times stated in the literature, since the elution conditions are different.\nThe Kovats index (KI) may be used to convert retention times into standardised retention indices (RI), based on retention times of alkane standards. The equation for Non-Isothermal Kovats RI is shown below.\n\\[\nI_x = 100n + 100(t_x-t_n) / (t_(n+1) − t_n)\n\\]\nPrior to learning R, I used to do the calculation on an excel spreadsheet. This was cumbersome, first I had to key in the retention times of each alkane standard, and then update my formula for the range of retention times between each alkane standard, and then copy and paste all the compiled retention times into 2 columns. That involved a lot of clicking with the mouse.\nR workflow\nRun alkane standards on instrument (for example, GCMS) and compile the retention times in either .csv or .xlsx.\nCreate a function to calculate KI.\nCalculate the KI for retention times between each pair of alkane standard\nMerge the compiled retention times and corresponding KI together\nExport the data to excel and use the vlookup function to find out the KI when retention time is keyed in; alternatively, use inner_join function to tabulate calculated KI before identifying the peaks. I am using the former as there may be some small peaks that were not integrated, or coeluted with other peaks, so there is still a degree of manual input that is required.\nExample\nSample retention time data was retrieved from: https://massfinder.com/wiki/Retention_index_guide\nLoad packages\n\n\nlibrary(tidyverse)\n\n\n\nImport\n\n\n# Key in values\ncarbon_number <- c(\"8\", \"9\", \"10\", \"11\", \"12\", \"13\", \"14\", \"15\", \"16\")\nMS_RT <- c(1.85, 2.71, 3.69, 4.59, 5.37, 6.19, 7.17, 8.40, 9.99)\n\n# Create a tibble\nms_rt <- cbind(carbon_number, MS_RT) %>% as_tibble()\nms_rt$carbon_number <- as.numeric(ms_rt$carbon_number)\nms_rt$MS_RT <- as.numeric(ms_rt$MS_RT)\n\n# The data may also be imported from excel\n\n\n\nTransform\n\n\n# create function to calculate KI ####\nto_Calc_KI = function(n,Tn,m,Tm,Ti){\n  RI = 100*n + (100*(m-n)*((Ti-Tn)/(Tm-Tn)))\n  round(RI, 0)\n  \n}\n\n\n\n\n\n# create function to filter by carbon number ####\n# dat refers to data\n# col refers to column\n# val refers to values\n\nfilter_by_carbon_number <- function(dat, col, val){\n  filter(dat, col %in%  val)\n}\n\n\n\nThe following step could be improved on by creating another function to repeat the codes rather than manually changing the values.\n\n\nfil_c8c9 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(8,9)) \n\nfil_c9c10 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(9,10)) \n\nfil_c10c11 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(10,11)) \nfil_c11c12 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(11,12)) \nfil_c12c13 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(12,13)) \nfil_c13c14 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(13,14)) \nfil_c14c15 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(14,15)) \nfil_c15c16 <- filter_by_carbon_number(ms_rt, ms_rt$carbon_number, c(15,16)) \n\n\n\n\n\n# create function to generate tibble for KI calculation\ncreate_KI_tibble <- function(msrt_col, n , m){\n  seq(from = min(msrt_col), to = max(msrt_col), by = 0.01) %>% \n    as_tibble() %>% \n    rename(\"Ti\" = value) %>% \n    mutate(n = n,\n           m = m,\n           Tn = min(msrt_col), \n           Tm = max(msrt_col)) %>% \n    dplyr::select(n, Tn, m, Tm, Ti) %>% \n    mutate(KI = pmap_dbl(., to_Calc_KI))\n}\n\n\n\n\n\nc8c9 <- create_KI_tibble(fil_c8c9$MS_RT, 8, 9)\nc9c10 <- create_KI_tibble(fil_c9c10$MS_RT, 9, 10)\nc10c11 <- create_KI_tibble(fil_c10c11$MS_RT, 10, 11)\nc11c12 <- create_KI_tibble(fil_c11c12$MS_RT, 11, 12)\nc12c13 <- create_KI_tibble(fil_c12c13$MS_RT, 12, 13)\nc13c14 <- create_KI_tibble(fil_c13c14$MS_RT, 13, 14)\nc14c15 <- create_KI_tibble(fil_c14c15$MS_RT, 14, 15)\nc15c16 <- create_KI_tibble(fil_c15c16$MS_RT, 15, 16)\n\ncalculated_MS_KI <- rbind(c8c9, c9c10, c10c11, c11c12, c12c13, \n                          c13c14, c14c15, c15c16) %>% \n  select(Ti, KI)\n\n# Export created file if needed\n# write_xlsx(calculated_MS_KI, \"Kovats_Indices.xlsx\")\n\n\n\nLooking at the first 6 lines of tabulated KI:\n\n\nhead(calculated_MS_KI)\n\n\n# A tibble: 6 x 2\n     Ti    KI\n  <dbl> <dbl>\n1  1.85   800\n2  1.86   801\n3  1.87   802\n4  1.88   803\n5  1.89   805\n6  1.9    806\n\nReferences:\nhttps://webbook.nist.gov/chemistry/gc-ri/\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-15T11:19:30+08:00",
    "input_file": {}
  },
  {
    "path": "posts/20210118_calibration curves/",
    "title": "Calibration Curves Data",
    "description": "R script for calculating Limit of Detection and Limit of Quantification",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-15",
    "categories": [],
    "contents": "\n\nContents\nLoading required packages\nBackground\nImport Example Dataset\nVisualize\nModel\nErrors in slope and intercept of regression line\nPredict the value of x from y:\n\nNote: the data and theory on calibration curve were with reference from: Statistics and Chemometrics for Analytical Chemistry, James N. Miller and Jane Charlotte Miller, 6th edition, Chapter 5\nLoading required packages\n\n\nlibrary(pacman)\np_load(tidyverse, broom, chemCal)\n\n\n\nBackground\nChemists often work with calibration data using standards of known concentrations and putting them through instrumental analysis. When plotting a calibration curve, it is of interest to calculate the limit of detection (LOD) and limit of quantification (LOQ) of the method.\nImport Example Dataset\nThe fluorescence intensities of standard aqueous fluorescein solutions were analysed with a spectrophotometer, and the fluorescence results are shown below:\n\n# A tibble: 7 x 2\n  conc_pgml  fluo\n      <dbl> <dbl>\n1         0   2.1\n2         2   5  \n3         4   9  \n4         6  12.6\n5         8  17.3\n6        10  21  \n7        12  24.7\n\nVisualize\n\n\n\nModel\nLet’s fit a linear model to get the slope (b) and intercept(a).\n\\[\ny = a + bx\n\\]\n\n\nCall:\nlm(formula = fluo ~ conc_pgml, data = data)\n\nResiduals:\n       1        2        3        4        5        6        7 \n 0.58214 -0.37857 -0.23929 -0.50000  0.33929  0.17857  0.01786 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)   1.5179     0.2949   5.146  0.00363 ** \nconc_pgml     1.9304     0.0409  47.197 8.07e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.4328 on 5 degrees of freedom\nMultiple R-squared:  0.9978,    Adjusted R-squared:  0.9973 \nF-statistic:  2228 on 1 and 5 DF,  p-value: 8.066e-08\n\nFrom above, we can see that slope = 1.9304, and intercept = 1.5179.\nErrors in slope and intercept of regression line\nThe limit of detection is defined as:\n\\[\nLOD = \\gamma_B + 3_{SB}\n\\] where LOD is the analyte concentration wich gives a signal equal to the blank signal plus three standard deviations of the blank.\nA function was created to calculate LOD and LOQ:\n\n\ncalcLOD_y <- function(model) {\n  SSE <- sum(model$residuals**2)\n  n <- length(model$residuals) -2\n  Syx <- sqrt(SSE/n)\n  intercept <- as.numeric(model$coefficients[1])\n  calculated_y <- intercept + 3*Syx\n  names(calculated_y) <- \"calculated_y\"\n  print(calculated_y)\n  \n  chemCal::inverse.predict(model,\n                  newdata = calculated_y,\n                  alpha = 0.05) \n}\n\n\n\n\n\ncalcLOQ_y <- function(model) {\n  SSE <- sum(model$residuals**2)\n  n <- length(model$residuals) -2\n  Syx <- sqrt(SSE/n)\n  intercept <- as.numeric(model$coefficients[1])\n  calculated_y <- intercept + 10*Syx\n  names(calculated_y) <- \"calculated_y\"\n  print(calculated_y)\n  \n  chemCal::inverse.predict(model,\n                  newdata = calculated_y,\n                  alpha = 0.05) \n}\n\n\n\nInserting the linear model from the fluorescence data:\n\n\nLOD_x <- calcLOD_y(fl_mod)\n\n\ncalculated_y \n      2.8164 \n\nLOD_x$Prediction \n\n\n[1] 0.6726958\n\n\n\nLOQ_x <- calcLOQ_y(fl_mod)\n\n\ncalculated_y \n    5.846334 \n\nLOQ_x$Prediction \n\n\n[1] 2.242319\n\nPredict the value of x from y:\nTo predict the concentration of fluorescein that has fluorescence units of 2.9, we use the function inverse.predict():\n\n\nchemCal::inverse.predict(fl_mod, \n                newdata = 2.9,\n                alpha = 0.05)\n\n\n$Prediction\n[1] 0.7160037\n\n$`Standard Error`\n[1] 0.2645698\n\n$Confidence\n[1] 0.6800982\n\n$`Confidence Limits`\n[1] 0.03590545 1.39610195\n\n\n\n\n",
    "preview": "posts/20210118_calibration curves/calibration-curves_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-01-18T23:23:27+08:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/20210114_motivations/",
    "title": "Motivations",
    "description": "why R?",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\nWhy R?\nI attended a short modular course on R, and was introduced to more effective and efficient ways of structuring data for customised plots that look way better than on Excel and SPSS. At the end of the course, I really wanted to retain what I have learnt, and build on what I have learnt, so that I can be better at R.\nR, to me, is a new form of literacy (like how Microsoft Office was taught in school last time). It is also an effective approach to learn problem solving, as well as a job skill.\nAristotle — ‘The more you know, the more you know you don’t know.’\nand that makes me want to learn even more.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-14T20:47:28+08:00",
    "input_file": {}
  },
  {
    "path": "posts/welcome/",
    "title": "My first post: Learning goals for 2021",
    "description": "pRactice corner for coding in R",
    "author": [
      {
        "name": "lruolin",
        "url": {}
      }
    ],
    "date": "2021-01-14",
    "categories": [],
    "contents": "\nHi, this is my practice corner for coding in R. I would want to:\nlearn tidyverse\npractice on data visualization, exploration.\nlearn tidymodels/machine learning\nwork on chemistry related datasets using R\nlearn Design of Experiment\nlearn Chemometrics\nlearn how to analyse sensory data\nbe able to communicate insights from data analysis using the Rmarkdown/distill packages\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-18T21:54:53+08:00",
    "input_file": {}
  }
]
