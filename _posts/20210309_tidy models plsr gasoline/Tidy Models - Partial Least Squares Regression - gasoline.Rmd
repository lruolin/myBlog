---
title: "Tidy Models - Regression"
description: |
  Predicting numerical outcomes using partial least squares on gasoline dataset
author:
  - name: lruolin
date: 03-09-2021
output:
  distill::distill_article:
    self_contained: false
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview

  The partial least square (PLS) regression method may be used to predict one or more numerical Y outcome variable when the X variables are highly correlated. In the chemistry field, it is very useful for relating spectra to chemical properties. 
  
  The problem with using ordinary least square linear regression for such datasets is that when X variables are highly correlated, it is difficult to interpret coefficients of different X meaningfully, since they are all correlated. 
  
  PLS technique is also useful when the number of samples is lesser than the number of variables. It maximises the **covariance** of **both** X and Y to calculate scores (how much a particular object contributes to a latent variable) and loadings (contribution of a particular variable). For principal component regression (PCR), only X variables are taken into account. If the variability of X is not related to variability of Y, then PCR will have difficulty identifying a predictive relationship, when one might exist. It helps to uncover latent structures in the highly correlated X variables, so as to predict Y. 
  
  In a way, PLS is a supervised dimension reduction. It finds components that maximally summarise the variation of the predictors while simultaneiously requiring these components to have maximum correlation with the response. 
  
  The workflow below is for predicting one Y variable, using multiple X variables, on the gasoline NIR dataset. The outcome variable of interest is the octane number. 
  

# General workflow:

1. Exploratory data analysis: Check if there are missing values, correlation of variables, identify the pre-processing steps required. How many variables are there? How are they distributed? Is it suitable for PLS? 

  The data may also be visualized before modelling.

2. Split the dataset into training and testing dataset. The training dataset is used to build and tune the models, and the testing dataset is used to evaluate the model. Define a 10-fold cross validation dataset, from the training dataset. Resampling is useful if the sample size is small, to have a better bias-variance tradeoff (minimise over-fitting of model) for better predictive ability.

3. Preprocess the training dataset. Data would have to be imputed if there are missing values, and also normalised since PLS is using variance to understand dissimilarity in the X variables.

4. Train the model using the training dataset. In this case, the pls model is used. For PLS model, there is one tuning parameter, which is the number of components. The tune package will be used for tuning, and the data will be the 10-fold cross validation dataset.

5. Assess the training model using root mean square error (RMSE) and mean absolute error (MAE), as well as r-sq for accuracy.

6. Determine which variables are important in the model

7. Predict new data

# Packages required

```{r}
library(pacman)
p_load(modeldata, pls, tidyverse, tidymodels, corrplot, 
       skimr, plsmod, ggthemes)

```

# Import data

This dataset is from the pls package, and has octane number (Y outcome variable), as well as NIR spectra of 60 gasoline samples.

```{r}
data(gasoline)
```

# Visualize

```{r}
data_plot <- cbind(gasoline, as.data.frame((unclass(gasoline$NIR)))) %>% 
  dplyr::select(-NIR) %>% 
  as_tibble() %>% 
  rowid_to_column() %>% 
  janitor::clean_names() %>% 
  pivot_longer(cols = starts_with("x"),
               names_to = "wavelength",
               values_to = "reflectance") %>% 
  mutate(wavelength_number = parse_number(wavelength))

data_plot %>% 
  ggplot(aes(x = wavelength_number, y = reflectance, col = factor(rowid))) +
  geom_line(show.legend = F) +
  labs(y = "log(1/R)",
       x = "nm",
       title = "Plot of NIR spectra for 60 gasoline samples") +
  coord_cartesian(xlim = c(900, 1800)) +
  theme_classic()
```

# Modelling

  To use the dataset for modelling using the tidymodels framework, convert the data into a tidy tibble structure. The pls package requires that the data be provided as a matrix format, but for the tidymodels framework, the data should be in a tibble format for ease in creating plots and modelling.
  
```{r}

gasoline_tidy <-  cbind(gasoline, as.data.frame((unclass(gasoline$NIR)))) %>% 
  dplyr::select(-NIR) %>% 
  as_tibble() %>% 
  janitor::clean_names() 

glimpse(gasoline_tidy) # 60 rows, 402 columns
```
# EDA

Check for missing values using purrr::map()

```{r}
gasoline_tidy %>% 
  purrr::map(is.na) %>% 
  map_df(sum) %>% 
  tidy() %>% 
  dplyr::select(column, mean) %>% 
  as_tibble() %>% 
  filter(mean>0) # no missing values
```

For other datasets, one should check the correlation again, but in this case, as this is a NIR dataset, I will not do this step.

# Split dataset

```{r}
# initial split
set.seed(20210308)
gasoline_split <- initial_split(gasoline_tidy, prop = 0.8)

gasoline_training <- gasoline_split %>% 
  training()

gasoline_testing <- gasoline_split %>% 
  testing()

gasoline_cv <- vfold_cv(gasoline_training) # to tune number of components later

```

# Modelling

```{r}
# recipe

gasoline_reciped <- recipe(octane ~ ., data = gasoline_training) %>% 
  update_role(octane, new_role = "outcome") %>% 
  step_normalize(all_predictors())

gasoline_reciped

# fit model

pls_model <- plsmod::pls(num_comp = tune()) %>% 
  set_mode("regression") %>% 
  set_engine("mixOmics")

# put into workflow

pls_workflow <- workflow() %>% 
  add_recipe(gasoline_reciped) %>% 
  add_model(pls_model)

# create grid
pls_grid <- expand.grid(num_comp = seq (from = 1, to = 20, by = 1))

tuned_pls_results <- pls_workflow %>% 
  tune_grid(resamples = gasoline_cv,
            grid = pls_grid,
            metrics = metric_set(mae, rmse, rsq))

(model_results <- tuned_pls_results %>% 
  collect_metrics())
```

Visualize:

```{r}
# plot

tuned_pls_results %>% 
  collect_metrics() %>% 
  ggplot(aes(num_comp, mean, col = .metric)) +
  geom_point() +
  geom_line() +
  scale_x_continuous(n.breaks = 20) +
  labs(x = "Number of components",
       y = "Indicator",
       title = "Plot of MAE, RMSE and R-SQ vs number of components for TRAINING dataset, with 10-fold repeated cross validation",
       subtitle = "Optimal number of components is 3") +
 facet_grid(.metric ~.) +
  theme_few() +
  theme(legend.position = "none")
```

Check against numerical values:

```{r}
# check against numerical values
tuned_best <- tuned_pls_results %>% 
  select_best("rsq") 

tuned_best

# check with other indicators
tuned_pls_results %>% 
  select_best("mae") 

tuned_pls_results %>% 
  select_best("rmse")

```

In this case, I will go for num_comp = 4.
The aim of PLS is to reduce number of variables, but there would be a slight trade-off in terms of accuracy/error in favor of simplicity of model when determining the number of components.

Update model and workflow:

```{r}
updated_pls_model <-  plsmod::pls(num_comp = 4) %>% 
  set_mode("regression") %>% 
  set_engine("mixOmics")

updated_workflow <- pls_workflow %>% 
  update_model(updated_pls_model)
  

pls_fit <- updated_workflow %>% 
  fit(data = gasoline_training)

```

Check the most important X variables for the updated model:

```{r}
# check the most important predictors

tidy_pls <- pls_fit %>% 
  pull_workflow_fit() %>% 
  tidy()


# variable importance
tidy_pls %>% 
  filter(term != "Y", # outcome variable col name
         component == c(1:4)) %>% 
  group_by(component) %>% 
  slice_max(abs(value), n = 20) %>% 
  ungroup() %>% 
  ggplot(aes(value, fct_reorder(term, value), fill = factor(component))) +
  geom_col(show.legend = F) +
  facet_wrap( ~ component, scales = "free_y") +
  labs( y = NULL) +
  theme_few()
```

# Assess

```{r}
# results_train
pls_fit %>% 
  predict(new_data = gasoline_training) %>% 
  mutate(truth = gasoline_training$octane) %>% 
  ggplot(aes(truth, .pred)) +
  geom_point() +
  geom_abline() +
  labs(title = "Actual vs Predicted for TRAINING dataset",
       x = "Actual Octane Number",
       y = "Predicted Octane Number") +
  theme_few()

pls_fit %>% 
  predict(new_data = gasoline_training) %>% 
  mutate(truth = gasoline_training$octane) %>% 
  metrics(truth = truth, estimate = .pred) %>% 
  as_tibble() # rsq is 0.981 for training dataset

# results_test
pls_fit %>% 
  predict(new_data = gasoline_testing) %>% 
  mutate(truth = gasoline_testing$octane) %>% 
  ggplot(aes(truth, .pred)) +
  geom_point(col = "deepskyblue3") +
  geom_abline(col = "deepskyblue3") +
  labs(title = "Actual vs Predicted for TESTING dataset",
       x = "Actual Octane Number",
       y = "Predicted Octane Number") +
  theme_few()

pls_fit %>% 
  predict(new_data = gasoline_testing) %>% 
  mutate(truth = gasoline_testing$octane) %>% 
  metrics(truth = truth, estimate = .pred) %>% 
  as_tibble() # r-sq is 0.989 for testing dataset

```

# To predict future data

In this case, I will just take the first dataset and check if the predicted value is the actual value. This should not be done, but I did it just to understand how the workflow is.

```{r}
trial_data <- gasoline_tidy %>% 
  head((1)) %>% 
  dplyr::select(-octane) # octane = 85.3

pls_fit %>% 
  predict(trial_data) # 85.3

# actual value = predicted value
```

# Reflections

Through this exercise, I learnt:

- how to format the matrix data into a tibble dataframe. I have not worked with AsIs structure before and had to look up stacksoverflow to understand how to code to get a plot for NIR spectra

- how to apply tidymodels for pls modelling. I also learnt that there were different types of pls algorithms. Perhaps I would compare the different model algorithms for my next practice. 

- how to extract variable importance

- how to predict Y variable using a new dataset, although in this case I did not use new data

- I would like to learn how to model for multi-outcome data in the future. 

# References

- <https://stackoverflow.com/questions/64254295/predictor-importance-for-pls-model-trained-with-tidymodels>

- <https://rpubs.com/RandallThompson15/HW10_624>

- <https://www.tidyverse.org/blog/2020/04/parsnip-adjacent/>

- Applied Predictive Modelling, Max Kuhn and Kjell Johnson, Chapter 6. <http://appliedpredictivemodeling.com/>

